## 性能测试初识

### 性能测试——分层

性能测试时一个复杂的过程，它更像是一个过程的统称。性能测试大体上可以分为三层：**服务端层、客户端层，网络层**。

![image-20200915193420554](性能测试.assets/image-20200915193420554.png)

**1、服务端**

学习性能测试我们首先要弄清楚两个方向，服务端方向和客户端方向。首先说服务端，无论是`web`还是`app`，服务端的性能测试方向大体上都是类似的。大体也可以分为：操作系统、中间件和容器。

![img](性能测试.assets/1180565-20200304135834951-864904952.png)

**2、客户端**

客户端性能一般是指具有图形界面的应用程序的性能，能看得到的页面，比如网站的各个页面，app的各个页面等。当客户端出现性能问题时，一般的表现就是应用的操作不流畅，图形界面发生卡顿等。这里要强调一点就是app的性能测试，**好多人分不清app的性能测试，首先app的性能测试也是大体分为前端性能测试（即app专项测试）和服务端性能测试，服务端性能测试也就是平常所说的性能测试**。

![img](性能测试.assets/1180565-20200304135850225-1855729639.png)

**3、区分服务端和客户端的性能问题**

当我们发现性能问题的时候，首先要大概区分是服务端的性能问题还是客户端的性能问题，然后再去做相应的分析调优。

**一般来说单机应用出现性能问题，大部分都是客户端问题，比如：**

- 单机游戏卡顿
- 画图软件打开图片超慢
- web页面切换卡顿，页面加载时间长

**一般来说下面的一些性能问题就有可能是服务端问题或网络问题，比如：**

- 微博api访问速度慢
- 数据查询速度慢，比如查询商品或者订单很慢

**还有一些联网的应用出现性能问题，可能是客户端也可能是服务端或网络问题，比如：**

- 聊天软件发送信息慢
- 邮件客户端收信发信都很卡
- 直播软件声音卡顿

### 性能测试——目的

- 1、压力测试下系统是否满足预期目标；
- 2、发现系统存在的瓶颈，为调优指明方向；
- 3、察看系统承受的最大压力以及最佳压力；
- 4、系统在长时间的规定压力下是否能正常处理各种请求，
  考察系统的稳定性；
- 5、容量规划，要考虑到未来的用户慢慢增加后系统是否能满足要求。

![img](性能测试.assets/1180565-20200304135912544-868588708.png)

### 性能测试——主要术语

**并发数：**

LoadRunner 中的虚拟用户数就是并发数，即和系统产生了交互操作，这里注意定义，是与服务器产生了交互的！

**注册用户数：**指系统中全部注册用户的数量

**在线用户数：**指在相同时间段内都登录了系统，但未必会产生交互操作。别和并发数混了！

**响应时间：**

客户端的请求响应时间 = N1 + A1 + N2 + A2 +N3 + A3 + N4 + A4 + N5 + A5 + N6：

![img](性能测试.assets/1180565-20200304135941116-544690103.png)

**TPS：**

- 吞吐量: 指在一次性能测试过程中网络上传输的数据量的总和。
- `TPS`：每秒钟处理完的事务次数，一般`TPS`是对整个系统来讲的。一个应用系统`1s`能完成多少事务处理，一个事务在分布式处理中，可能会对应多个请求，对于衡量单个接口服务的处理能力，用`QPS`比较多。
- 并发数：系统能同时处理的请求数/事物数
- `RT`：响应时间，处理一次请求所需要的平均处理时间

计算关系：

1. `TPS` = 并发数 / 平均响应时间
2. 并发数 = `TPS` * 平均响应时间
3. `TPS` 不等于吞吐量！！！！不同纬度的统计，吞吐量从网络角度看，指单位时间内网络上传输的数据量

性能测试中`TPS`上不去的几种原因浅析：

- 1、网络带宽
  在压力测试中，有时候要模拟大量的用户请求，如果单位时间内传递的数据包过大，超过了带宽的传输能力，那么就会造成网络资源竞争，间接导致服务端接收到的请求数达不到服务端的处理能力上限。
- 2、硬件资源
  包括`CPU`（配置、使用率等）、内存（占用率等）、磁盘（`I/O`、页交换等）。
- 3、数据库配置
  高并发情况下，如果请求数据需要写入数据库，且需要写入多个表的时候，如果数据库的最大连接数不够，或者写入数据的`SQL`没有索引没有绑定变量，抑或没有主从分离、读写分离等，就会导致数据库事务处理过慢，影响到`TPS`。
- 4、压力机
  比如`jmeter`，单机负载能力有限，如果需要模拟的用户请求数超过其负载极限，也会间接影响`TPS`（这个时候就需要进行分布式压测来解决其单机负载的问题）。
- 5、业务逻辑
  业务解耦度较低，较为复杂，整个事务处理线被拉长导致的问题。

**事务：**

事务是性能测试脚本的一个重要特性。要度量服务器的性能，需要定义事务，每个事务都包含事务开始和事务结束标记。事务用来衡量脚本中一行代码或多行代码的执行所耗费的时间。

**PV**

`page view（PV）`：页面浏览量或点击量，用户每次刷新即被计算一次。我们可以认为，用户的一次刷新，给服务器造成了一次请求

### 去并发数

**并发数的定义：并发数，计算机网络术语，是指同时访问服务器站点的连接数。**

大部分人在刚开始做性能测试的时候都一直在纠结“并发数”，并发用户数并没有那么重要，也不是衡量的重要指标。

- 1、假如 `1` 个用户在 `1s` 内完成 `1` 笔事物，`tps=1`
- 2、假如某笔业务响应时间是 `1ms`，`1` 个用户在 `1s` 内完成 `1000` 笔事物，`tps=1000`；
- 3、如果某笔业务响应时间是 `1s`，1 个用户在 `1s` 内完成 1 笔事物，要想达到 `1000tps`，至少需要 `1000` 个用户；

所以，`1` 个用户可以产生 `1000tps`，`1000` 个用户也可以产生 `1000tps`，无非是看响应时间快还是慢，用并发数来衡量没什么意义。因此，我们要弱化并发数的概念。而且如果你加入思考时间，并发数基本可以增加一倍。

**再举个例子：**每个地铁口闸机每秒钟只能通过1个人（`TPS=1`），10个人去排队，想一起通过（并发数`=10`），那是不行的，只能慢慢排队一个一个通过。把通过闸机的时间压缩，比如刷卡相应很快，`0.1s`就能通过一个人了。那`1`秒钟通过的人数就是`10`了，抑或增加闸机数（相当于加服务器），那每秒钟通过的人数（`tps`）是不是多了。所以并发更多的是去帮我们找到性能瓶颈的那个点。

在性能测试中做上万的用户并发这种情况非常少，如果只需要保证系统处理业务时间足够快，那么几百个甚至几十个足已。日活量有`10`万，同时在线的有多少？`1`万？同时对一个接口产生的压力又有多少人（比如同时加入购物车，注意是同时），1千？就算你是同时点击加入购物车，到达服务器的时间也是有差别的哦，中间不是还要经过网络么，所以仔细想想，并发真的需要这么多么。

### 性能测试典型模型分析

![img](性能测试.assets/1180565-20200304140004942-1831156118.png)

1. 最开始，随着并发用户数的增长，资源占用率和吞吐量会相应的增长，但是响应时间的
   变化不大；
2. 不过当并发用户数增长到一定程度后，资源占用达到饱和，吞吐量增长明显放缓甚至停
   止增长，而响应时间却进一步延长。
3. 如果并发用户数继续增长，你会发现软硬件资源占用继续维持在饱和状态，吞吐量开始
   下降，响应时间明显的超出了用户可接受的范围，并且最终导致用户放弃了这次请求甚
   至离开。
4. 根据这种性能表现，图中划分了三个区域，分别是较轻的压力区、较重的压力区和用户无法忍受并放弃请求区域。在较轻的压力区和较重的压力区两个区域交界处的并发用户数，我们称为“最佳并发用户数（`The Optimum Number of Concurrent Users`）”。而 较重的压力区 和用户无法忍受并放弃请求区域两个区域交界处的并发用户数则称为“最大并发用户数（`The Maximum Number of Concurrent Users`）”。
5. 当系统的负载等于最佳并发用户数时，系统的整体效率最高，没有资源被浪费，用户也
   不需要等待
6. 当系统负载处于最佳并发用户数和最大并发用户数之间时，系统可以继续工作，但是用
   户的等待时间延长，满意度开始降低，并且如果负载一直持续，将最终会导致有些用户
   无法忍受而放弃；
7. 而当系统负载大于最大并发用户数时，将注定会导致某些用户无法忍受超长的响应时间
   而放弃。

### 木桶原理/短板效应

![img](性能测试.assets/1180565-20200304140020249-371037241.png)

**一个木桶能装多少水不是取决于最长的板，而是最短的板。所以在性能测试中我们应该优先调整最短的那个板，而不是一箩筐都搞！**

### 多角度看待性能测试

从三个不同层面来对性能进行阐述：

- 从用户角度，软件性能就是软件对用户操作的响应时间；
- 从运维人员角度，软件性能表现在系统是否能够提供给用户稳定、可靠、可持续服务，包括服务器资源等；
- 从开发员角度，软件性能表现在如何调整设计和代码的实现，通过调整系统的设置等提
  高性能；故考虑性能测试，要从不同角度去思考问题，从用户角度和从应用系统角度来看性能
  测试，理解上会存在一些差异；电商网站每年的双 11 活动都是对其服务器性能的挑战。因为在这一天很多商品特价
  （但其实特价没有只有鬼知道），购物的用户量剧增。做为网站的高层更多的关心是什么指标？对于一名技术人员，我们可能更关心什么指标？

### 性能测试——流程

![img](性能测试.assets/1180565-20200304140033103-258126600.png)

**过程分析：**

**（1）性能需求点获取**

- 根据客户的需求由客户方提出
- 根据历史数据分析
- 参考历史项目或者其他同行业的项目
- 业内通用规则
- 确实没有数据，那就根据自己来，然后一边分析

**（2）测试点的提取，常放在客户常用的、重点的模块和功能上**

- 用户常用功能，比如登录
- 数据流转向复杂或频繁的地方
- 发生频率高的地方，比如搜索，提交订单，下单结账。
- 关键程度高的（比如产品经理认为绝对不能出现问题的地方，登录、下单等）
- 资源占用非常严重的
- 关键的接口

**（3）测试环境**

- 最好能和线上保持一致
- 如果不能那就等比的放大或缩小
- 软件版本应该一致

**（4）测试数据**

- 铺底数据的准备：是空库还是有数据量。数据量的选择参考线上的数据量进行
  等比的放大或缩小。
- 最好的数据来源于线上的真实数据，因为分布合理
- 如果涉及到保密的数据，注意数据的脱密处理

**（5）测试过程**

- 性能测试时一个需要不断改进的过程，每一次尽量做得更好，多做一点点以前
  没想到的东西，然后不断积累总结，然后你会发现自己对性能测试有了更深的
  理解

**（6）响应时间预估**

- 线上监控系统得知
- 业界统一参考标准：2 - 5 - 8

**（7）预估并发用户数**

- 系统的性能由TPS决定，跟并发用户数没有多大关系。**系统最大的TPS是一定的，就好比池塘里装的水是有限的。但是并发用户数不一定，可以通过减小思考时间来增大并发用户数。**
- 新系统：没有历史数据参考，只能通过业务部门来评估；
  80-20：百分之八十的事物是在百分之二十的时间内完成的。（只知道系统注册用户数 or 在线用户数的时候可选择）
- 旧系统：最好通过日志分析来得出
- 可以选取峰值时刻，在一定时间内（单位：秒）使用系统的人数，并发用户数取 10%，
  之后可根据实际情况梯阶式增加

### 前端性能测试

https://www.cnblogs.com/lei2007/archive/2013/08/16/3262897.html

## 地铁站模型

**地铁模型分析**

和绝大部分人一样，小白每天都要乘坐地铁上下班，那么就拿地铁来分析，再次深刻理解下性能。早上乘坐地铁上班，最典型的就是深圳地铁1、3、4号线等，人多得简直没法形容！为了方便理解分析，先做如下假设。

某地铁站进站只有3个刷卡机。

人少的情况下，每位乘客很快就可以刷卡进站，假设进站需要1s。

乘客耐心有限，如果等待超过30min，就会暴躁、唠叨，甚至选择放弃。

**按照上述的假设，最初会出现如下的场景。**

- 场景一：只有1名乘客进站时，这名乘客可以在1s的时间内完成进站，且只利用了一台刷卡机，剩余2台等待着。
- 场景二：只有2名乘客进站时，2名乘客仍都可以在1s的时间内完成进站，且利用了2台刷卡机，剩余1台等待着。
- 场景三：只有3名乘客进站时，3名乘客还能在1s的时间内完成进站，且利用了3台刷卡机，资源得到充分利用。

想到这里，小白越来越觉得有意思了，原来技术与生活这么息息相关，真的可以快乐学习哦。随着上班高峰的到来，乘客也越来越多，新的场景也慢慢出现了。

- 场景四：`A、B、C`三名乘客进站，同时`D、E、F`乘客也要进站，因为A、B、C先到，所以D、E、F乘客需要排队，等A、B、C三名乘客进站完成后才行。那么，A、B、C乘客进站时间为1s，而D、E、F乘客必须等待1s，所以他们3位在进站的时间是2s。

 通过上面这个场景可以发现，每秒能使3名乘客进站，第1s是A、B、C，第2s是D、E、F，但是对于乘客D、E、F来说，“响应时间”延长了。

- 场景五：假设这次进站一次来了9名乘客，根据上面的场景，不难推断出，这9名乘客中有3名的“响应时间”为1s，有3名的“响应时间”为2s（等待1s+进站1s），还有3名的“响应时间”为3s（等待2s+进站1s）。

- 场景六：假设这次进站一次来了10名乘客，根据上面的推算，必然存在1名乘客的“响应时间”为4s，如果随着大量的人流涌入进站，可想而知就会达到乘客的忍耐极限。

- 场景七：如果地铁正好在火车站，例如，著名的北京西站、北京站。每名乘客都拿着大小不同的包，有的乘客拿的包太大导致卡在刷卡机那（堵塞），这样每名乘客的进站时间就会又不一样。

小白突然想到，貌似很多地铁进站的刷卡机有加宽的和正常宽度的两种类型，那么拿大包的乘客可以通过加宽的刷卡机快速进站（增加带宽），这样就能避免场景七中的现象。

- 场景八：进站的乘客越来越多，3台刷卡机已经无法满足需求，于是为了减少人流的积压，需要再多开几个刷卡机，增加进站的人流与速度（提升TPS、增大连接数）。

- 场景九：终于到了上班高峰时间了，乘客数量上升太快，现有的进站措施已经无法满足，越来越多的人开始抱怨、拥挤，情况越来越糟。单单增加刷卡机已经不行了，此时的乘客就相当于“请求”，乘客不是在地铁进站排队，就是在站台排队等车，已经造成严重的“堵塞”，那么增加发车频率（加快应用、数据库的处理速度）、增加车厢数量（增加内存、增大吞吐量）、增加线路（增加服务的线程）、限流、分流等多种措施便应需而生。

分析到这里，小白可以熟练地把性能指标与场景结合运用起来了，初步学习成果还是不错的。

##  jemter阶梯式压测-stepping

性能测试中，有时需要模拟一种实际生产中经常出现的情况，即：**从某个值开始不断增加压力，直至达到某个值，或者使用快增长或者慢增长模式增加并发**，然后持续运行一段时间。一般持续运行的时间是10-20分钟。

在`jmeter`中，有这样一个插件，可以帮我们实现这个功能，这个插件就是：`Stepping Thread Group`。这个插件类似于`LoadRunner`中的 `Controller`，作用就是模拟实际的生产情况，不断对服务器施加压力，直至到某个值，然后持续运行一段时间。

 `stepping thread group`：下载链接：https://jmeter-plugins.org/downloads/old/

![image-20200915193823532](性能测试.assets/image-20200915193823532.png)

 下载后，解压，将`JMeterPlugins-Standard.jar`包放在`jmeter`安装目录的`lib\ext`下，然后重新启动`jmeter`。

在测试计划下添加`stepping thread group`：

![image-20200915194036742](性能测试.assets/image-20200915194036742.png)

可设置参数如下：

![image-20200915194224657](性能测试.assets/image-20200915194224657.png)

参数设置解释：

`This group will start 100 threads`：设置线程组启动的线程总数为`100`个；

`First,wait for N seconds`：启动第一个线程之前，需要等待`N`秒；

`Then start N threads`：设置最开始时启动`N`个线程；

`Next,add 10 threads every 30 seconds,using ramp-up 5 seconds`：每隔`30`秒，在`5`秒内启动`10`个线程；

`Then hold load for 60 seconds`：启动的线程总数达到最大值之后，再持续运行`60`秒；

`Finally,stop 5 threads every 1 seconds`：每秒停止`5`个线程；

## 使用stepping group进行阶梯压力测试

### windows进行阶梯压力测试

在`NonGUI`模式下运行`jmeter`，并进行阶梯压力测试，步骤如下：

#### 第一步：安装插件	

保证`stepping thread group`插件已经安装成功

#### 第二步：创建并保存jmx脚本

2、打开`jmeter`，创建好`stepping thread group`后并设置好参数后，随便设置一些请求，并保存好`jmeter`脚本，如下：

![image-20200915194756906](性能测试.assets/image-20200915194756906.png)

这里要注意的是：

1. 保存的`jmeter`脚本尽量不要使用中文名命名

#### 第三步：cmd运行jmx脚本

打开`cmd`，使用`jmeter`的`NONGUI`运行`jmx`脚本：

```sh
jmeter -n -t E:\jmeter_test\login_xiaoqiang.jmx -l E:\testcases\script\test.jtl -e -o E:\testcases\report2

E:\testcases\script\test.jtl不需要存在
E:\testcases\report2也不需要存在
```

![image-20200915200242137](性能测试.assets/image-20200915200242137.png)

运行后，界面如下：

![image-20200915200341450](性能测试.assets/image-20200915200341450.png)

运行完成后，生成了`report2`目录：

![image-20200915200430701](性能测试.assets/image-20200915200430701.png)

打开`index.html`文件，查看测试报告信息：

![image-20200915200512342](性能测试.assets/image-20200915200512342.png)



### Linux下进行JMeter压测

在`Linux`下进行`JMeter`压测，把`Linux`作为压力机。

#### 第一步：安装jdk和jmeter

确保安装好了`JDK`和`Jmeter`并配置好了环境变量

`linux`上安装`jmeter`方法：

1、直接将`windows`上的`jmeter`的`zip`安装包上传到`linux`，解压（最好上传解压到`/usr/local`目录下）

2、配置环境变量，`vi /etc/profile`，并使其生效`source /etc/profile`

![image-20200915210114473](性能测试.assets/image-20200915210114473.png)

3、 查看是否安装成功，执行命令：`jmeter -v`

![image-20200915210352456](性能测试.assets/image-20200915210352456.png)

#### 第二步：创建目录并上传jmx脚本

创建目录：

```sh
cd /mendao/install
mkdir jmeter_store
cd jmeter_store
mkdir jmx   存放脚本的路径
mkdir report  测试运行生成的报告的存放路径
```

把`windows`调好的脚本上传到刚建立的`jmx`目录，**注意`windows`上`jmeter`版本要和`linux`上的`jmeter`版本一致。**

如果有使用`csv`参数化，也要把`csv`文件上传到`jmx`目录，且要进行`csv`参数化的话，注意要写相对路径，直接写文件名即可（前提是`csv`文档和脚本放在同一个目录下）。

![image-20200915211057644](性能测试.assets/image-20200915211057644.png)

可能出现的问题：

（1）`linux`环境`jmeter`与`win`环境编写脚本的`jmeter`版本不一致，版本改为一致。

（2）脚本中存在中文，去除中文。

（3）脚本中存在类似于`jp@gc - Active Threads Over Time` 监听器，去除监听器（查看结果树和聚合报告可以保留）

（4）记得`windows`中`jmeter`已安装的插件也要传到`linux`的`jmeter`安装目录的`lib\ext`中，如`JMeterPlugins-Standard.jar`包：

![image-20200915211224475](性能测试.assets/image-20200915211224475.png)

####  第三步：执行jmx脚本

```sh
jmeter -n -t /mendao/install/jmeter_store/jmx/login_xiaoqiang.jmx -l /mendao/install/jmeter_store/result/test915.jtl -e -o /mendao/install/jmeter_store/result/

-n:表示要使用nongui形式运行jmx脚本
-t：表示要运行的脚本
-l: 表示要生成的jtl路径，此jtl文件必须事先不存在
-e：表示要生成html报告，如果不写就不会生成
-o: 表示要生成的html报告目录,此目录可存在可不存在
```

![image-20200915211409942](性能测试.assets/image-20200915211409942.png)

 查看结果：

![image-20200915211704215](性能测试.assets/image-20200915211704215.png)

 把`report`整个文件夹传到`windows`上，打开即可，使用`sz`命令可以上传

![image-20200915211805110](性能测试.assets/image-20200915211805110.png)

## Linux监控

`Linux CPU`概念

- 物理`CPU`：主板上实际接入的`CPU`个数，可用数`physical id`来确定。
- `CPU`核数：每个`CPU`上面实际接入的芯片组数量，如双核、四核等。
- 逻辑`CPU`：一般情况下，**逻辑`CPU` = 物理CPU数量 * `CPU`核数**，如果逻辑`cpu`多于物理`cpu`，说明该`cpu`支持超线程技术（简单来说，它可使处理器中的`1` 颗内核如`2` 颗内核那样在操作系统中发挥作用。这样一来，操作系统可使用的执行资源扩大了一倍，大幅提高了系统的整体性能，此时逻辑`cpu=`物理`CPU`个数×每颗核数`x2`）。

### 查看`cpu`的详细信息

`cat /proc/cpuinfo`

![image-20200916101156180](性能测试.assets/image-20200916101156180.png)

### 查看内存信息

`cat /proc/meminfo`

```sh
cat /proc/meminfo
MemTotal:        2052440 kB //总内存
MemFree:           50004 kB //空闲内存
Buffers:           19976 kB //给文件的缓冲大小
Cached:           436412 kB //高速缓冲存储器(http://baike.baidu.com/view/496990.htm)使用的大小
SwapCached:        19864 kB //被高速缓冲存储用的交换空间大小
Active:          1144512 kB //活跃使用中的高速缓冲存储器页面文件大小
Inactive:         732788 kB //不经常使用的高速缓冲存储器页面文件大小
Active(anon):     987640 kB //anon：不久
Inactive(anon):   572512 kB
Active(file):     156872 kB
Inactive(file):   160276 kB
Unevictable:           8 kB
Mlocked:               8 kB
HighTotal:       1177160 kB //The total and free amount of memory, in kilobytes, that is not directly mapped into kernel space.
HighFree:           7396 kB // The HighTotal value can vary based on the type of kernel used.
LowTotal:         875280 kB // The total and free amount of memory, in kilobytes, that is directly mapped into kernel space.  used.
LowFree:           42608 kB //The LowTotal value can vary based on the type of kernel
SwapTotal:        489940 kB //交换空间总大小
SwapFree:         450328 kB //空闲交换空间
Dirty:               104 kB //等待被写回到磁盘的大小
Writeback:             0 kB //正在被写回的大小
AnonPages:       1408256 kB //未映射的页的大小
Mapped:           131964 kB //设备和文件映射的大小
Slab:              37368 kB //内核数据结构缓存的大小，可减少申请和释放内存带来的消耗
SReclaimable:      14164 kB //可收回slab的大小
SUnreclaim:        23204 kB //不可收回的slab的大小23204+14164=37368
PageTables:        13308 kB //管理内存分页的索引表的大小
NFS_Unstable:          0 kB //不稳定页表的大小
Bounce:                0 kB //bounce:退回
WritebackTmp:          0 kB //
CommitLimit:     1516160 kB
Committed_AS:    2511900 kB
VmallocTotal:     122880 kB //虚拟内存大小
VmallocUsed:       28688 kB //已经被使用的虚拟内存大小
VmallocChunk:      92204 kB
HugePages_Total:       0 //大页面的分配
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       10232 kB
DirectMap2M:      899072 kB
```

`io` : 输入输出，与硬盘或者磁盘交互的状态，比如读写磁盘 ，`load` ：平均负载，`Network` ：网络， 这几个指标之间的关系是相互依赖的，任何一个高负载都会导致其他指标出现问题，比如：

![image-20200916101617326](性能测试.assets/image-20200916101617326.png)     

所以：在分析的时候要尝试从多个入口去分析

### `CPU`的两个组成部分

1、`user time` : 非内核操作消耗cpu的时间，是用户或者程序所占用的时间。

2、`sys time` ：内核操作消耗cpu的时间，即操作系统消耗的时间。这个值越大，那么整个系统的性能就会越差，它能反映系统本身的情况。

小结：

- 比如发现`sys time`很高，那应该想到可能是linux操作系统本身的问题，可能需要去调一些内核的参数，提升系统性能；
- 如果`sys time`不高，`user time`很高的话，需要去看一下是不是程序代码的问题或者配置文件的问题

`Cpu`参考：

- 好： `user% + sys% < 70%`
- 一般：`user% + sys% = 80%`
- 糟糕：`user% + sys% >=90%`

###  load average

`load average`：`cpu`使用队列长度的统计信息，指一段时间内`cpu`**正在处理，等待处理的进程数之和**的统计信息。

理想的`load average < cpu`个数 `*` 核数`*0.7`

查询`cpu`个数的命令：

```sh
grep 'physical id' /proc/cpuinfo | sort -u
```

查询核数的命令：

```sh
grep 'core id' /proc/cpuinfo | sort -u | wc -l 
```

###  监控工具

####  top

  `top`命令能够**实时监控**系统的运行状态，可以按照`cpu`、内存和执行时间排序，通过交互式命令设定显示。

![image-20200916145055684](性能测试.assets/image-20200916145055684.png)

![image-20200916142622996](性能测试.assets/image-20200916142622996.png)

**第一行，任务队列信息，同 uptime 命令的执行结果**

> 系统时间：`14:14:48`
>
> 运行时间：`up 4:10 min,`
>
> 当前登录用户： `2 user`
>
> 负载均衡(`uptime`) `load average: 0.00, 0.01, 0.05`
>
> `average`后面的三个数分别是`1`分钟、`5`分钟、`15`分钟的负载情况。
>
> `load average`数据是**每隔`5`秒钟检查一次活跃的进程数，然后按特定算法计算出的数值**。如果这个数除以逻辑`CPU`的数量，结果高于`5`的时候就表明系统在超负荷运转了

**第二行，Tasks — 任务（进程）**

> 总进程:105 `total`, 运行:1 `running`, 休眠:104 `sleeping`, 停止: 0 `stopped`, 僵尸进程: 0 `zombie`

**第三行，cpu状态信息**

> `%id` ：空闲cpu的百分比
>
> `%wa` ：`wa`相当于`wait`嘛，就是等待输出输入的`cpu`百分比

**第四行,内存状态**

**第五行，swap交换分区信息**

> 2031612k total,   536k used, 2031076k free,  505864k cached【缓冲的交换区总量】

> 备注：
>
> 可用内存=`free + buffer + cached`
>
> 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。
>
> 第四行中使用中的内存总量（`used`）指的是现在系统内核控制的内存数，
>
> 第四行中空闲内存总量（`free`）是内核还未纳入其管控范围的数量。
>
> 纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到`free`中去，因此在`linux`上`free`内存会越来越少，但不用为此担心。

**第六行，空行**

**第七行以下：各进程（任务）的状态监控**

> `PID` : 进程id
>
> `USER` : 进程所有者
>
> `PR` : 进程优先级
>
> `NI` : nice值。负值表示高优先级，正值表示低优先级
>
> `VIRT` : 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES
>
> `RES` : 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA
>
> `SHR` : 共享内存大小，单位kb
>
> `S` :进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程
>
> `%CPU` : 上次更新到现在的CPU时间占用百分比
>
> `%MEM` : 进程使用的物理内存百分比
>
> `TIME+` : 进程使用的CPU时间总计，单位1/100秒
>
> `COMMAND` : 进程名称（命令名/命令行）`

`swap` ：交换区总量，相当于`windows`下的虚拟内存，唯一和`windows`不同的是`windows`虚拟内存可以与内存同时使用，但是`swap`是物理内存使用光之后才使用`swap`空间（系统会把物理内存里的访问频率低的内存对象引动到`swap`里，再在物理内存里产生新的连接指向`swap`里的那个对象）。

判断内存够不够的最直接简单的方法就是看内存的使用情况，当物理内存逐渐降低，swap逐渐被占用，那么说明内存已经不够了或者是出现了内存溢出、内存泄漏。

`cache` ：高速缓存，是位于`cpu`与主内存间的一种容量较小但速度很高的存储器。由于`cpu`的速度远高于主内存，`cpu`直接从内存中存储数据要等待一定时间周期，`cache`中保存着`cpu`刚用过或者循环使用到的一部分数据，当`cpu`再次使用该部分数据时可从`cache`中直接调用，这样就减少了`cpu`的等待时间，提高了系统的效率

`buffer` ：缓冲区，一个用于存储速度不同步的设备或者优先级不同的设备之间传输数据的区域。

& 两者相同点：都是缓存

& 两者不同点：存在的地方不一样，`cache`存在`cpu`与主内存之间，`buffer`存在于各个设备之间的缓存。

**Linux内存管理机制**

`linux`有自己的内存管理机制。`Linux`会尽可能的使用内存来提升`IO`效率，细节不必探究。

如果系统的`free`不够用，达到触发机制后，系统会自动释放`cache`和`buffer`的内存共程序使用（`cache`和`buffer`是由内核进行动态管理的）

如果`used`很多，而`cache`和`buffer`所占比率很小，那说明可能内存不够用了。不能单看`free`的大小来判断，所以你可以简单理解为`cache`和`buffer`也是`free`的一部分。

`cat /proc/meminfo`

（可用的`memory=free memory + buffers + cached`）

`swap`是磁盘上开辟的虚拟内存，所以他的变化有可能导致与`IO`的交互也会增加。

 **如何看多核:**

 对于多核的`cpu`来说，`cpu 0`是相当关键的，因为`cpu`各核间的调度都是通过`cpu 0`来完成的，`cpu0`的负载高就会影响其他核的性能。

`windows`下：任务管理器 > 某个进程 > 设置相关性

![image-20200916205942606](性能测试.assets/image-20200916205942606.png)

要某个程序运行在哪个核上就在哪个核前面打钩，然后点击确定。

`linux`下：利用`taskset`命令，设置并限制某个程序能被运行在哪个核上

![image-20200916205906225](性能测试.assets/image-20200916205906225.png)

上图代表对`cpu1 3 5 7`的各自进程进行限定。

#### Vmstat

 `Vmstat`是一款功能较齐全的性能监控工具，它是‘虚拟内存统计’的英文缩写，可以对操作系统的内存信息、进程状态、`cpu`活动、磁盘等信息进程监控，不足之处是无法对某个进程进行深入分析。

 ![image-20200916145337017](性能测试.assets/image-20200916145337017.png)

 `Vmstat`输出字段说明：

**（1）、`procs`区**

`r` ：表示等待和运行的队列，如果这个值过大，说明`cpu`可能比较忙，使用率高，需要增加`cpu`个数。

`b` ：堵塞并等待`io`的进程数，如正在等待`I/O`或者内存交换等。

 **（2）、`memory`区域**

`swpd` ：表示切换到内存交换区的内存大小（单位为`kb`），通俗讲就是虚拟内存的大小。

`free` ：表示当前空闲的物理内存（单位`kb`）

`buff` ：就是缓冲的大小，一般对设备的读写才需要缓冲，`IO`之间。

`cache` ：被用来作为高速缓存，一般对系统文件进行缓冲，频繁访问的文件都会被缓存，如果`cache`值比较大，则说明缓存文件比较多，如果此时`bi`比较小，说明系统效率较高。

**（3）、swap区**

`si` ：从磁盘分页到内存的数量，也就是由内存进入内存交换区的内存大小。

`so`：从内存分页到磁盘的数量，也就是由内存交换区进入内存的内存大小。

这两列表示内存交换的频繁程度，一般情况下，si、so的值都是0，如果值长期不是0，数值较大，则说明系统内存不足。

**（4）、I/O区**

bi ：读磁盘

bo ：写磁盘

**（5）I/O瓶颈**

`Cpu` 区域下的`wa`（等待`cpu`的百分比）值过大，`I/O`等待越严重（参考值 超过`20%`）

`Bo + bi`过大（参考值 超过`2000`），如果`bi + bo`的值过大，而且`cpu`区域`wa`值较大，则表示系统磁盘I/O瓶颈，应该提高磁盘的读写性能，或者减少应用程序不必要的磁盘读写。

 `vmstat`参数的使用：

![image-20200916145741923](性能测试.assets/image-20200916145741923.png)

 `vmstat 5 5`表示运行`5`次，每次`5`秒

小结：`vmstat`使用场景

![image-20200916150039421](性能测试.assets/image-20200916150039421.png)

 

####  sysstat工具包

安装`sysstat`：

```sh
yum -y install sysstat
```

##### 使用`mpstat`

![image-20200916155205805](性能测试.assets/image-20200916155205805.png)

`mpstat`参数的使用，如下：

`10`表示每`10`秒监控一次；`20`表示监控`20`次。`-P和ALL`必须大写

![image-20200916155224492](性能测试.assets/image-20200916155224492.png)

##### 使用iostat

 `Iostat`是`I/O statistics`（磁盘输入/输出统计）的缩写，主要是对系统的磁盘`I/O`操作进行监控，它的输出主要显示磁盘读写操作的统计信息，同时给出`cpu`的使用情况。

 直接输入`iostat:`

![image-20200916210213158](性能测试.assets/image-20200916210213158.png)

输入`iostat -x` ，得出的信息更加详细:

![image-20200916210246694](性能测试.assets/image-20200916210246694.png)

- `rrqm/s` ：每秒这个设备相关的读取请求有多少被合并（请求相同`block`时，请求合并）
- `wrqm/s` ：每秒这个设备相关的写入请求有多少被合并
- `r/s` ：每秒读取请求数（`rio`）
- `w/s` ：每秒写入请求数（`wio`）
- `R/s + w/s`：即为每秒总的`IO`操作次数
- `resc/s` ：每秒读扇区数
- `wsec/s` ：每秒写扇区数
- `rkB/s` ：每秒读取数据量，单位`K`字节
- `avgqu - sz` ：平均`IO`队列长度
- `await` ：平均每次设备`IO`操作的等待时间（毫秒）
- `svctm` ：平均每次设备`IO`操作的服务时间（毫秒），`svctm`越接近于`await`则说明等待时间少。
- `%util` ：表示了设备的繁忙程度，`80%`表示设备已经很忙了。

 IO瓶颈判断：

![image-20200916210653625](性能测试.assets/image-20200916210653625.png)

#####   使用glances

 如果是`centos7`系统，可能要事先安装一下：`yum -y install glances`

```sh
glances
```

![image-20200916214108738](性能测试.assets/image-20200916214108738.png)

`tx`是发送（`transport`），`rx`是接收(`receive`)

外网：一般`RX`指的是下载，`TX`指的是上传

内网恰恰相反

## 性能测试的指标预估

一般我们应该和功能测试一样，会有一个预期指标，然后和实际结果做对比的。

 当然，如果没有做过性能测试的公司可能就会比较迷茫，那么你可以建议他们从运维系统上获取数据作为参考

 但是，如果你们连运维系统都木有，那咋办呢。也好办，你自己做一次基准的性能测试得出数据，然后以后可以在这个数据基础上慢慢进行优化，逐步积累

举个例子：

> 新浪邮箱去年全年处理邮件约 `100` 万条，考虑到 `3` 年后可能递增到每年 `200`万条。
>
> 假设每年处理量集中在 `8` 个月，每个月 `20` 个工作日，每个工作日 `8` 小时，试采用 `80～20` 原理估算系统服务器高峰期的处理能力应达到怎样的一个神马水平？（计算他的TPS）
>
> 参考答案
>
> • `200万/8=25万/月`
>
> • `25万/20=1.25万/日`
>
> • `1.25万*80%/(8*20%*3600)`=`1.74TPS`

预估并发数

新系统：没有历史数据作参考，只能通过业务部门进行评估。

旧系统：对于已经上线的系统，可以选取峰值时刻，在一定时间内（单位：秒）使用系统的人数，并发用户数取`10%`，之后可根据实际情况梯阶式增加

峰值`PV/s`（通过日志分析系统`or`线上监控系统得知）

[曲线拐点模型分析](https://www.cnblogs.com/lily1989/p/8579798.html)

对于初学者来说，培养观察与分析思想是很重要的，首先来看一张典型的曲线拐点模型图

![img](性能测试.assets/clip_image002-1600263898385.jpg)

 

分析图1-2最好是先看一个个指标，然后再综合分析，这样的步骤更容易理解，思路也更加清晰明了。接下来就和小白一起来分析吧，分析思路如下。

1）`X`轴代表并发用户数，`Y`轴代表资源利用率、吞吐量、响应时间。`X`轴与`Y`轴区域从左往右分别是轻压力区、重压力区、拐点区。

2）然后一个个分析，根据前面学习的性能术语与指标进行理解，**随着并发用户数的增加，在轻压力区的响应时间变化不大，比较平缓，进入重压力区后呈现增长的趋势，最后进入拐点区后倾斜率增大，响应时间急剧增加**。

3）接着看吞吐量，**随着并发用户数的增加，吞吐量增加，进入重压力区后逐步平稳，到达拐点区后急剧下降，说明系统已经达到了处理极限，有点要扛不住的感觉。**

4）同理，随着并发用户数的增加，资源利用率逐步上升，最后达到饱和状态。

5）最后，把所有指标融合到一起来分析，随着并发用户数的增加，吞吐量与资源利用率增加，说明系统在积极处理，所以响应时间增加得并不明显，处于比较好的状态。但随着并发用户数的持续增加，压力也在持续加大，吞吐量与资源利用率都达到了饱和，随后吞吐量急剧下降，造成响应时间急剧增长。**轻压力区与重压力区的交界点是系统的最佳并发用户数，因为各种资源都利用充分，响应也很快；**而**重压力区与拐点区的交界点就是系统的最大并发用户数，因为超过这个点，系统性能将会急剧下降甚至崩溃。**

分析到这里，小白终于找到点成就感了，同时也庆幸自己没有忽略基础，看来基础对于日后的学习有着重要意义！

##  性能测试怎么做

```lua
概述一下性能测试流程？
1.分析性能需求。挑选用户使用最频繁的场景来测试。确定性能指标，比如：事务通过率为100%，TOP99%是5秒，最大并发用户为500人，CPU和内存的使用率在70%以下
2.制定性能测试计划，明确测试时间(通常在功能稳定后，如第一轮测试后进行)和测试环境和测试工具
3.编写测试用例
4.搭建测试环境，准备好测试数据
5.编写性能测试脚本
6.性能测试脚本调优。设置检查点（断言）、参数化、关联、集合点（阶梯式压测里面的快增长）、事务，调整思考时间，删除冗余脚本
7.设计测试场景，运行测试脚本，监控服务器
8.分析测试结果，收集相关的日志提单给开发
9.回归性能测试
10.编写测试报告
 
如何确定系统最大负载？
通过负载测试，不断增加用户数，随着用户数的增加，各项性能指标也会相应产生变化，当出现了性能拐点，比如，当用户数达到某个数量级时，响应时间突然增长，那么这个拐点处对应的用户数就是系统能承载的最大用户数

你们系统哪些地方(哪些功能)做了性能测试？
选用了用户使用最频繁的功能来做测试，比如：登陆，搜索，提交订单
 
你们的并发用户数是怎么确定的？
1）会先上线一段时间，根据收集到的用户访问数据进行预估
2）根据需求来确定（使用高峰时间段，注册用户数，单次响应时间等
 
你们性能测试在什么环境执行？
参考答案：我们会搭建一套独立的性能测试环境进行测试
 
你们性能测试什么时间执行？
基准测试：功能测试之后，系统比较稳定的时候再做。
负载测试：夜深人静，系统没人用的时候
 
怎么分析性能测试结果？
首先查看事物通过率，然后分析其他性能指标，比如，确认响应时间，事务通过率，CPU等指标是否满足需求；如果测试结果不可信，要分析异常的原因，修改后重新测试。
在确定性能测试结果可信后，如果发现以下问题，按下面的思路来定位问题

问题一：响应时间不达标
查看事务所消耗的时间主要在网络传输还是服务器，如果是网络，就结合Throughput(网络吞吐量)图，计算带宽是否存在瓶颈，如果存在瓶颈，就要考虑增加带宽，或对数据的传输进行压缩处理；如果不存在瓶颈，那么，可能是网路不稳定导致。如果主要时间是消耗在服务器上，就要分别查看web服务器和数据库服务器的CPU，内存的使用率是否过高，因为过高的CPU，内存必定会造成响应时间过长，如果是web服务器的问题，就把web服务器对应上对应的用户操作日志取下来，发给开发定位；如果是数据库的问题，就把数据库服务器对应上对应的日志取下来，发给开发定位。
 
问题二：服务器CPU指标异常
分析思路：就把web服务器对应上对应的用户操作日志取下来，发给开发定位。

问题三：数据库CPU指标异常
分析思路：把数据库服务器对应上对应的日志取下来，发给开发定位。

问题四：内存泄漏
分析思路：把内存的heap数据取出来，分析是哪个对象消耗内存最多，然后发给开发定位。

问题五：程序在单用户场景下运行成功，多用户运行则失败，提示连不上服务器。
原因：程序可能是单线程处理机制
 
如何识别系统瓶颈？
从TPS指标分析，TPS即系统单位时间内处理事务的数量。观察当前随着用户数的增长期系统每秒可处理的事务数是否也会增长
 
如何判断系统的性能是变好了还是变坏了
通过基准测试对比性能指标
 
 
你们的性能测试需求哪里来？
1：客户提供需求
2：运维提供需求
3：开发提供需求
 
如何实现200用户的并发？
在脚本对应的请求后添加集合点
 
 
什么情况下要做关联，关联是怎么做的？
当脚本的上下文有联系，就用关联。
比如登录的token关联，增删改查主键id关联
 
有验证码的功能，怎么做性能测试？
1、将验证码暂时屏蔽，完成性能测试后，再恢复
2、使用万能的验证码
 
 
你们性能测试做的是前台还是后台？
BS项目：测试的是后台服务器的性能和浏览器端性能；
APP项目：手机端和服务器端的性能都做
（客户端性能：比如，使用GZIP压缩；一个页面请求不宜过多）
 
性能测试指标有哪些
响应时间
吞吐量（tps）
cpu
内存
io
disk
 
 
如何脚本增强？
1、做参数化
2、做关联
3、添加事务
4、添加断言
5、添加集合点
6、添加思考时间


```

## 性能测试常见问题

```lua
1、cpu高导致响应时间长
用jmeter做性能测试的时候，遇到一个响应时间长的性能问题.
top命令一看，负载高，用户cpu将近100%，cpu已经达到性能瓶颈了，shift+p，或者键盘大写状态下按P，将所有进程按cpu使用率从高到低排序，这样，我们关注消耗cpu最多的进程即可。
数据库没有加索引，导致查询时间长，cpu持续增加。

2、TPS波动较大或者并发情况下大量报错。
原因一般有网络波动、其他服务资源竞争等
性能测试环境一般都是在内网或者压测机和服务在同一网段，可通过监控网络的出入流量来排查；
其他服务资源竞争也可能造成这一问题，可以通过Top命令或服务梳理方式来排查在压测时是否有其他服务运行导致资源竞争；
调优方案：
网络波动问题，可以让运维同事协助解决（比如切换网段或选择内网压测），或者等到网络较为稳定时候进行压测验证；
资源竞争问题：通过命令监控和服务梳理，找出压测时正在运行的其他服务，通过沟通协调停止该服务（或者换个没资源竞争的服务节点重新压测也可以）；
线程池问题：修改服务节点中容器的server.xml文件中的配置参数，主要修改如下几个参数：
 
在tomcat的安装目录conf目录下，打开server.xml文件
 


3、TPS偏小
1、网络带宽
在压力测试中，有时候要模拟大量的用户请求，如果单位时间内传递的数据包过大，超过了带宽的传输能力，那么就会造成网络资源竞争，间接导致服务端接收到的请求数达不到服务端的处理能力上限。
2、硬件资源
包括CPU（配置、使用率等）、内存（占用率等）、磁盘（I/O、页交换等）。
3、数据库配置
	高并发情况下，如果请求数据需要写入数据库，且需要写入多个表的时候，如果数据库的最大连	接数不够，或者写入数据的SQL没有索引没有绑定变量，抑或没有主从分离、读写分离等，就	会导致数据库事务处理过慢，影响到TPS。

4、压力机
比如jmeter，单机负载能力有限，如果需要模拟的用户请求数超过其负载极限，也会间接影响TPS（这个时候就需要进行分布式压测来解决其单机负载的问题）。
5、业务逻辑
业务解耦度较低，较为复杂，整个事务处理线被拉长导致的问题。

6、对于性能测试，如果一个查询比较慢不能达到预期响应指标数据，你怎么分析瓶颈在哪
通常查询涉及到数据库的操作，所以我们可以从以下几个方面入手数据库服务器的cpu、磁盘情况，对于常用查询是否做了缓存或者视图，数据库字段索引是否合理，sql语句是否合理；当然了，还要检查是否是网络的问题，或者是前端页面展示的问题。


4.你在性能测试中遇到哪些性能问题？
    响应时间突然变长：检查tomcat连接数
    tps毛刺过多：ramp up值过高，压力过大
    tps突然出现断崖式下跌：防火墙拦截，tcp连接中断
    进程无缘无故失踪：swap空间耗尽，触发了oomkiller
```



## 案例1：确定系统最大负载

本案例对小强系统的登录等步骤进行负载测试，初步确定将测试在80并发、100并发、120并发、

### 第一步：在jmeter调试好脚本

## CPU高的问题的定位

软件测试遇到功能问题，通过日志，单步调试相对比较好定位，而若是性能问题，例如线上服务器`CPU100%`，如何找到相关服务，如何定位问题代码，更考验技术人的功底。

题目：某服务器上部署了若干tomcat实例，即若干垂直切分的Java站点服务，以及若干Java微服务，突然收到运维的CPU异常告警。

问：如何定位是哪个服务进程导致CPU过载，哪个线程导致CPU过载，哪段代码导致CPU过载？

## 在一台机器实现多个tomcat的负载均衡

### nginx介绍

#### 前言

之前我们搭建网站的时候，把`war`包放到`tomcat`下就能运行起来了，为什么部署上线的时候，又用到了`nginx`呢？

**`nginx`可以做多台服务器的负载均衡**，当用户非常少的时候，可以用一台服务直接部署`web`环境，那么当用户达到百万级别，千万级别的时候，就需要增加服务器，多台服务器又如何管理协作的呢？

`nginx`有以下功能：

1.静态`HTTP`服务器——`Nginx`是一个`HTTP`服务器，可以将服务器上的静态文件（如`HTML`、图片）通过`HTTP`协议展现给客户端。

2.反向代理服务器——客户端本来可以直接通过`HTTP`协议访问某网站应用服务器，网站管理员可以在中间加上一个`Nginx`，客户端请求`Nginx`，`Nginx`请求应用服务器，然后将结果返回给客户端，此时`Nginx`就是反向代理服务器。

3.负载均衡——当网站访问量非常大，网站站长开心赚钱的同时，也摊上事儿了。因为网站越来越慢，一台服务器已经不够用了。于是将同一个应用部署在多台服务器上，将大量用户的请求分配给多台机器处理。同时带来的好处是，其中一台服务器万一挂了，只要还有其他服务器正常运行，就不会影响用户使用。

4.虚拟主机——有的网站访问量大，需要负载均衡。然而并不是所有网站都如此出色，有的网站，由于访问量太小，需要节省成本，将多个网站部署在同一台服务器上。

5.`FastCGI`——`Nginx`本身不支持`PHP`等语言，但是它可以通过`FastCGI`来将请求扔给某些语言或框架处理（例如`PHP`、`Python`、`Perl`）。

#### 什么是nginx?

`Nginx`是一款自由的、开源的、高性能的`HTTP`服务器和反向代理服务器；同时也是一个`IMAP`、`POP3`、`SMTP`代理服务器；

`Nginx`可以作为一个`HTTP`服务器进行网站的发布处理，另外`Nginx`可以作为反向代理进行负载均衡的实现。

**正向代理**，代理的是客户端，隐藏了客户端的信息。

![image-20200917183359543](性能测试.assets/image-20200917183359543.png)

**反向代理**，它代理的是服务端，主要用于服务器集群分布式部署的情况下，反向代理隐藏了服务器的信息。

![image-20200917183352380](性能测试.assets/image-20200917183352380.png)

### nginx安装

先安装依赖包

1.`gcc`安装：安装 `nginx` 需要先将官网下载的源码进行编译，编译依赖 `gcc` 环境，如果没有 `gcc` 环境，则需要安装

2.`PCRE(Perl Compatible Regular Expressions)` 是一个`Perl`库，包括 `perl` 兼容的正则表达式库。`nginx` 的 `http` 模块使用 `pcre` 来解析正则表达式，所以需要在 `linux` 上安装 `pcre` 库，`pcre-devel` 是使用 `pcre` 开发的一个二次开发库。`nginx`也需要此库

3.`zlib`库提供了很多种压缩和解压缩的方式， `nginx` 使用 `zlib` 对 `http` 包的内容进行 `gzip` ，所以需要在 `Centos` 上安装 `zlib` 库。

4.`OpenSSL` 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。`nginx` 不仅支持 `http` 协议，还支持 `https`（即在`ssl`协议上传输`http`），所以需要在 `Centos` 安装 `OpenSSL` 库。

```sh
yum install -y gcc-c++ 
yum install -y pcre pcre-devel 
yum install -y zlib zlib-devel 
yum install -y openssl openssl-devel
```

下载安装包并解压

```sh
cd /usr/local
mkdir nginx
cd nginx
wget -c https://nginx.org/download/nginx-1.12.0.tar.gz
tar -zxvf nginx-1.12.0.tar.gz
```

编译安装

```sh
cd /usr/local/nginx/nginx-1.12.0
./configure
make
make install
```

启动`nginx`

```sh
cd /usr/local/nginx/sbin
./nginx
```

浏览器访问`nginx`，`nginx`默认是在`80`端口启动的，在浏览器输入http://ip:80，正常访问到的页面如下：

![image-20200917185420640](性能测试.assets/image-20200917185420640.png)

`nginx`的相关命令：

```sh
1.启动服务
./nginx

2.停止服务,此方式停止步骤是待nginx进程处理任务完毕进行停止。
./nginx -s stop

3.退出服务,此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程。
./nginx -s quit

4.重新加载,当 ngin x的配置文件 nginx.conf 修改后，要想让配置生效需要重启 nginx，
使用-s reload不用先停止 ngin x再启动 nginx 即可将配置信息在 nginx 中生效
./nginx -s reload

5.查询nginx进程
ps aux|grep nginx
```

### 什么是负载均衡

#### 前言

当自己的`web`网站访问的人越来越多，一台服务器无法满足现有的业务时，此时会想到多加几台服务器来实现负载均衡。
 网站的访问量越来越大，服务器的服务模式也得进行相应的升级，怎样将同一个域名的访问分散到两台或更多的机器上呢？这就需要用nginx来配置负载均衡的环境了。

以多个`tomcat`服务为例，用`nginx`配置管理多个`tomcat`服务

#### 什么是负载均衡

负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。负载均衡，英文名称为`Load Balance`，其意思就是分摊到多个操作单元上进行执行，例如`Web`服务器、`FTP`服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。

如果还是不懂的话，可以举个例子：
 假设你是个妹子，你败家太厉害，以至于你的男友根本吃不消，于是乎你找了两个男朋友，一三五单号，二四六双号限行，从而减少一个男朋友所面临的压力，这叫负载均衡。

`nginx`的负载均衡策略有`2`种，第一种是**轮询**：也就是上面说的“两个男朋友，一三五单号，二四六双号限行”，看下图

![img](性能测试.assets/clip_image002-1600340598731.jpg)

### 同一个机器安装多个tomcat

下载tomcat安装包并解压：

```sh
cd /usr/local
mkdir tomcat
cd tomcat
wget http://mirrors.hust.edu.cn/apache/tomcat/tomcat-8/v8.5.35/bin/apache-tomcat-8.5.35.tar.gz
tar -zxvf apache-tomcat-8.5.35.tar.gz
mv apache-tomcat-8.5.50 apache-tomcat-1
cp -r apache-tomcat-1 apache-tomcat-2
cp -r apache-tomcat-1 apache-tomcat-3
```

修改**每一个**`tomcat`的`server.xml`配置文件：

修改1：

![image-20200917185921370](性能测试.assets/image-20200917185921370.png)

修改2：

![image-20200917190012422](性能测试.assets/image-20200917190012422.png)

修改3：

![image-20200917190118727](性能测试.assets/image-20200917190118727.png)

开放防火墙端口：

```sh
firewall-cmd --add-port=8081/tcp --permanent
firewall-cmd --add-port=8081/udp --permanent

firewall-cmd --add-port=8082/tcp --permanent
firewall-cmd --add-port=8082/udp --permanent

firewall-cmd --add-port=8083/tcp --permanent
firewall-cmd --add-port=8083/udp --permanent

firewall-cmd --reload
```

启动`3`个`tomcat`:

```sh
/usr/local/tomcat/apache-tomcat-1/bin/startup.sh
/usr/local/tomcat/apache-tomcat-2/bin/startup.sh
/usr/local/tomcat/apache-tomcat-3/bin/startup.sh
```

浏览器访问`3`个`tomcat`：`ip:8081`、`ip:8082`、`ip:8083`

确保`3`个`tomcat`都能访问成功后，修改每一个`tomcat`的首页`index.jsp`文件，以便区分：

```sh
vi /usr/local/tomcat/apache-tomcat-1/webapps/ROOT/index.jsp
vi /usr/local/tomcat/apache-tomcat-2/webapps/ROOT/index.jsp
vi /usr/local/tomcat/apache-tomcat-3/webapps/ROOT/index.jsp
```

修改的内容如下：

![image-20200917191416570](性能测试.assets/image-20200917191416570.png)

改完后，重新刷新`tomcat`的页面：

![image-20200917191453554](性能测试.assets/image-20200917191453554.png)

### 在nginx上配置tomcat的负载均衡

修改`nginx`的配置文件`nginx.conf`，实现让`nginx`把客户端请求轮询地指向其中一个`tomcat`服务

```sh
vi /usr/local/nginx/conf/nginx.conf
```

在文件里面的`server`上面添加内容（切记不要漏了末尾的分号'`;`'）：

![image-20200917192054650](性能测试.assets/image-20200917192054650.png)

`upstream`的作用是定义一组服务器， 这些服务器可以监听不同的端口，语法格式为：`upstream name { ... }`

继续在`nginx.conf`里面添加内容：

![image-20200917192812803](性能测试.assets/image-20200917192812803.png)

保存退出，重启`nginx`（如果已经启动了就不需要，多次启动会报错）：

```sh
cd /usr/local/nginx/sbin
./nginx -s reload
```

确保`3`个`tomcat`都已经启动了，然后浏览器访问`ip`地址： http://ip （不需要端口号），然后不断刷新，观察页面变化，可发现每次刷新，都会访问不同的`tomcat`服务器：

![image-20200917193224631](性能测试.assets/image-20200917193224631.png)

## 系统架构演化历程

现在出去找工作如果不会点分布式和微服务相关的内容，都不太好更面试官扯蛋。但这些架构也不是突然就出现的，而是经过不但演变才出现及流行起来的，本文就给大家来梳理下java项目架构的演变历程。

### 单体架构

大型网站都是从小型网站发展而来的，网站架构也是一样，是从小型网站架构逐步演化而来的，小型网站最开始没有太多人访问，只需要一台服务器就绰绰有余了，这时的架构如下:

![image-20200917202346161](性能测试.assets/image-20200917202346161.png)

应用程序、数据库、文件等所有的资源都在一台服务器上，通常服务器操作系统使用`Linux`、应用程序使用`java`或者其他语句，然后部署在`Apache`或者`Nginx`上。数据库使用`MYSQL`，使用开源的技术实现，然后部署在一台廉价的服务器上就开始了网站的发展之路。

### 应用服务和数据服务分离

好景不长，随着公司业务的发展，一台服务逐渐满足不了需求，越来越多的用户访问导致性能越来越差，数据存储空间开始不足，这时我们需要将应用和数据分离，分离后开始使用三台服务器：应用服务器、文件服务器、数据库服务器。如图:

![image-20200917202541808](性能测试.assets/image-20200917202541808.png)

应用和数据分离后，不同特性的服务器承担着不同的服务角色，网站的并发处理能力和数据存储空间都得到了很大的改善，支持网站业务进一步发展，但是随着用户逐渐增多，数据库压力越来越大，访问延迟，进而影响整个网站的性能，此时需要进一步优化。

### 缓存的使用

网站访问有个著名的二八定律，即`80%`的业务集中访问在`20%`的数据上，如果我们将这一小部分的数据缓存在内存中，能够很好的减少数据库的访问压力，提高整个网站的数据访问速度。

![image-20200917202708939](性能测试.assets/image-20200917202708939.png)

缓存常用的组件可以是`Redis`,`ehcache`等

### 集群的使用

缓存解决了数据库访问量比较大的问题，但是并不能解决随着业务增多造成的服务器并发压力大的问题，这时我们需要增加一台应用服务器来分担原来服务器的访问压力和存储压力。如图:

![image-20200917202749672](性能测试.assets/image-20200917202749672.png)

**通过负载均衡调度服务器，可将来自用户的访问请求分发到应用服务器中的任何一台服务器中**，这样多台服务器就分担了原来一台服务器的压力，我们只需要注意会话的一致性就可以了。

### 数据库读写分离

系统正常运行了一段时间后，虽然加的有缓存，使绝大多数的数据库操作可以不通过数据库就能完成，但是任然有一部分的操作(缓存访问不命中，缓存过期)和全部的写操作需要访问数据库，当用户达到一定规模后，数据库因为负载压力过大还是会成为系统的瓶颈， 这时主流的数据库都提供的有**主从热备份**功能，通过**配置两台数据库实现主从关系**，可以将一台数据库服务器的数据更新同步到另一台服务器上。可以利用这一功能来实现数据库读写分离。从而改善数据库的负载压力，如图:

![image-20200917202834196](性能测试.assets/image-20200917202834196.png)

`mysql`的读写分离可以通过自身自带的从主复制实现，`Oracle`的话可以通过阿里巴巴的`mycat`组件来实现。

### 反向代理和[CDN](https://cloud.tencent.com/product/cdn?from=10680)加速

为了应付复杂的网络环境和不同地区用户的访问，通过`CDN`和反向代理加快用户访问的速度，同时减轻后端服务器的负载压力。`CDN`与反向代理的基本原理都是缓存。`CDN`部署在网络提供商的机房。用户请求到来的时候从距离自己最近的网络提供商机房获取数据，而反向代理则部署在网站的中心机房中，请求带来的时候先去反向代理服务器中查看请求资源，如果有则直接返回。如图：

![image-20200917202939249](性能测试.assets/image-20200917202939249.png)

使用`CDN`和反向代理的目的都是尽早返回数据给用户，一方面加快用户的访问速度，另一方面也减轻后端服务器的负载压力。

帮助理解CDN链接：https://www.zhihu.com/question/36514327?rf=37353035

### 分布式文件和分布式数据库

任何强大的单一服务器都满足不了大型网站持续增长的业务需求。数据库经过读写分离后，从一台服务器拆分成两天服务器，但是随着业务的增长后面依然不能满足需求，这时我们需要使用分布式数据库，同时文件系统也一样，需要使用分布式文件系统。   

**分布式数据库是数据库拆分的最后的手段，只有在表单数据规模非常庞大的时候才使用**，不到不得已时，我们更常用的手段是业务分库，将不同的业务数据部署在不同的物理服务器上。

![image-20200917203516037](性能测试.assets/image-20200917203516037.png)

### NoSql和搜索引擎

随着业务越来越复杂，对数据存储和检索的需求也越来越复杂，这时一些`NoSQL`(`Reids`,[HBase](https://cloud.tencent.com/product/hbase?from=10680),`mongodb`)数据库技术和搜索引擎(`Solr`,`Elasticsearch`)的时候就显得很有必要。如下图:

![image-20200917203633726](性能测试.assets/image-20200917203633726.png)

`NoSQL`和搜索引擎对可伸缩的分布式特性具有更好的支持，应用服务器通过一个统一的数据访问模块访问各种数据，减轻应用程序管理诸多数据源的麻烦。

### 业务拆分

当访问量达到一定规模的时候我们可以通过分而治之的手段将整个系统的业务分成不同的产品线，例如我们将系统的首页，商铺，订单，买家，卖家，支付，订单等拆分成不同的产品线。

具体到技术实现上，也可以根据产品线划分，**将一个网站拆分成许多不同的应用，每个应用独立部署维护，应用之间通过`RPC`框架(`dubbo`,`webService`,`httpClient`…)建立连接，也可以通过消息队列实现异步分发处理。**来构成一个完整的系统，如下图:

![image-20200917203831485](性能测试.assets/image-20200917203831485.png)



### 分布式服务

随着业务拆分越来越小，存储系统越来越庞大，应用系统的整体复杂度呈指数级增加，部署维护越来越困难，由于所有应用要和所有数据库系统连接，最终导致数据库连接资源不足，拒绝服务。

1. 当服务越来越多时，服务`URL`配置管理变得非常困难，`F5`硬件负载均衡器的单点压力也越来越大。
2. 当进一步发展，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。
3. 接着，服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑？什么时候该加机器？
4. 服务多了，沟通成本也开始上升，调某个服务失败该找谁？服务的参数都有什么约定？
5. 一个服务有多个业务消费者，如何确保服务质量？
6. 随着服务的不停升级，总有些意想不到的事发生，比如`cache`写错了导致内存溢出，故障不可避免，每次核心服务一挂，影响一大片，人心慌慌，如何控制故障的影响面？服务是否可以功能降级？或者资源劣化？

解决方案：公共的应用模块被提取出来，部署在分布式服务器上供应用服务器调用。也就是我们将的分布式服务或者微服务。

微服务的设计原则参考此文: https://dpb-bobokaoya-sm.blog.csdn.net/article/details/87305626

## 分布式压测

todo（待写）

## 收集监控系统

`influxdb+grafana+telegra`监控系统的简单理解：`telegraf`收集系统`cpu`等数据，并写入数据到`influxdb`中，`jmeter`运行脚本，也写入测试数据到`influxdb`中，最终由`grafana`以图形化实时展示系统和测试数据。

![image-20200919120439834](性能测试.assets/image-20200919120439834.png)

### influx

官方文档：https://docs.influxdata.com/platform/getting-started/ 

或者https://docs.influxdata.com/influxdb/v1.6/

`InfluxDB` 是用`Go`语言编写的一个**开源分布式时序、事件和指标数据库**，无需外部依赖。类似的数据库有`Elasticsearch`、`Graphite`等。

**主要功能：**

1）基于时间序列，支持与时间有关的相关函数（如最大，最小，求和等）

2）可度量性：你可以实时对大量数据进行计算

3）基于事件：它支持任意的事件数据

**InfluxDB主要特点：**

1）无结构（无模式）：可以是任意数量的列

2）可拓展的

3）支持`min`, `max`, `sum`, `count`, `mean`, `median` 等一系列函数，方便统计

4）原生的`HTTP`支持，内置`HTTP API`

5）强大的类`SQL`语法

6）自带管理界面，方便使用

下载`influx`安装包：

```sh
mkdir /usr/local/influx
cd /usr/local/influx
wget https://dl.influxdata.com/influxdb/releases/influxdb-1.7.0.x86_64.rpm
```

下载完之后，直接就可以安装

```sh
yum localinstall influxdb-1.7.0.x86_64.rpm
```

启动`influxdb`数据库：

```sh
service influxdb start

停止：service influxdb stop
```

`inlufxdb`各个服务的默认端口：

```lua
8083：访问web页面的地址，8083为默认端口；
8086：数据写入influxdb的地址，8086为默认端口；（这个端口很重要）
8088：数据备份恢复地址，8088为默认端口；
```

防火墙添加端口：

```sh
firewall-cmd --add-port=8086/tcp --permanent
firewall-cmd --add-port=8086/udp --permanent 
firewall-cmd --reload
```

查看`8086`端口：

![image-20200919115012463](性能测试.assets/image-20200919115012463.png)

### grafana

`Grafana`是一个跨平台的开源的**度量分析和可视化工具**，可以通过**将采集的数据查询然后可视化的展示，并及时通知**。它主要有以下六大特点：

- 1、展示方式：快速灵活的客户端图表，面板插件有许多不同方式的可视化指标和日志，官方库中具有丰富的仪表盘插件，比如热图、折线图、图表等多种展示方式；
- 2、数据源：`Graphite`，`InfluxDB`，`OpenTSDB`，`Prometheus`，`Elasticsearch`，`CloudWatch`和`KairosDB`等；
- 3、通知提醒：以可视方式定义最重要指标的警报规则，Grafana将不断计算并发送通知，在数据达到阈值时通过`Slack`、`PagerDuty`等获得通知；
- 4、混合展示：在同一图表中混合使用不同的数据源，可以基于每个查询指定数据源，甚至自定义数据源；
- 5、注释：使用来自不同数据源的丰富事件注释图表，将鼠标悬停在事件上会显示完整的事件元数据和标记；
- 6、过滤器：`Ad-hoc`过滤器允许动态创建新的键/值过滤器，这些过滤器会自动应用于使用该数据源的所有查询。

官方文档：https://grafana.com/docs/grafana/latest/

#### grafana安装

下载安装包：

```sh
mkdir /usr/lccal/gragana
cd /usr/local/grafana
wget https://dl.grafana.com/oss/release/grafana-6.2.2-1.x86_64.rpm
```

安装`grafana`：

```
yum localinstall grafana-6.2.2-1.x86_64.rpm
```

启动`grafana`服务：

```sh
service grafana-server start

停止： service grafana-server stop

若是centos7，建议使用systemctl命令，如 systemctl start grafana-server
```

防火墙添加`3000`端口，`3000`为`Grafana`的默认侦听端口：

```sh
firewall-cmd --add-port=3000/tcp --permanent
firewall-cmd --add-port=3000/udp --permanent 
firewall-cmd --reload
```

浏览器访问`grafana`，http://ip:3000，初始账号和密码都是`admin`，初次登录需要修改密码。

![image-20200919121326250](性能测试.assets/image-20200919121326250.png)

#### grafana使用

1、添加`Data Source`，并选择`inlufxDB`数据源

![image-20200919191128494](性能测试.assets/image-20200919191128494.png)

选择`influxDB`:

![image-20200919191215403](性能测试.assets/image-20200919191215403.png)

2、设置数据源`Data Source`:

![image-20200919191412009](性能测试.assets/image-20200919191412009.png)

3、创建`dashboard`，有两种创建方式。

第一种，根据别人保存分享的模板来进行创建，可到`grafana`官网找别人分享的模板来创建`dashboard`：https://grafana.com/grafana/dashboards，官网上的每个`dashboard`都一个唯一的`dashboard id`，随便点击一个`dashboard`都可以看到，如：

![image-20200919191931746](性能测试.assets/image-20200919191931746.png)

想要导入模板，只需要把`ID`复制下来，根据下面步骤导入即可：

![image-20200919191706576](性能测试.assets/image-20200919191706576.png)

![image-20200919192222813](性能测试.assets/image-20200919192222813.png)

第二种，自己新建一个`dashboard`，步骤如下：

![image-20200919191615081](性能测试.assets/image-20200919191615081.png)

![image-20200919192317567](性能测试.assets/image-20200919192317567.png)

![image-20200919192401407](性能测试.assets/image-20200919192401407.png)

### telegraf

#### 安装telegraf

官方学习文档：https://docs.influxdata.com/telegraf/v1.9/introduction/getting-started/

`telegraf`是一个可收集系统和服务的统计数据，比如说`cpu`使用率等，并写入到`InfluxDB`数据库的程序。通过`telegraf`，我们可将系统`cpu`、内存、磁盘等使用数据实时地写入`InfluxDB`数据库，从而在`grafana`的`web`页面中以图表形式展示出来。

在`opt`目录使用`wget`获取安装包：

```sh
cd /opt
wget https://dl.influxdata.com/telegraf/releases/telegraf-1.0.1.x86_64.rpm
```

`yum`本地安装`telegraf`：

```sh
yum localinstall telegraf-1.0.1.x86_64.rpm
```

修改配置文件 `telegraf.conf`（此步根据自身情况修改）

```sh
vi /etc/telegraf/telegraf.conf
```

![image-20200918182514619](性能测试.assets/image-20200918182514619.png)

配置文件里可设置收集的数据要存放到`telegraf`的哪个库：

![image-20200918185702071](性能测试.assets/image-20200918185702071.png)

查看官方文档：

![image-20200918184949586](性能测试.assets/image-20200918184949586.png)

可以知道，在启动`telegraf`服务之前，需要先编辑或者初始化创建一个配置文件（其实这步可以跳过），这个配置文件包含了一些信息，如：

1. 统计的指标从哪里来
2. 统计的指标存放到哪里去

配置`telegraf`：

```sh
telegraf -sample-config -input-filter cpu:mem:disk -output-filter influxdb > telegraf.conf

解释：
-input-filter cpu:mem:disk     代表获取cpu、内存、磁盘的指标
-output-filter influxdb			代表这些指标存放到influxdb数据库里面
> telegraf.conf				代表这些配置信息将会存放到当前所在路径的telegraf.conf文件中，运行这行后会自动生成这个conf文件
```

启动`telegraf`:

```sh
service telegraf start 

经过观察，发现telegraf启动时默认读取的/etc/telegraf路径下的telegraf.conf，而不是前面一步配置生成的。
因此，如果要修改要搜集的指标时，建议直接cd到/etc/telegraf，再生成新的telegraf.conf文件，覆盖掉原来的就行。
```

进入`influxdb`，查看存在的库，会发现多了一个`telegraf`库：

![image-20200918190223614](性能测试.assets/image-20200918190223614.png)

查看`telegraf`库存在的表：`show measurements;`

![image-20200918190434827](性能测试.assets/image-20200918190434827.png)

查看`telegraf`库的表的所有字段：`show field keys;`

![image-20200918190542912](性能测试.assets/image-20200918190542912.png)

查看某个表的某些内容：`SELECT usage_idle FROM cpu WHERE cpu = 'cpu-total' LIMIT 5`

![image-20200918190725417](性能测试.assets/image-20200918190725417.png)

#### 实时监控cpu等信息

接下来要在`grafana`上以图表形式实时查看`telegraf`收集的存放在`influxdb`中的指标和数据。

第一步：添加`Data Sources`

![image-20200918191250052](性能测试.assets/image-20200918191250052.png)

设置的关键信息如下（其它默认即可）：

![image-20200918191332075](性能测试.assets/image-20200918191332075.png)

![image-20200918191341990](性能测试.assets/image-20200918191341990.png)



第二步：新建`Dashboard`

![image-20200918191516876](性能测试.assets/image-20200918191516876.png)

第三步：添加`query`查询

![image-20200918191540141](性能测试.assets/image-20200918191540141.png)

第四步：设置`query`：

![image-20200918191651152](性能测试.assets/image-20200918191651152.png)

![image-20200918191928197](性能测试.assets/image-20200918191928197.png)

举个例子：

![image-20200918192119406](性能测试.assets/image-20200918192119406.png)

添加多个`query`后的显示效果：

![image-20200918200553996](性能测试.assets/image-20200918200553996.png)



#### telegraf plugins

查看`telegraf`支持的`plugins`: https://docs.influxdata.com/telegraf/v1.15/plugins/#input-plugins

打开上面的网页后，可以看到`telegraf`支持的`input`和`output`等各种`plugins`，举个例子：

![image-20200919175518833](性能测试.assets/image-20200919175518833.png)

可以看到，`CPU`的`pugin id`为`inputs.cpu`，点击`View`：

![image-20200919175634583](性能测试.assets/image-20200919175634583.png)

因此，如果我们要使用`telegraf`收集`cpu`相关指标时，就直接在`telegraf.conf`配置文件中的`input plugins`下添加一下内容即可：

![image-20200919175840579](性能测试.assets/image-20200919175840579.png)

所以说，**修改要收集的指标时，有两种方法：**

1. 直接`vi`修改配置文件`telegraf.conf`，想要添加收集什么指标，直接把`[[inputs.xxx]]`等内容追加到`input plugins`里面即可。
2. 使用配置命令来修改`telegraf.conf`配置文件：`telegraf -sample-config -input-filter cpu:mem:disk -output-filter influxdb > telegraf.conf`

常见的`plugins`：

- `mem` ：系统内存信息，如物理、虚拟、交换内存量等等。
- `disk` ：磁盘占用信息。
- `diskio` ：磁盘`IO`性能。
- `net`和`netstat` ：网卡和网络信息。
- `system` ：当前系统负载信息，类似`uptime`信息。
- `file` ：每个时间周期读取文件所有信息。
- `cpu`
- `swap`
- `processes`

### 我自己的dashboard

我自己在`grafana`官网上`id`为`dashboard`模板的基础上，修改完善了后，个人认为还不错的，大致效果如下，可以监控`linux`系统的`cpu`、内存、磁盘、负载、网络等系统运行信息。

![image-20200919192813846](性能测试.assets/image-20200919192813846.png)

完成`dashboard`修改后，可点击分享：

![image-20200919192929440](性能测试.assets/image-20200919192929440.png)

分享方法1：别人可根据我的`link`链接来创建`dashboard`：http://192.168.239.135:3000/d/OrznxYOMk/influxdb_centos7_info?orgId=1&refresh=5s&from=1600511376487&to=1600514976487&var-host=Jimmy

![image-20200919192948563](性能测试.assets/image-20200919192948563.png)

分享方法2：`snapshot`:

![image-20200919193300707](性能测试.assets/image-20200919193300707.png)

分享方法3：`json`信息，别人可以根据我分享的`json`信息创建`dashboard`（将`dashboard`的`json`信息文件下载保存到本地可以是我们保存`dashboard`的一种方法）

![image-20200919193339404](性能测试.assets/image-20200919193339404.png)

![image-20200919193530124](性能测试.assets/image-20200919193530124.png)

`pannel`的查看技巧：

单击某个指标名称可仅查看单个指标信息：

![image-20200919202908700](性能测试.assets/image-20200919202908700.png)

双击其它名称时又可查看全部指标信息：

![image-20200919203039672](性能测试.assets/image-20200919203039672.png)

## 

### 性能测试初识

#### 性能测试——分层

性能测试时一个复杂的过程，它更像是一个过程的统称。性能测试大体上可以分为三层：**服务端层、客户端层，网络层**。

![image-20200915193420554](../../../Testing_learn/typora/performanceTesting.assets/image-20200915193420554.png)

**1、服务端**

学习性能测试我们首先要弄清楚两个方向，服务端方向和客户端方向。首先说服务端，无论是`web`还是`app`，服务端的性能测试方向大体上都是类似的。大体也可以分为：操作系统、中间件和容器。

![img](../../../Testing_learn/typora/performanceTesting.assets/1180565-20200304135834951-864904952.png)

**2、客户端**

客户端性能一般是指具有图形界面的应用程序的性能，能看得到的页面，比如网站的各个页面，app的各个页面等。当客户端出现性能问题时，一般的表现就是应用的操作不流畅，图形界面发生卡顿等。这里要强调一点就是app的性能测试，**好多人分不清app的性能测试，首先app的性能测试也是大体分为前端性能测试（即app专项测试）和服务端性能测试，服务端性能测试也就是平常所说的性能测试**。

![img](../../../Testing_learn/typora/performanceTesting.assets/1180565-20200304135850225-1855729639.png)

**3、区分服务端和客户端的性能问题**

当我们发现性能问题的时候，首先要大概区分是服务端的性能问题还是客户端的性能问题，然后再去做相应的分析调优。

**一般来说单机应用出现性能问题，大部分都是客户端问题，比如：**

- 单机游戏卡顿
- 画图软件打开图片超慢
- web页面切换卡顿，页面加载时间长

**一般来说下面的一些性能问题就有可能是服务端问题或网络问题，比如：**

- 微博api访问速度慢
- 数据查询速度慢，比如查询商品或者订单很慢

**还有一些联网的应用出现性能问题，可能是客户端也可能是服务端或网络问题，比如：**

- 聊天软件发送信息慢
- 邮件客户端收信发信都很卡
- 直播软件声音卡顿

#### 性能测试——目的

- 1、压力测试下系统是否满足预期目标；
- 2、发现系统存在的瓶颈，为调优指明方向；
- 3、察看系统承受的最大压力以及最佳压力；
- 4、系统在长时间的规定压力下是否能正常处理各种请求，
  考察系统的稳定性；
- 5、容量规划，要考虑到未来的用户慢慢增加后系统是否能满足要求。

![img](../../../Testing_learn/typora/performanceTesting.assets/1180565-20200304135912544-868588708.png)

#### 性能测试——主要术语

**并发数：**

LoadRunner 中的虚拟用户数就是并发数，即和系统产生了交互操作，这里注意定义，是与服务器产生了交互的！

**注册用户数：**指系统中全部注册用户的数量

**在线用户数：**指在相同时间段内都登录了系统，但未必会产生交互操作。别和并发数混了！

**响应时间：**

客户端的请求响应时间 = N1 + A1 + N2 + A2 +N3 + A3 + N4 + A4 + N5 + A5 + N6：

![img](../../../Testing_learn/typora/performanceTesting.assets/1180565-20200304135941116-544690103.png)

**TPS：**

- 吞吐量: 指在一次性能测试过程中网络上传输的数据量的总和。
- `TPS`：每秒钟处理完的事务次数，一般`TPS`是对整个系统来讲的。一个应用系统`1s`能完成多少事务处理，一个事务在分布式处理中，可能会对应多个请求，对于衡量单个接口服务的处理能力，用`QPS`比较多。
- 并发数：系统能同时处理的请求数/事物数
- `RT`：响应时间，处理一次请求所需要的平均处理时间

计算关系：

1. `TPS` = 并发数 / 平均响应时间
2. 并发数 = `TPS` * 平均响应时间
3. `TPS` 不等于吞吐量！！！！不同纬度的统计，吞吐量从网络角度看，指单位时间内网络上传输的数据量

性能测试中`TPS`上不去的几种原因浅析：

- 1、网络带宽
  在压力测试中，有时候要模拟大量的用户请求，如果单位时间内传递的数据包过大，超过了带宽的传输能力，那么就会造成网络资源竞争，间接导致服务端接收到的请求数达不到服务端的处理能力上限。
- 2、硬件资源
  包括`CPU`（配置、使用率等）、内存（占用率等）、磁盘（`I/O`、页交换等）。
- 3、数据库配置
  高并发情况下，如果请求数据需要写入数据库，且需要写入多个表的时候，如果数据库的最大连接数不够，或者写入数据的`SQL`没有索引没有绑定变量，抑或没有主从分离、读写分离等，就会导致数据库事务处理过慢，影响到`TPS`。
- 4、压力机
  比如`jmeter`，单机负载能力有限，如果需要模拟的用户请求数超过其负载极限，也会间接影响`TPS`（这个时候就需要进行分布式压测来解决其单机负载的问题）。
- 5、业务逻辑
  业务解耦度较低，较为复杂，整个事务处理线被拉长导致的问题。

**事务：**

事务是性能测试脚本的一个重要特性。要度量服务器的性能，需要定义事务，每个事务都包含事务开始和事务结束标记。事务用来衡量脚本中一行代码或多行代码的执行所耗费的时间。

**PV**

`page view（PV）`：页面浏览量或点击量，用户每次刷新即被计算一次。我们可以认为，用户的一次刷新，给服务器造成了一次请求

#### 去并发数

**并发数的定义：并发数，计算机网络术语，是指同时访问服务器站点的连接数。**

大部分人在刚开始做性能测试的时候都一直在纠结“并发数”，并发用户数并没有那么重要，也不是衡量的重要指标。

- 1、假如 `1` 个用户在 `1s` 内完成 `1` 笔事物，`tps=1`
- 2、假如某笔业务响应时间是 `1ms`，`1` 个用户在 `1s` 内完成 `1000` 笔事物，`tps=1000`；
- 3、如果某笔业务响应时间是 `1s`，1 个用户在 `1s` 内完成 1 笔事物，要想达到 `1000tps`，至少需要 `1000` 个用户；

所以，`1` 个用户可以产生 `1000tps`，`1000` 个用户也可以产生 `1000tps`，无非是看响应时间快还是慢，用并发数来衡量没什么意义。因此，我们要弱化并发数的概念。而且如果你加入思考时间，并发数基本可以增加一倍。

**再举个例子：**每个地铁口闸机每秒钟只能通过1个人（`TPS=1`），10个人去排队，想一起通过（并发数`=10`），那是不行的，只能慢慢排队一个一个通过。把通过闸机的时间压缩，比如刷卡相应很快，`0.1s`就能通过一个人了。那`1`秒钟通过的人数就是`10`了，抑或增加闸机数（相当于加服务器），那每秒钟通过的人数（`tps`）是不是多了。所以并发更多的是去帮我们找到性能瓶颈的那个点。

在性能测试中做上万的用户并发这种情况非常少，如果只需要保证系统处理业务时间足够快，那么几百个甚至几十个足已。日活量有`10`万，同时在线的有多少？`1`万？同时对一个接口产生的压力又有多少人（比如同时加入购物车，注意是同时），1千？就算你是同时点击加入购物车，到达服务器的时间也是有差别的哦，中间不是还要经过网络么，所以仔细想想，并发真的需要这么多么。

#### 性能测试典型模型分析

![img](../../../Testing_learn/typora/performanceTesting.assets/1180565-20200304140004942-1831156118.png)

1. 最开始，随着并发用户数的增长，资源占用率和吞吐量会相应的增长，但是响应时间的
   变化不大；
2. 不过当并发用户数增长到一定程度后，资源占用达到饱和，吞吐量增长明显放缓甚至停
   止增长，而响应时间却进一步延长。
3. 如果并发用户数继续增长，你会发现软硬件资源占用继续维持在饱和状态，吞吐量开始
   下降，响应时间明显的超出了用户可接受的范围，并且最终导致用户放弃了这次请求甚
   至离开。
4. 根据这种性能表现，图中划分了三个区域，分别是较轻的压力区、较重的压力区和用户无法忍受并放弃请求区域。在较轻的压力区和较重的压力区两个区域交界处的并发用户数，我们称为“最佳并发用户数（`The Optimum Number of Concurrent Users`）”。而 较重的压力区 和用户无法忍受并放弃请求区域两个区域交界处的并发用户数则称为“最大并发用户数（`The Maximum Number of Concurrent Users`）”。
5. 当系统的负载等于最佳并发用户数时，系统的整体效率最高，没有资源被浪费，用户也
   不需要等待
6. 当系统负载处于最佳并发用户数和最大并发用户数之间时，系统可以继续工作，但是用
   户的等待时间延长，满意度开始降低，并且如果负载一直持续，将最终会导致有些用户
   无法忍受而放弃；
7. 而当系统负载大于最大并发用户数时，将注定会导致某些用户无法忍受超长的响应时间
   而放弃。

#### 木桶原理/短板效应

![img](../../../Testing_learn/typora/performanceTesting.assets/1180565-20200304140020249-371037241.png)

**一个木桶能装多少水不是取决于最长的板，而是最短的板。所以在性能测试中我们应该优先调整最短的那个板，而不是一箩筐都搞！**

#### 多角度看待性能测试

从三个不同层面来对性能进行阐述：

- 从用户角度，软件性能就是软件对用户操作的响应时间；
- 从运维人员角度，软件性能表现在系统是否能够提供给用户稳定、可靠、可持续服务，包括服务器资源等；
- 从开发员角度，软件性能表现在如何调整设计和代码的实现，通过调整系统的设置等提
  高性能；故考虑性能测试，要从不同角度去思考问题，从用户角度和从应用系统角度来看性能
  测试，理解上会存在一些差异；电商网站每年的双 11 活动都是对其服务器性能的挑战。因为在这一天很多商品特价
  （但其实特价没有只有鬼知道），购物的用户量剧增。做为网站的高层更多的关心是什么指标？对于一名技术人员，我们可能更关心什么指标？

#### 性能测试——流程

![img](../../../Testing_learn/typora/performanceTesting.assets/1180565-20200304140033103-258126600.png)

**过程分析：**

**（1）性能需求点获取**

- 根据客户的需求由客户方提出
- 根据历史数据分析
- 参考历史项目或者其他同行业的项目
- 业内通用规则
- 确实没有数据，那就根据自己来，然后一边分析

**（2）测试点的提取，常放在客户常用的、重点的模块和功能上**

- 用户常用功能，比如登录
- 数据流转向复杂或频繁的地方
- 发生频率高的地方，比如搜索，提交订单，下单结账。
- 关键程度高的（比如产品经理认为绝对不能出现问题的地方，登录、下单等）
- 资源占用非常严重的
- 关键的接口

**（3）测试环境**

- 最好能和线上保持一致
- 如果不能那就等比的放大或缩小
- 软件版本应该一致

**（4）测试数据**

- 铺底数据的准备：是空库还是有数据量。数据量的选择参考线上的数据量进行
  等比的放大或缩小。
- 最好的数据来源于线上的真实数据，因为分布合理
- 如果涉及到保密的数据，注意数据的脱密处理

**（5）测试过程**

- 性能测试时一个需要不断改进的过程，每一次尽量做得更好，多做一点点以前
  没想到的东西，然后不断积累总结，然后你会发现自己对性能测试有了更深的
  理解

**（6）响应时间预估**

- 线上监控系统得知
- 业界统一参考标准：2 - 5 - 8

**（7）预估并发用户数**

- 系统的性能由TPS决定，跟并发用户数没有多大关系。**系统最大的TPS是一定的，就好比池塘里装的水是有限的。但是并发用户数不一定，可以通过减小思考时间来增大并发用户数。**
- 新系统：没有历史数据参考，只能通过业务部门来评估；
  80-20：百分之八十的事物是在百分之二十的时间内完成的。（只知道系统注册用户数 or 在线用户数的时候可选择）
- 旧系统：最好通过日志分析来得出
- 可以选取峰值时刻，在一定时间内（单位：秒）使用系统的人数，并发用户数取 10%，
  之后可根据实际情况梯阶式增加

#### 前端性能测试

https://www.cnblogs.com/lei2007/archive/2013/08/16/3262897.html

### 地铁站模型

**地铁模型分析**

和绝大部分人一样，小白每天都要乘坐地铁上下班，那么就拿地铁来分析，再次深刻理解下性能。早上乘坐地铁上班，最典型的就是深圳地铁1、3、4号线等，人多得简直没法形容！为了方便理解分析，先做如下假设。

某地铁站进站只有3个刷卡机。

人少的情况下，每位乘客很快就可以刷卡进站，假设进站需要1s。

乘客耐心有限，如果等待超过30min，就会暴躁、唠叨，甚至选择放弃。

**按照上述的假设，最初会出现如下的场景。**

- 场景一：只有1名乘客进站时，这名乘客可以在1s的时间内完成进站，且只利用了一台刷卡机，剩余2台等待着。
- 场景二：只有2名乘客进站时，2名乘客仍都可以在1s的时间内完成进站，且利用了2台刷卡机，剩余1台等待着。
- 场景三：只有3名乘客进站时，3名乘客还能在1s的时间内完成进站，且利用了3台刷卡机，资源得到充分利用。

想到这里，小白越来越觉得有意思了，原来技术与生活这么息息相关，真的可以快乐学习哦。随着上班高峰的到来，乘客也越来越多，新的场景也慢慢出现了。

- 场景四：`A、B、C`三名乘客进站，同时`D、E、F`乘客也要进站，因为A、B、C先到，所以D、E、F乘客需要排队，等A、B、C三名乘客进站完成后才行。那么，A、B、C乘客进站时间为1s，而D、E、F乘客必须等待1s，所以他们3位在进站的时间是2s。

 通过上面这个场景可以发现，每秒能使3名乘客进站，第1s是A、B、C，第2s是D、E、F，但是对于乘客D、E、F来说，“响应时间”延长了。

- 场景五：假设这次进站一次来了9名乘客，根据上面的场景，不难推断出，这9名乘客中有3名的“响应时间”为1s，有3名的“响应时间”为2s（等待1s+进站1s），还有3名的“响应时间”为3s（等待2s+进站1s）。

- 场景六：假设这次进站一次来了10名乘客，根据上面的推算，必然存在1名乘客的“响应时间”为4s，如果随着大量的人流涌入进站，可想而知就会达到乘客的忍耐极限。

- 场景七：如果地铁正好在火车站，例如，著名的北京西站、北京站。每名乘客都拿着大小不同的包，有的乘客拿的包太大导致卡在刷卡机那（堵塞），这样每名乘客的进站时间就会又不一样。

小白突然想到，貌似很多地铁进站的刷卡机有加宽的和正常宽度的两种类型，那么拿大包的乘客可以通过加宽的刷卡机快速进站（增加带宽），这样就能避免场景七中的现象。

- 场景八：进站的乘客越来越多，3台刷卡机已经无法满足需求，于是为了减少人流的积压，需要再多开几个刷卡机，增加进站的人流与速度（提升TPS、增大连接数）。

- 场景九：终于到了上班高峰时间了，乘客数量上升太快，现有的进站措施已经无法满足，越来越多的人开始抱怨、拥挤，情况越来越糟。单单增加刷卡机已经不行了，此时的乘客就相当于“请求”，乘客不是在地铁进站排队，就是在站台排队等车，已经造成严重的“堵塞”，那么增加发车频率（加快应用、数据库的处理速度）、增加车厢数量（增加内存、增大吞吐量）、增加线路（增加服务的线程）、限流、分流等多种措施便应需而生。

分析到这里，小白可以熟练地把性能指标与场景结合运用起来了，初步学习成果还是不错的。

###  jemter阶梯式压测-stepping

性能测试中，有时需要模拟一种实际生产中经常出现的情况，即：**从某个值开始不断增加压力，直至达到某个值，或者使用快增长或者慢增长模式增加并发**，然后持续运行一段时间。一般持续运行的时间是10-20分钟。

在`jmeter`中，有这样一个插件，可以帮我们实现这个功能，这个插件就是：`Stepping Thread Group`。这个插件类似于`LoadRunner`中的 `Controller`，作用就是模拟实际的生产情况，不断对服务器施加压力，直至到某个值，然后持续运行一段时间。

 `stepping thread group`：下载链接：https://jmeter-plugins.org/downloads/old/

![image-20200915193823532](../../../Testing_learn/typora/performanceTesting.assets/image-20200915193823532.png)

 下载后，解压，将`JMeterPlugins-Standard.jar`包放在`jmeter`安装目录的`lib\ext`下，然后重新启动`jmeter`。

在测试计划下添加`stepping thread group`：

![image-20200915194036742](../../../Testing_learn/typora/performanceTesting.assets/image-20200915194036742.png)

可设置参数如下：

![image-20200915194224657](../../../Testing_learn/typora/performanceTesting.assets/image-20200915194224657.png)

参数设置解释：

`This group will start 100 threads`：设置线程组启动的线程总数为`100`个；

`First,wait for N seconds`：启动第一个线程之前，需要等待`N`秒；

`Then start N threads`：设置最开始时启动`N`个线程；

`Next,add 10 threads every 30 seconds,using ramp-up 5 seconds`：每隔`30`秒，在`5`秒内启动`10`个线程；

`Then hold load for 60 seconds`：启动的线程总数达到最大值之后，再持续运行`60`秒；

`Finally,stop 5 threads every 1 seconds`：每秒停止`5`个线程；

### 使用stepping group进行阶梯压力测试

#### windows进行阶梯压力测试

在`NonGUI`模式下运行`jmeter`，并进行阶梯压力测试，步骤如下：

##### 第一步：安装插件	

保证`stepping thread group`插件已经安装成功

##### 第二步：创建并保存jmx脚本

2、打开`jmeter`，创建好`stepping thread group`后并设置好参数后，随便设置一些请求，并保存好`jmeter`脚本，如下：

![image-20200915194756906](../../../Testing_learn/typora/performanceTesting.assets/image-20200915194756906.png)

这里要注意的是：

1. 保存的`jmeter`脚本尽量不要使用中文名命名

##### 第三步：cmd运行jmx脚本

打开`cmd`，使用`jmeter`的`NONGUI`运行`jmx`脚本：

```sh
jmeter -n -t E:\jmeter_test\login_xiaoqiang.jmx -l E:\testcases\script\test.jtl -e -o E:\testcases\report2

E:\testcases\script\test.jtl不需要存在
E:\testcases\report2也不需要存在
```

![image-20200915200242137](../../../Testing_learn/typora/performanceTesting.assets/image-20200915200242137.png)

运行后，界面如下：

![image-20200915200341450](../../../Testing_learn/typora/performanceTesting.assets/image-20200915200341450.png)

运行完成后，生成了`report2`目录：

![image-20200915200430701](../../../Testing_learn/typora/performanceTesting.assets/image-20200915200430701.png)

打开`index.html`文件，查看测试报告信息：

![image-20200915200512342](../../../Testing_learn/typora/performanceTesting.assets/image-20200915200512342.png)



#### Linux下进行JMeter压测

在`Linux`下进行`JMeter`压测，把`Linux`作为压力机。

##### 第一步：安装jdk和jmeter

确保安装好了`JDK`和`Jmeter`并配置好了环境变量

`linux`上安装`jmeter`方法：

1、直接将`windows`上的`jmeter`的`zip`安装包上传到`linux`，解压（最好上传解压到`/usr/local`目录下）

2、配置环境变量，`vi /etc/profile`，并使其生效`source /etc/profile`

![image-20200915210114473](../../../Testing_learn/typora/performanceTesting.assets/image-20200915210114473.png)

3、 查看是否安装成功，执行命令：`jmeter -v`

![image-20200915210352456](../../../Testing_learn/typora/performanceTesting.assets/image-20200915210352456.png)

##### 第二步：创建目录并上传jmx脚本

创建目录：

```sh
cd /mendao/install
mkdir jmeter_store
cd jmeter_store
mkdir jmx   存放脚本的路径
mkdir report  测试运行生成的报告的存放路径
```

把`windows`调好的脚本上传到刚建立的`jmx`目录，**注意`windows`上`jmeter`版本要和`linux`上的`jmeter`版本一致。**

如果有使用`csv`参数化，也要把`csv`文件上传到`jmx`目录，且要进行`csv`参数化的话，注意要写相对路径，直接写文件名即可（前提是`csv`文档和脚本放在同一个目录下）。

![image-20200915211057644](../../../Testing_learn/typora/performanceTesting.assets/image-20200915211057644.png)

可能出现的问题：

（1）`linux`环境`jmeter`与`win`环境编写脚本的`jmeter`版本不一致，版本改为一致。

（2）脚本中存在中文，去除中文。

（3）脚本中存在类似于`jp@gc - Active Threads Over Time` 监听器，去除监听器（查看结果树和聚合报告可以保留）

（4）记得`windows`中`jmeter`已安装的插件也要传到`linux`的`jmeter`安装目录的`lib\ext`中，如`JMeterPlugins-Standard.jar`包：

![image-20200915211224475](../../../Testing_learn/typora/performanceTesting.assets/image-20200915211224475.png)

#####  第三步：执行jmx脚本

```sh
jmeter -n -t /mendao/install/jmeter_store/jmx/login_xiaoqiang.jmx -l /mendao/install/jmeter_store/result/test915.jtl -e -o /mendao/install/jmeter_store/result/

-n:表示要使用nongui形式运行jmx脚本
-t：表示要运行的脚本
-l: 表示要生成的jtl路径，此jtl文件必须事先不存在
-e：表示要生成html报告，如果不写就不会生成
-o: 表示要生成的html报告目录,此目录可存在可不存在
```

![image-20200915211409942](../../../Testing_learn/typora/performanceTesting.assets/image-20200915211409942.png)

 查看结果：

![image-20200915211704215](../../../Testing_learn/typora/performanceTesting.assets/image-20200915211704215.png)

 把`report`整个文件夹传到`windows`上，打开即可，使用`sz`命令可以上传

![image-20200915211805110](../../../Testing_learn/typora/performanceTesting.assets/image-20200915211805110.png)

### Linux监控

`Linux CPU`概念

- 物理`CPU`：主板上实际接入的`CPU`个数，可用数`physical id`来确定。
- `CPU`核数：每个`CPU`上面实际接入的芯片组数量，如双核、四核等。
- 逻辑`CPU`：一般情况下，**逻辑`CPU` = 物理CPU数量 * `CPU`核数**，如果逻辑`cpu`多于物理`cpu`，说明该`cpu`支持超线程技术（简单来说，它可使处理器中的`1` 颗内核如`2` 颗内核那样在操作系统中发挥作用。这样一来，操作系统可使用的执行资源扩大了一倍，大幅提高了系统的整体性能，此时逻辑`cpu=`物理`CPU`个数×每颗核数`x2`）。

#### 查看`cpu`的详细信息

`cat /proc/cpuinfo`

![image-20200916101156180](../../../Testing_learn/typora/performanceTesting.assets/image-20200916101156180.png)

#### 查看内存信息

`cat /proc/meminfo`

```sh
cat /proc/meminfo
MemTotal:        2052440 kB //总内存
MemFree:           50004 kB //空闲内存
Buffers:           19976 kB //给文件的缓冲大小
Cached:           436412 kB //高速缓冲存储器(http://baike.baidu.com/view/496990.htm)使用的大小
SwapCached:        19864 kB //被高速缓冲存储用的交换空间大小
Active:          1144512 kB //活跃使用中的高速缓冲存储器页面文件大小
Inactive:         732788 kB //不经常使用的高速缓冲存储器页面文件大小
Active(anon):     987640 kB //anon：不久
Inactive(anon):   572512 kB
Active(file):     156872 kB
Inactive(file):   160276 kB
Unevictable:           8 kB
Mlocked:               8 kB
HighTotal:       1177160 kB //The total and free amount of memory, in kilobytes, that is not directly mapped into kernel space.
HighFree:           7396 kB // The HighTotal value can vary based on the type of kernel used.
LowTotal:         875280 kB // The total and free amount of memory, in kilobytes, that is directly mapped into kernel space.  used.
LowFree:           42608 kB //The LowTotal value can vary based on the type of kernel
SwapTotal:        489940 kB //交换空间总大小
SwapFree:         450328 kB //空闲交换空间
Dirty:               104 kB //等待被写回到磁盘的大小
Writeback:             0 kB //正在被写回的大小
AnonPages:       1408256 kB //未映射的页的大小
Mapped:           131964 kB //设备和文件映射的大小
Slab:              37368 kB //内核数据结构缓存的大小，可减少申请和释放内存带来的消耗
SReclaimable:      14164 kB //可收回slab的大小
SUnreclaim:        23204 kB //不可收回的slab的大小23204+14164=37368
PageTables:        13308 kB //管理内存分页的索引表的大小
NFS_Unstable:          0 kB //不稳定页表的大小
Bounce:                0 kB //bounce:退回
WritebackTmp:          0 kB //
CommitLimit:     1516160 kB
Committed_AS:    2511900 kB
VmallocTotal:     122880 kB //虚拟内存大小
VmallocUsed:       28688 kB //已经被使用的虚拟内存大小
VmallocChunk:      92204 kB
HugePages_Total:       0 //大页面的分配
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       10232 kB
DirectMap2M:      899072 kB
```

`io` : 输入输出，与硬盘或者磁盘交互的状态，比如读写磁盘 ，`load` ：平均负载，`Network` ：网络， 这几个指标之间的关系是相互依赖的，任何一个高负载都会导致其他指标出现问题，比如：

![image-20200916101617326](../../../Testing_learn/typora/performanceTesting.assets/image-20200916101617326.png)     

所以：在分析的时候要尝试从多个入口去分析

#### `CPU`的两个组成部分

1、`user time` : 非内核操作消耗cpu的时间，是用户或者程序所占用的时间。

2、`sys time` ：内核操作消耗cpu的时间，即操作系统消耗的时间。这个值越大，那么整个系统的性能就会越差，它能反映系统本身的情况。

小结：

- 比如发现`sys time`很高，那应该想到可能是linux操作系统本身的问题，可能需要去调一些内核的参数，提升系统性能；
- 如果`sys time`不高，`user time`很高的话，需要去看一下是不是程序代码的问题或者配置文件的问题

`Cpu`参考：

- 好： `user% + sys% < 70%`
- 一般：`user% + sys% = 80%`
- 糟糕：`user% + sys% >=90%`

####  load average

`load average`：`cpu`使用队列长度的统计信息，指一段时间内`cpu`**正在处理，等待处理的进程数之和**的统计信息。

理想的`load average < cpu`个数 `*` 核数`*0.7`

查询`cpu`个数的命令：

```sh
grep 'physical id' /proc/cpuinfo | sort -u
```

查询核数的命令：

```sh
grep 'core id' /proc/cpuinfo | sort -u | wc -l 
```

####  监控工具

#####  top

  `top`命令能够**实时监控**系统的运行状态，可以按照`cpu`、内存和执行时间排序，通过交互式命令设定显示。

![image-20200916145055684](../../../Testing_learn/typora/performanceTesting.assets/image-20200916145055684.png)

![image-20200916142622996](../../../Testing_learn/typora/performanceTesting.assets/image-20200916142622996.png)

**第一行，任务队列信息，同 uptime 命令的执行结果**

> 系统时间：`14:14:48`
>
> 运行时间：`up 4:10 min,`
>
> 当前登录用户： `2 user`
>
> 负载均衡(`uptime`) `load average: 0.00, 0.01, 0.05`
>
> `average`后面的三个数分别是`1`分钟、`5`分钟、`15`分钟的负载情况。
>
> `load average`数据是**每隔`5`秒钟检查一次活跃的进程数，然后按特定算法计算出的数值**。如果这个数除以逻辑`CPU`的数量，结果高于`5`的时候就表明系统在超负荷运转了

**第二行，Tasks — 任务（进程）**

> 总进程:105 `total`, 运行:1 `running`, 休眠:104 `sleeping`, 停止: 0 `stopped`, 僵尸进程: 0 `zombie`

**第三行，cpu状态信息**

> `%id` ：空闲cpu的百分比
>
> `%wa` ：`wa`相当于`wait`嘛，就是等待输出输入的`cpu`百分比

**第四行,内存状态**

**第五行，swap交换分区信息**

> 2031612k total,   536k used, 2031076k free,  505864k cached【缓冲的交换区总量】

> 备注：
>
> 可用内存=`free + buffer + cached`
>
> 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。
>
> 第四行中使用中的内存总量（`used`）指的是现在系统内核控制的内存数，
>
> 第四行中空闲内存总量（`free`）是内核还未纳入其管控范围的数量。
>
> 纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到`free`中去，因此在`linux`上`free`内存会越来越少，但不用为此担心。

**第六行，空行**

**第七行以下：各进程（任务）的状态监控**

> `PID` : 进程id
>
> `USER` : 进程所有者
>
> `PR` : 进程优先级
>
> `NI` : nice值。负值表示高优先级，正值表示低优先级
>
> `VIRT` : 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES
>
> `RES` : 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA
>
> `SHR` : 共享内存大小，单位kb
>
> `S` :进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程
>
> `%CPU` : 上次更新到现在的CPU时间占用百分比
>
> `%MEM` : 进程使用的物理内存百分比
>
> `TIME+` : 进程使用的CPU时间总计，单位1/100秒
>
> `COMMAND` : 进程名称（命令名/命令行）`

`swap` ：交换区总量，相当于`windows`下的虚拟内存，唯一和`windows`不同的是`windows`虚拟内存可以与内存同时使用，但是`swap`是物理内存使用光之后才使用`swap`空间（系统会把物理内存里的访问频率低的内存对象引动到`swap`里，再在物理内存里产生新的连接指向`swap`里的那个对象）。

判断内存够不够的最直接简单的方法就是看内存的使用情况，当物理内存逐渐降低，swap逐渐被占用，那么说明内存已经不够了或者是出现了内存溢出、内存泄漏。

`cache` ：高速缓存，是位于`cpu`与主内存间的一种容量较小但速度很高的存储器。由于`cpu`的速度远高于主内存，`cpu`直接从内存中存储数据要等待一定时间周期，`cache`中保存着`cpu`刚用过或者循环使用到的一部分数据，当`cpu`再次使用该部分数据时可从`cache`中直接调用，这样就减少了`cpu`的等待时间，提高了系统的效率

`buffer` ：缓冲区，一个用于存储速度不同步的设备或者优先级不同的设备之间传输数据的区域。

& 两者相同点：都是缓存

& 两者不同点：存在的地方不一样，`cache`存在`cpu`与主内存之间，`buffer`存在于各个设备之间的缓存。

**Linux内存管理机制**

`linux`有自己的内存管理机制。`Linux`会尽可能的使用内存来提升`IO`效率，细节不必探究。

如果系统的`free`不够用，达到触发机制后，系统会自动释放`cache`和`buffer`的内存共程序使用（`cache`和`buffer`是由内核进行动态管理的）

如果`used`很多，而`cache`和`buffer`所占比率很小，那说明可能内存不够用了。不能单看`free`的大小来判断，所以你可以简单理解为`cache`和`buffer`也是`free`的一部分。

`cat /proc/meminfo`

（可用的`memory=free memory + buffers + cached`）

`swap`是磁盘上开辟的虚拟内存，所以他的变化有可能导致与`IO`的交互也会增加。

 **如何看多核:**

 对于多核的`cpu`来说，`cpu 0`是相当关键的，因为`cpu`各核间的调度都是通过`cpu 0`来完成的，`cpu0`的负载高就会影响其他核的性能。

`windows`下：任务管理器 > 某个进程 > 设置相关性

![image-20200916205942606](../../../Testing_learn/typora/performanceTesting.assets/image-20200916205942606.png)

要某个程序运行在哪个核上就在哪个核前面打钩，然后点击确定。

`linux`下：利用`taskset`命令，设置并限制某个程序能被运行在哪个核上

![image-20200916205906225](../../../Testing_learn/typora/performanceTesting.assets/image-20200916205906225.png)

上图代表对`cpu1 3 5 7`的各自进程进行限定。

##### Vmstat

 `Vmstat`是一款功能较齐全的性能监控工具，它是‘虚拟内存统计’的英文缩写，可以对操作系统的内存信息、进程状态、`cpu`活动、磁盘等信息进程监控，不足之处是无法对某个进程进行深入分析。

 ![image-20200916145337017](../../../Testing_learn/typora/performanceTesting.assets/image-20200916145337017.png)

 `Vmstat`输出字段说明：

**（1）、`procs`区**

`r` ：表示等待和运行的队列，如果这个值过大，说明`cpu`可能比较忙，使用率高，需要增加`cpu`个数。

`b` ：堵塞并等待`io`的进程数，如正在等待`I/O`或者内存交换等。

 **（2）、`memory`区域**

`swpd` ：表示切换到内存交换区的内存大小（单位为`kb`），通俗讲就是虚拟内存的大小。

`free` ：表示当前空闲的物理内存（单位`kb`）

`buff` ：就是缓冲的大小，一般对设备的读写才需要缓冲，`IO`之间。

`cache` ：被用来作为高速缓存，一般对系统文件进行缓冲，频繁访问的文件都会被缓存，如果`cache`值比较大，则说明缓存文件比较多，如果此时`bi`比较小，说明系统效率较高。

**（3）、swap区**

`si` ：从磁盘分页到内存的数量，也就是由内存进入内存交换区的内存大小。

`so`：从内存分页到磁盘的数量，也就是由内存交换区进入内存的内存大小。

这两列表示内存交换的频繁程度，一般情况下，si、so的值都是0，如果值长期不是0，数值较大，则说明系统内存不足。

**（4）、I/O区**

bi ：读磁盘

bo ：写磁盘

**（5）I/O瓶颈**

`Cpu` 区域下的`wa`（等待`cpu`的百分比）值过大，`I/O`等待越严重（参考值 超过`20%`）

`Bo + bi`过大（参考值 超过`2000`），如果`bi + bo`的值过大，而且`cpu`区域`wa`值较大，则表示系统磁盘I/O瓶颈，应该提高磁盘的读写性能，或者减少应用程序不必要的磁盘读写。

 `vmstat`参数的使用：

![image-20200916145741923](../../../Testing_learn/typora/performanceTesting.assets/image-20200916145741923.png)

 `vmstat 5 5`表示运行`5`次，每次`5`秒

小结：`vmstat`使用场景

![image-20200916150039421](../../../Testing_learn/typora/performanceTesting.assets/image-20200916150039421.png)

 

#####  sysstat工具包

安装`sysstat`：

```sh
yum -y install sysstat
```

###### 使用`mpstat`

![image-20200916155205805](../../../Testing_learn/typora/performanceTesting.assets/image-20200916155205805.png)

`mpstat`参数的使用，如下：

`10`表示每`10`秒监控一次；`20`表示监控`20`次。`-P和ALL`必须大写

![image-20200916155224492](../../../Testing_learn/typora/performanceTesting.assets/image-20200916155224492.png)

###### 使用iostat

 `Iostat`是`I/O statistics`（磁盘输入/输出统计）的缩写，主要是对系统的磁盘`I/O`操作进行监控，它的输出主要显示磁盘读写操作的统计信息，同时给出`cpu`的使用情况。

 直接输入`iostat:`

![image-20200916210213158](../../../Testing_learn/typora/performanceTesting.assets/image-20200916210213158.png)

输入`iostat -x` ，得出的信息更加详细:

![image-20200916210246694](../../../Testing_learn/typora/performanceTesting.assets/image-20200916210246694.png)

- `rrqm/s` ：每秒这个设备相关的读取请求有多少被合并（请求相同`block`时，请求合并）
- `wrqm/s` ：每秒这个设备相关的写入请求有多少被合并
- `r/s` ：每秒读取请求数（`rio`）
- `w/s` ：每秒写入请求数（`wio`）
- `R/s + w/s`：即为每秒总的`IO`操作次数
- `resc/s` ：每秒读扇区数
- `wsec/s` ：每秒写扇区数
- `rkB/s` ：每秒读取数据量，单位`K`字节
- `avgqu - sz` ：平均`IO`队列长度
- `await` ：平均每次设备`IO`操作的等待时间（毫秒）
- `svctm` ：平均每次设备`IO`操作的服务时间（毫秒），`svctm`越接近于`await`则说明等待时间少。
- `%util` ：表示了设备的繁忙程度，`80%`表示设备已经很忙了。

 IO瓶颈判断：

![image-20200916210653625](../../../Testing_learn/typora/performanceTesting.assets/image-20200916210653625.png)

######   使用glances

 如果是`centos7`系统，可能要事先安装一下：`yum -y install glances`

```sh
glances
```

![image-20200916214108738](../../../Testing_learn/typora/performanceTesting.assets/image-20200916214108738.png)

`tx`是发送（`transport`），`rx`是接收(`receive`)

外网：一般`RX`指的是下载，`TX`指的是上传

内网恰恰相反

### 性能测试的指标预估

一般我们应该和功能测试一样，会有一个预期指标，然后和实际结果做对比的。

 当然，如果没有做过性能测试的公司可能就会比较迷茫，那么你可以建议他们从运维系统上获取数据作为参考

 但是，如果你们连运维系统都木有，那咋办呢。也好办，你自己做一次基准的性能测试得出数据，然后以后可以在这个数据基础上慢慢进行优化，逐步积累

举个例子：

> 新浪邮箱去年全年处理邮件约 `100` 万条，考虑到 `3` 年后可能递增到每年 `200`万条。
>
> 假设每年处理量集中在 `8` 个月，每个月 `20` 个工作日，每个工作日 `8` 小时，试采用 `80～20` 原理估算系统服务器高峰期的处理能力应达到怎样的一个神马水平？（计算他的TPS）
>
> 参考答案
>
> • `200万/8=25万/月`
>
> • `25万/20=1.25万/日`
>
> • `1.25万*80%/(8*20%*3600)`=`1.74TPS`

预估并发数

新系统：没有历史数据作参考，只能通过业务部门进行评估。

旧系统：对于已经上线的系统，可以选取峰值时刻，在一定时间内（单位：秒）使用系统的人数，并发用户数取`10%`，之后可根据实际情况梯阶式增加

峰值`PV/s`（通过日志分析系统`or`线上监控系统得知）

[曲线拐点模型分析](https://www.cnblogs.com/lily1989/p/8579798.html)

对于初学者来说，培养观察与分析思想是很重要的，首先来看一张典型的曲线拐点模型图

![img](../../../Testing_learn/typora/performanceTesting.assets/clip_image002-1600263898385.jpg)

 

分析图1-2最好是先看一个个指标，然后再综合分析，这样的步骤更容易理解，思路也更加清晰明了。接下来就和小白一起来分析吧，分析思路如下。

1）`X`轴代表并发用户数，`Y`轴代表资源利用率、吞吐量、响应时间。`X`轴与`Y`轴区域从左往右分别是轻压力区、重压力区、拐点区。

2）然后一个个分析，根据前面学习的性能术语与指标进行理解，**随着并发用户数的增加，在轻压力区的响应时间变化不大，比较平缓，进入重压力区后呈现增长的趋势，最后进入拐点区后倾斜率增大，响应时间急剧增加**。

3）接着看吞吐量，**随着并发用户数的增加，吞吐量增加，进入重压力区后逐步平稳，到达拐点区后急剧下降，说明系统已经达到了处理极限，有点要扛不住的感觉。**

4）同理，随着并发用户数的增加，资源利用率逐步上升，最后达到饱和状态。

5）最后，把所有指标融合到一起来分析，随着并发用户数的增加，吞吐量与资源利用率增加，说明系统在积极处理，所以响应时间增加得并不明显，处于比较好的状态。但随着并发用户数的持续增加，压力也在持续加大，吞吐量与资源利用率都达到了饱和，随后吞吐量急剧下降，造成响应时间急剧增长。**轻压力区与重压力区的交界点是系统的最佳并发用户数，因为各种资源都利用充分，响应也很快；**而**重压力区与拐点区的交界点就是系统的最大并发用户数，因为超过这个点，系统性能将会急剧下降甚至崩溃。**

分析到这里，小白终于找到点成就感了，同时也庆幸自己没有忽略基础，看来基础对于日后的学习有着重要意义！

###  性能测试怎么做

```lua
概述一下性能测试流程？
1.分析性能需求。挑选用户使用最频繁的场景来测试。确定性能指标，比如：事务通过率为100%，TOP99%是5秒，最大并发用户为500人，CPU和内存的使用率在70%以下
2.制定性能测试计划，明确测试时间(通常在功能稳定后，如第一轮测试后进行)和测试环境和测试工具
3.编写测试用例
4.搭建测试环境，准备好测试数据
5.编写性能测试脚本
6.性能测试脚本调优。设置检查点（断言）、参数化、关联、集合点（阶梯式压测里面的快增长）、事务，调整思考时间，删除冗余脚本
7.设计测试场景，运行测试脚本，监控服务器
8.分析测试结果，收集相关的日志提单给开发
9.回归性能测试
10.编写测试报告
 
如何确定系统最大负载？
通过负载测试，不断增加用户数，随着用户数的增加，各项性能指标也会相应产生变化，当出现了性能拐点，比如，当用户数达到某个数量级时，响应时间突然增长，那么这个拐点处对应的用户数就是系统能承载的最大用户数

你们系统哪些地方(哪些功能)做了性能测试？
选用了用户使用最频繁的功能来做测试，比如：登陆，搜索，提交订单
 
你们的并发用户数是怎么确定的？
1）会先上线一段时间，根据收集到的用户访问数据进行预估
2）根据需求来确定（使用高峰时间段，注册用户数，单次响应时间等
 
你们性能测试在什么环境执行？
参考答案：我们会搭建一套独立的性能测试环境进行测试
 
你们性能测试什么时间执行？
基准测试：功能测试之后，系统比较稳定的时候再做。
负载测试：夜深人静，系统没人用的时候
 
怎么分析性能测试结果？
首先查看事物通过率，然后分析其他性能指标，比如，确认响应时间，事务通过率，CPU等指标是否满足需求；如果测试结果不可信，要分析异常的原因，修改后重新测试。
在确定性能测试结果可信后，如果发现以下问题，按下面的思路来定位问题

问题一：响应时间不达标
查看事务所消耗的时间主要在网络传输还是服务器，如果是网络，就结合Throughput(网络吞吐量)图，计算带宽是否存在瓶颈，如果存在瓶颈，就要考虑增加带宽，或对数据的传输进行压缩处理；如果不存在瓶颈，那么，可能是网路不稳定导致。如果主要时间是消耗在服务器上，就要分别查看web服务器和数据库服务器的CPU，内存的使用率是否过高，因为过高的CPU，内存必定会造成响应时间过长，如果是web服务器的问题，就把web服务器对应上对应的用户操作日志取下来，发给开发定位；如果是数据库的问题，就把数据库服务器对应上对应的日志取下来，发给开发定位。
 
问题二：服务器CPU指标异常
分析思路：就把web服务器对应上对应的用户操作日志取下来，发给开发定位。

问题三：数据库CPU指标异常
分析思路：把数据库服务器对应上对应的日志取下来，发给开发定位。

问题四：内存泄漏
分析思路：把内存的heap数据取出来，分析是哪个对象消耗内存最多，然后发给开发定位。

问题五：程序在单用户场景下运行成功，多用户运行则失败，提示连不上服务器。
原因：程序可能是单线程处理机制
 
如何识别系统瓶颈？
从TPS指标分析，TPS即系统单位时间内处理事务的数量。观察当前随着用户数的增长期系统每秒可处理的事务数是否也会增长
 
如何判断系统的性能是变好了还是变坏了
通过基准测试对比性能指标
 
 
你们的性能测试需求哪里来？
1：客户提供需求
2：运维提供需求
3：开发提供需求
 
如何实现200用户的并发？
在脚本对应的请求后添加集合点
 
 
什么情况下要做关联，关联是怎么做的？
当脚本的上下文有联系，就用关联。
比如登录的token关联，增删改查主键id关联
 
有验证码的功能，怎么做性能测试？
1、将验证码暂时屏蔽，完成性能测试后，再恢复
2、使用万能的验证码
 
 
你们性能测试做的是前台还是后台？
BS项目：测试的是后台服务器的性能和浏览器端性能；
APP项目：手机端和服务器端的性能都做
（客户端性能：比如，使用GZIP压缩；一个页面请求不宜过多）
 
性能测试指标有哪些
响应时间
吞吐量（tps）
cpu
内存
io
disk
 
 
如何脚本增强？
1、做参数化
2、做关联
3、添加事务
4、添加断言
5、添加集合点
6、添加思考时间


```

### 性能测试常见问题

```lua
1、cpu高导致响应时间长
用jmeter做性能测试的时候，遇到一个响应时间长的性能问题.
top命令一看，负载高，用户cpu将近100%，cpu已经达到性能瓶颈了，shift+p，或者键盘大写状态下按P，将所有进程按cpu使用率从高到低排序，这样，我们关注消耗cpu最多的进程即可。
数据库没有加索引，导致查询时间长，cpu持续增加。

2、TPS波动较大或者并发情况下大量报错。
原因一般有网络波动、其他服务资源竞争等
性能测试环境一般都是在内网或者压测机和服务在同一网段，可通过监控网络的出入流量来排查；
其他服务资源竞争也可能造成这一问题，可以通过Top命令或服务梳理方式来排查在压测时是否有其他服务运行导致资源竞争；
调优方案：
网络波动问题，可以让运维同事协助解决（比如切换网段或选择内网压测），或者等到网络较为稳定时候进行压测验证；
资源竞争问题：通过命令监控和服务梳理，找出压测时正在运行的其他服务，通过沟通协调停止该服务（或者换个没资源竞争的服务节点重新压测也可以）；
线程池问题：修改服务节点中容器的server.xml文件中的配置参数，主要修改如下几个参数：
 
在tomcat的安装目录conf目录下，打开server.xml文件
 


3、TPS偏小
1、网络带宽
在压力测试中，有时候要模拟大量的用户请求，如果单位时间内传递的数据包过大，超过了带宽的传输能力，那么就会造成网络资源竞争，间接导致服务端接收到的请求数达不到服务端的处理能力上限。
2、硬件资源
包括CPU（配置、使用率等）、内存（占用率等）、磁盘（I/O、页交换等）。
3、数据库配置
	高并发情况下，如果请求数据需要写入数据库，且需要写入多个表的时候，如果数据库的最大连	接数不够，或者写入数据的SQL没有索引没有绑定变量，抑或没有主从分离、读写分离等，就	会导致数据库事务处理过慢，影响到TPS。

4、压力机
比如jmeter，单机负载能力有限，如果需要模拟的用户请求数超过其负载极限，也会间接影响TPS（这个时候就需要进行分布式压测来解决其单机负载的问题）。
5、业务逻辑
业务解耦度较低，较为复杂，整个事务处理线被拉长导致的问题。

6、对于性能测试，如果一个查询比较慢不能达到预期响应指标数据，你怎么分析瓶颈在哪
通常查询涉及到数据库的操作，所以我们可以从以下几个方面入手数据库服务器的cpu、磁盘情况，对于常用查询是否做了缓存或者视图，数据库字段索引是否合理，sql语句是否合理；当然了，还要检查是否是网络的问题，或者是前端页面展示的问题。


4.你在性能测试中遇到哪些性能问题？
    响应时间突然变长：检查tomcat连接数
    tps毛刺过多：ramp up值过高，压力过大
    tps突然出现断崖式下跌：防火墙拦截，tcp连接中断
    进程无缘无故失踪：swap空间耗尽，触发了oomkiller
```



### 案例1：确定系统最大负载

本案例对小强系统的登录等步骤进行负载测试，初步确定将测试在80并发、100并发、120并发、

#### 第一步：在jmeter调试好脚本

### CPU高的问题的定位

软件测试遇到功能问题，通过日志，单步调试相对比较好定位，而若是性能问题，例如线上服务器`CPU100%`，如何找到相关服务，如何定位问题代码，更考验技术人的功底。

题目：某服务器上部署了若干tomcat实例，即若干垂直切分的Java站点服务，以及若干Java微服务，突然收到运维的CPU异常告警。

问：如何定位是哪个服务进程导致CPU过载，哪个线程导致CPU过载，哪段代码导致CPU过载？

### 在一台机器实现多个tomcat的负载均衡

#### nginx介绍

##### 前言

之前我们搭建网站的时候，把`war`包放到`tomcat`下就能运行起来了，为什么部署上线的时候，又用到了`nginx`呢？

**`nginx`可以做多台服务器的负载均衡**，当用户非常少的时候，可以用一台服务直接部署`web`环境，那么当用户达到百万级别，千万级别的时候，就需要增加服务器，多台服务器又如何管理协作的呢？

`nginx`有以下功能：

1.静态`HTTP`服务器——`Nginx`是一个`HTTP`服务器，可以将服务器上的静态文件（如`HTML`、图片）通过`HTTP`协议展现给客户端。

2.反向代理服务器——客户端本来可以直接通过`HTTP`协议访问某网站应用服务器，网站管理员可以在中间加上一个`Nginx`，客户端请求`Nginx`，`Nginx`请求应用服务器，然后将结果返回给客户端，此时`Nginx`就是反向代理服务器。

3.负载均衡——当网站访问量非常大，网站站长开心赚钱的同时，也摊上事儿了。因为网站越来越慢，一台服务器已经不够用了。于是将同一个应用部署在多台服务器上，将大量用户的请求分配给多台机器处理。同时带来的好处是，其中一台服务器万一挂了，只要还有其他服务器正常运行，就不会影响用户使用。

4.虚拟主机——有的网站访问量大，需要负载均衡。然而并不是所有网站都如此出色，有的网站，由于访问量太小，需要节省成本，将多个网站部署在同一台服务器上。

5.`FastCGI`——`Nginx`本身不支持`PHP`等语言，但是它可以通过`FastCGI`来将请求扔给某些语言或框架处理（例如`PHP`、`Python`、`Perl`）。

##### 什么是nginx?

`Nginx`是一款自由的、开源的、高性能的`HTTP`服务器和反向代理服务器；同时也是一个`IMAP`、`POP3`、`SMTP`代理服务器；

`Nginx`可以作为一个`HTTP`服务器进行网站的发布处理，另外`Nginx`可以作为反向代理进行负载均衡的实现。

**正向代理**，代理的是客户端，隐藏了客户端的信息。

![image-20200917183359543](../../../Testing_learn/typora/performanceTesting.assets/image-20200917183359543.png)

**反向代理**，它代理的是服务端，主要用于服务器集群分布式部署的情况下，反向代理隐藏了服务器的信息。

![image-20200917183352380](../../../Testing_learn/typora/performanceTesting.assets/image-20200917183352380.png)

#### nginx安装

先安装依赖包

1.`gcc`安装：安装 `nginx` 需要先将官网下载的源码进行编译，编译依赖 `gcc` 环境，如果没有 `gcc` 环境，则需要安装

2.`PCRE(Perl Compatible Regular Expressions)` 是一个`Perl`库，包括 `perl` 兼容的正则表达式库。`nginx` 的 `http` 模块使用 `pcre` 来解析正则表达式，所以需要在 `linux` 上安装 `pcre` 库，`pcre-devel` 是使用 `pcre` 开发的一个二次开发库。`nginx`也需要此库

3.`zlib`库提供了很多种压缩和解压缩的方式， `nginx` 使用 `zlib` 对 `http` 包的内容进行 `gzip` ，所以需要在 `Centos` 上安装 `zlib` 库。

4.`OpenSSL` 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。`nginx` 不仅支持 `http` 协议，还支持 `https`（即在`ssl`协议上传输`http`），所以需要在 `Centos` 安装 `OpenSSL` 库。

```sh
yum install -y gcc-c++ 
yum install -y pcre pcre-devel 
yum install -y zlib zlib-devel 
yum install -y openssl openssl-devel
```

下载安装包并解压

```sh
cd /usr/local
mkdir nginx
cd nginx
wget -c https://nginx.org/download/nginx-1.12.0.tar.gz
tar -zxvf nginx-1.12.0.tar.gz
```

编译安装

```sh
cd /usr/local/nginx/nginx-1.12.0
./configure
make
make install
```

启动`nginx`

```sh
cd /usr/local/nginx/sbin
./nginx
```

浏览器访问`nginx`，`nginx`默认是在`80`端口启动的，在浏览器输入http://ip:80，正常访问到的页面如下：

![image-20200917185420640](../../../Testing_learn/typora/performanceTesting.assets/image-20200917185420640.png)

`nginx`的相关命令：

```sh
1.启动服务
./nginx

2.停止服务,此方式停止步骤是待nginx进程处理任务完毕进行停止。
./nginx -s stop

3.退出服务,此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程。
./nginx -s quit

4.重新加载,当 ngin x的配置文件 nginx.conf 修改后，要想让配置生效需要重启 nginx，
使用-s reload不用先停止 ngin x再启动 nginx 即可将配置信息在 nginx 中生效
./nginx -s reload

5.查询nginx进程
ps aux|grep nginx
```

#### 什么是负载均衡

##### 前言

当自己的`web`网站访问的人越来越多，一台服务器无法满足现有的业务时，此时会想到多加几台服务器来实现负载均衡。
 网站的访问量越来越大，服务器的服务模式也得进行相应的升级，怎样将同一个域名的访问分散到两台或更多的机器上呢？这就需要用nginx来配置负载均衡的环境了。

以多个`tomcat`服务为例，用`nginx`配置管理多个`tomcat`服务

##### 什么是负载均衡

负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。负载均衡，英文名称为`Load Balance`，其意思就是分摊到多个操作单元上进行执行，例如`Web`服务器、`FTP`服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。

如果还是不懂的话，可以举个例子：
 假设你是个妹子，你败家太厉害，以至于你的男友根本吃不消，于是乎你找了两个男朋友，一三五单号，二四六双号限行，从而减少一个男朋友所面临的压力，这叫负载均衡。

`nginx`的负载均衡策略有`2`种，第一种是**轮询**：也就是上面说的“两个男朋友，一三五单号，二四六双号限行”，看下图

![img](../../../Testing_learn/typora/performanceTesting.assets/clip_image002-1600340598731.jpg)

#### 同一个机器安装多个tomcat

下载tomcat安装包并解压：

```sh
cd /usr/local
mkdir tomcat
cd tomcat
wget http://mirrors.hust.edu.cn/apache/tomcat/tomcat-8/v8.5.35/bin/apache-tomcat-8.5.35.tar.gz
tar -zxvf apache-tomcat-8.5.35.tar.gz
mv apache-tomcat-8.5.50 apache-tomcat-1
cp -r apache-tomcat-1 apache-tomcat-2
cp -r apache-tomcat-1 apache-tomcat-3
```

修改**每一个**`tomcat`的`server.xml`配置文件：

修改1：

![image-20200917185921370](../../../Testing_learn/typora/performanceTesting.assets/image-20200917185921370.png)

修改2：

![image-20200917190012422](../../../Testing_learn/typora/performanceTesting.assets/image-20200917190012422.png)

修改3：

![image-20200917190118727](../../../Testing_learn/typora/performanceTesting.assets/image-20200917190118727.png)

开放防火墙端口：

```sh
firewall-cmd --add-port=8081/tcp --permanent
firewall-cmd --add-port=8081/udp --permanent

firewall-cmd --add-port=8082/tcp --permanent
firewall-cmd --add-port=8082/udp --permanent

firewall-cmd --add-port=8083/tcp --permanent
firewall-cmd --add-port=8083/udp --permanent

firewall-cmd --reload
```

启动`3`个`tomcat`:

```sh
/usr/local/tomcat/apache-tomcat-1/bin/startup.sh
/usr/local/tomcat/apache-tomcat-2/bin/startup.sh
/usr/local/tomcat/apache-tomcat-3/bin/startup.sh
```

浏览器访问`3`个`tomcat`：`ip:8081`、`ip:8082`、`ip:8083`

确保`3`个`tomcat`都能访问成功后，修改每一个`tomcat`的首页`index.jsp`文件，以便区分：

```sh
vi /usr/local/tomcat/apache-tomcat-1/webapps/ROOT/index.jsp
vi /usr/local/tomcat/apache-tomcat-2/webapps/ROOT/index.jsp
vi /usr/local/tomcat/apache-tomcat-3/webapps/ROOT/index.jsp
```

修改的内容如下：

![image-20200917191416570](../../../Testing_learn/typora/performanceTesting.assets/image-20200917191416570.png)

改完后，重新刷新`tomcat`的页面：

![image-20200917191453554](../../../Testing_learn/typora/performanceTesting.assets/image-20200917191453554.png)

#### 在nginx上配置tomcat的负载均衡

修改`nginx`的配置文件`nginx.conf`，实现让`nginx`把客户端请求轮询地指向其中一个`tomcat`服务

```sh
vi /usr/local/nginx/conf/nginx.conf
```

在文件里面的`server`上面添加内容（切记不要漏了末尾的分号'`;`'）：

![image-20200917192054650](../../../Testing_learn/typora/performanceTesting.assets/image-20200917192054650.png)

`upstream`的作用是定义一组服务器， 这些服务器可以监听不同的端口，语法格式为：`upstream name { ... }`

继续在`nginx.conf`里面添加内容：

![image-20200917192812803](../../../Testing_learn/typora/performanceTesting.assets/image-20200917192812803.png)

保存退出，重启`nginx`（如果已经启动了就不需要，多次启动会报错）：

```sh
cd /usr/local/nginx/sbin
./nginx -s reload
```

确保`3`个`tomcat`都已经启动了，然后浏览器访问`ip`地址： http://ip （不需要端口号），然后不断刷新，观察页面变化，可发现每次刷新，都会访问不同的`tomcat`服务器：

![image-20200917193224631](../../../Testing_learn/typora/performanceTesting.assets/image-20200917193224631.png)

### 系统架构演化历程

现在出去找工作如果不会点分布式和微服务相关的内容，都不太好更面试官扯蛋。但这些架构也不是突然就出现的，而是经过不但演变才出现及流行起来的，本文就给大家来梳理下java项目架构的演变历程。

#### 单体架构

大型网站都是从小型网站发展而来的，网站架构也是一样，是从小型网站架构逐步演化而来的，小型网站最开始没有太多人访问，只需要一台服务器就绰绰有余了，这时的架构如下:

![image-20200917202346161](../../../Testing_learn/typora/performanceTesting.assets/image-20200917202346161.png)

应用程序、数据库、文件等所有的资源都在一台服务器上，通常服务器操作系统使用`Linux`、应用程序使用`java`或者其他语句，然后部署在`Apache`或者`Nginx`上。数据库使用`MYSQL`，使用开源的技术实现，然后部署在一台廉价的服务器上就开始了网站的发展之路。

#### 应用服务和数据服务分离

好景不长，随着公司业务的发展，一台服务逐渐满足不了需求，越来越多的用户访问导致性能越来越差，数据存储空间开始不足，这时我们需要将应用和数据分离，分离后开始使用三台服务器：应用服务器、文件服务器、数据库服务器。如图:

![image-20200917202541808](../../../Testing_learn/typora/performanceTesting.assets/image-20200917202541808.png)

应用和数据分离后，不同特性的服务器承担着不同的服务角色，网站的并发处理能力和数据存储空间都得到了很大的改善，支持网站业务进一步发展，但是随着用户逐渐增多，数据库压力越来越大，访问延迟，进而影响整个网站的性能，此时需要进一步优化。

#### 缓存的使用

网站访问有个著名的二八定律，即`80%`的业务集中访问在`20%`的数据上，如果我们将这一小部分的数据缓存在内存中，能够很好的减少数据库的访问压力，提高整个网站的数据访问速度。

![image-20200917202708939](../../../Testing_learn/typora/performanceTesting.assets/image-20200917202708939.png)

缓存常用的组件可以是`Redis`,`ehcache`等

#### 集群的使用

缓存解决了数据库访问量比较大的问题，但是并不能解决随着业务增多造成的服务器并发压力大的问题，这时我们需要增加一台应用服务器来分担原来服务器的访问压力和存储压力。如图:

![image-20200917202749672](../../../Testing_learn/typora/performanceTesting.assets/image-20200917202749672.png)

**通过负载均衡调度服务器，可将来自用户的访问请求分发到应用服务器中的任何一台服务器中**，这样多台服务器就分担了原来一台服务器的压力，我们只需要注意会话的一致性就可以了。

#### 数据库读写分离

系统正常运行了一段时间后，虽然加的有缓存，使绝大多数的数据库操作可以不通过数据库就能完成，但是任然有一部分的操作(缓存访问不命中，缓存过期)和全部的写操作需要访问数据库，当用户达到一定规模后，数据库因为负载压力过大还是会成为系统的瓶颈， 这时主流的数据库都提供的有**主从热备份**功能，通过**配置两台数据库实现主从关系**，可以将一台数据库服务器的数据更新同步到另一台服务器上。可以利用这一功能来实现数据库读写分离。从而改善数据库的负载压力，如图:

![image-20200917202834196](../../../Testing_learn/typora/performanceTesting.assets/image-20200917202834196.png)

`mysql`的读写分离可以通过自身自带的从主复制实现，`Oracle`的话可以通过阿里巴巴的`mycat`组件来实现。

#### 反向代理和[CDN](https://cloud.tencent.com/product/cdn?from=10680)加速

为了应付复杂的网络环境和不同地区用户的访问，通过`CDN`和反向代理加快用户访问的速度，同时减轻后端服务器的负载压力。`CDN`与反向代理的基本原理都是缓存。`CDN`部署在网络提供商的机房。用户请求到来的时候从距离自己最近的网络提供商机房获取数据，而反向代理则部署在网站的中心机房中，请求带来的时候先去反向代理服务器中查看请求资源，如果有则直接返回。如图：

![image-20200917202939249](../../../Testing_learn/typora/performanceTesting.assets/image-20200917202939249.png)

使用`CDN`和反向代理的目的都是尽早返回数据给用户，一方面加快用户的访问速度，另一方面也减轻后端服务器的负载压力。

帮助理解CDN链接：https://www.zhihu.com/question/36514327?rf=37353035

#### 分布式文件和分布式数据库

任何强大的单一服务器都满足不了大型网站持续增长的业务需求。数据库经过读写分离后，从一台服务器拆分成两天服务器，但是随着业务的增长后面依然不能满足需求，这时我们需要使用分布式数据库，同时文件系统也一样，需要使用分布式文件系统。   

**分布式数据库是数据库拆分的最后的手段，只有在表单数据规模非常庞大的时候才使用**，不到不得已时，我们更常用的手段是业务分库，将不同的业务数据部署在不同的物理服务器上。

![image-20200917203516037](../../../Testing_learn/typora/performanceTesting.assets/image-20200917203516037.png)

#### NoSql和搜索引擎

随着业务越来越复杂，对数据存储和检索的需求也越来越复杂，这时一些`NoSQL`(`Reids`,[HBase](https://cloud.tencent.com/product/hbase?from=10680),`mongodb`)数据库技术和搜索引擎(`Solr`,`Elasticsearch`)的时候就显得很有必要。如下图:

![image-20200917203633726](../../../Testing_learn/typora/performanceTesting.assets/image-20200917203633726.png)

`NoSQL`和搜索引擎对可伸缩的分布式特性具有更好的支持，应用服务器通过一个统一的数据访问模块访问各种数据，减轻应用程序管理诸多数据源的麻烦。

#### 业务拆分

当访问量达到一定规模的时候我们可以通过分而治之的手段将整个系统的业务分成不同的产品线，例如我们将系统的首页，商铺，订单，买家，卖家，支付，订单等拆分成不同的产品线。

具体到技术实现上，也可以根据产品线划分，**将一个网站拆分成许多不同的应用，每个应用独立部署维护，应用之间通过`RPC`框架(`dubbo`,`webService`,`httpClient`…)建立连接，也可以通过消息队列实现异步分发处理。**来构成一个完整的系统，如下图:

![image-20200917203831485](../../../Testing_learn/typora/performanceTesting.assets/image-20200917203831485.png)



#### 分布式服务

随着业务拆分越来越小，存储系统越来越庞大，应用系统的整体复杂度呈指数级增加，部署维护越来越困难，由于所有应用要和所有数据库系统连接，最终导致数据库连接资源不足，拒绝服务。

1. 当服务越来越多时，服务`URL`配置管理变得非常困难，`F5`硬件负载均衡器的单点压力也越来越大。
2. 当进一步发展，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。
3. 接着，服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑？什么时候该加机器？
4. 服务多了，沟通成本也开始上升，调某个服务失败该找谁？服务的参数都有什么约定？
5. 一个服务有多个业务消费者，如何确保服务质量？
6. 随着服务的不停升级，总有些意想不到的事发生，比如`cache`写错了导致内存溢出，故障不可避免，每次核心服务一挂，影响一大片，人心慌慌，如何控制故障的影响面？服务是否可以功能降级？或者资源劣化？

解决方案：公共的应用模块被提取出来，部署在分布式服务器上供应用服务器调用。也就是我们将的分布式服务或者微服务。

微服务的设计原则参考此文: https://dpb-bobokaoya-sm.blog.csdn.net/article/details/87305626

### 分布式压测

todo（待写）

### 收集监控系统

`influxdb+grafana+telegra`监控系统的简单理解：`telegraf`收集系统`cpu`等数据，并写入数据到`influxdb`中，`jmeter`运行脚本，也写入测试数据到`influxdb`中，最终由`grafana`以图形化实时展示系统和测试数据。

![image-20200919120439834](../../../Testing_learn/typora/performanceTesting.assets/image-20200919120439834.png)

#### influx

官方文档：https://docs.influxdata.com/platform/getting-started/ 

或者https://docs.influxdata.com/influxdb/v1.6/

`InfluxDB` 是用`Go`语言编写的一个**开源分布式时序、事件和指标数据库**，无需外部依赖。类似的数据库有`Elasticsearch`、`Graphite`等。

**主要功能：**

1）基于时间序列，支持与时间有关的相关函数（如最大，最小，求和等）

2）可度量性：你可以实时对大量数据进行计算

3）基于事件：它支持任意的事件数据

**InfluxDB主要特点：**

1）无结构（无模式）：可以是任意数量的列

2）可拓展的

3）支持`min`, `max`, `sum`, `count`, `mean`, `median` 等一系列函数，方便统计

4）原生的`HTTP`支持，内置`HTTP API`

5）强大的类`SQL`语法

6）自带管理界面，方便使用

下载`influx`安装包：

```sh
mkdir /usr/local/influx
cd /usr/local/influx
wget https://dl.influxdata.com/influxdb/releases/influxdb-1.7.0.x86_64.rpm
```

下载完之后，直接就可以安装

```sh
yum localinstall influxdb-1.7.0.x86_64.rpm
```

启动`influxdb`数据库：

```sh
service influxdb start

停止：service influxdb stop
```

`inlufxdb`各个服务的默认端口：

```lua
8083：访问web页面的地址，8083为默认端口；
8086：数据写入influxdb的地址，8086为默认端口；（这个端口很重要）
8088：数据备份恢复地址，8088为默认端口；
```

防火墙添加端口：

```sh
firewall-cmd --add-port=8086/tcp --permanent
firewall-cmd --add-port=8086/udp --permanent 
firewall-cmd --reload
```

查看`8086`端口：

![image-20200919115012463](../../../Testing_learn/typora/performanceTesting.assets/image-20200919115012463.png)

#### grafana

`Grafana`是一个跨平台的开源的**度量分析和可视化工具**，可以通过**将采集的数据查询然后可视化的展示，并及时通知**。它主要有以下六大特点：

- 1、展示方式：快速灵活的客户端图表，面板插件有许多不同方式的可视化指标和日志，官方库中具有丰富的仪表盘插件，比如热图、折线图、图表等多种展示方式；
- 2、数据源：`Graphite`，`InfluxDB`，`OpenTSDB`，`Prometheus`，`Elasticsearch`，`CloudWatch`和`KairosDB`等；
- 3、通知提醒：以可视方式定义最重要指标的警报规则，Grafana将不断计算并发送通知，在数据达到阈值时通过`Slack`、`PagerDuty`等获得通知；
- 4、混合展示：在同一图表中混合使用不同的数据源，可以基于每个查询指定数据源，甚至自定义数据源；
- 5、注释：使用来自不同数据源的丰富事件注释图表，将鼠标悬停在事件上会显示完整的事件元数据和标记；
- 6、过滤器：`Ad-hoc`过滤器允许动态创建新的键/值过滤器，这些过滤器会自动应用于使用该数据源的所有查询。

官方文档：https://grafana.com/docs/grafana/latest/

##### grafana安装

下载安装包：

```sh
mkdir /usr/lccal/gragana
cd /usr/local/grafana
wget https://dl.grafana.com/oss/release/grafana-6.2.2-1.x86_64.rpm
```

安装`grafana`：

```
yum localinstall grafana-6.2.2-1.x86_64.rpm
```

启动`grafana`服务：

```sh
service grafana-server start

停止： service grafana-server stop

若是centos7，建议使用systemctl命令，如 systemctl start grafana-server
```

防火墙添加`3000`端口，`3000`为`Grafana`的默认侦听端口：

```sh
firewall-cmd --add-port=3000/tcp --permanent
firewall-cmd --add-port=3000/udp --permanent 
firewall-cmd --reload
```

浏览器访问`grafana`，http://ip:3000，初始账号和密码都是`admin`，初次登录需要修改密码。

![image-20200919121326250](../../../Testing_learn/typora/performanceTesting.assets/image-20200919121326250.png)

##### grafana使用

1、添加`Data Source`，并选择`inlufxDB`数据源

![image-20200919191128494](../../../Testing_learn/typora/performanceTesting.assets/image-20200919191128494.png)

选择`influxDB`:

![image-20200919191215403](../../../Testing_learn/typora/performanceTesting.assets/image-20200919191215403.png)

2、设置数据源`Data Source`:

![image-20200919191412009](../../../Testing_learn/typora/performanceTesting.assets/image-20200919191412009.png)

3、创建`dashboard`，有两种创建方式。

第一种，根据别人保存分享的模板来进行创建，可到`grafana`官网找别人分享的模板来创建`dashboard`：https://grafana.com/grafana/dashboards，官网上的每个`dashboard`都一个唯一的`dashboard id`，随便点击一个`dashboard`都可以看到，如：

![image-20200919191931746](../../../Testing_learn/typora/performanceTesting.assets/image-20200919191931746.png)

想要导入模板，只需要把`ID`复制下来，根据下面步骤导入即可：

![image-20200919191706576](../../../Testing_learn/typora/performanceTesting.assets/image-20200919191706576.png)

![image-20200919192222813](../../../Testing_learn/typora/performanceTesting.assets/image-20200919192222813.png)

第二种，自己新建一个`dashboard`，步骤如下：

![image-20200919191615081](../../../Testing_learn/typora/performanceTesting.assets/image-20200919191615081.png)

![image-20200919192317567](../../../Testing_learn/typora/performanceTesting.assets/image-20200919192317567.png)

![image-20200919192401407](../../../Testing_learn/typora/performanceTesting.assets/image-20200919192401407.png)

#### telegraf

##### 安装telegraf

官方学习文档：https://docs.influxdata.com/telegraf/v1.9/introduction/getting-started/

`telegraf`是一个可收集系统和服务的统计数据，比如说`cpu`使用率等，并写入到`InfluxDB`数据库的程序。通过`telegraf`，我们可将系统`cpu`、内存、磁盘等使用数据实时地写入`InfluxDB`数据库，从而在`grafana`的`web`页面中以图表形式展示出来。

在`opt`目录使用`wget`获取安装包：

```sh
cd /opt
wget https://dl.influxdata.com/telegraf/releases/telegraf-1.0.1.x86_64.rpm
```

`yum`本地安装`telegraf`：

```sh
yum localinstall telegraf-1.0.1.x86_64.rpm
```

修改配置文件 `telegraf.conf`（此步根据自身情况修改）

```sh
vi /etc/telegraf/telegraf.conf
```

![image-20200918182514619](../../../Testing_learn/typora/performanceTesting.assets/image-20200918182514619.png)

配置文件里可设置收集的数据要存放到`telegraf`的哪个库：

![image-20200918185702071](../../../Testing_learn/typora/performanceTesting.assets/image-20200918185702071.png)

查看官方文档：

![image-20200918184949586](../../../Testing_learn/typora/performanceTesting.assets/image-20200918184949586.png)

可以知道，在启动`telegraf`服务之前，需要先编辑或者初始化创建一个配置文件（其实这步可以跳过），这个配置文件包含了一些信息，如：

1. 统计的指标从哪里来
2. 统计的指标存放到哪里去

配置`telegraf`：

```sh
telegraf -sample-config -input-filter cpu:mem:disk -output-filter influxdb > telegraf.conf

解释：
-input-filter cpu:mem:disk     代表获取cpu、内存、磁盘的指标
-output-filter influxdb			代表这些指标存放到influxdb数据库里面
> telegraf.conf				代表这些配置信息将会存放到当前所在路径的telegraf.conf文件中，运行这行后会自动生成这个conf文件
```

启动`telegraf`:

```sh
service telegraf start 

经过观察，发现telegraf启动时默认读取的/etc/telegraf路径下的telegraf.conf，而不是前面一步配置生成的。
因此，如果要修改要搜集的指标时，建议直接cd到/etc/telegraf，再生成新的telegraf.conf文件，覆盖掉原来的就行。
```

进入`influxdb`，查看存在的库，会发现多了一个`telegraf`库：

![image-20200918190223614](../../../Testing_learn/typora/performanceTesting.assets/image-20200918190223614.png)

查看`telegraf`库存在的表：`show measurements;`

![image-20200918190434827](../../../Testing_learn/typora/performanceTesting.assets/image-20200918190434827.png)

查看`telegraf`库的表的所有字段：`show field keys;`

![image-20200918190542912](../../../Testing_learn/typora/performanceTesting.assets/image-20200918190542912.png)

查看某个表的某些内容：`SELECT usage_idle FROM cpu WHERE cpu = 'cpu-total' LIMIT 5`

![image-20200918190725417](../../../Testing_learn/typora/performanceTesting.assets/image-20200918190725417.png)

##### 实时监控cpu等信息

接下来要在`grafana`上以图表形式实时查看`telegraf`收集的存放在`influxdb`中的指标和数据。

第一步：添加`Data Sources`

![image-20200918191250052](../../../Testing_learn/typora/performanceTesting.assets/image-20200918191250052.png)

设置的关键信息如下（其它默认即可）：

![image-20200918191332075](../../../Testing_learn/typora/performanceTesting.assets/image-20200918191332075.png)

![image-20200918191341990](../../../Testing_learn/typora/performanceTesting.assets/image-20200918191341990.png)



第二步：新建`Dashboard`

![image-20200918191516876](../../../Testing_learn/typora/performanceTesting.assets/image-20200918191516876.png)

第三步：添加`query`查询

![image-20200918191540141](../../../Testing_learn/typora/performanceTesting.assets/image-20200918191540141.png)

第四步：设置`query`：

![image-20200918191651152](../../../Testing_learn/typora/performanceTesting.assets/image-20200918191651152.png)

![image-20200918191928197](../../../Testing_learn/typora/performanceTesting.assets/image-20200918191928197.png)

举个例子：

![image-20200918192119406](../../../Testing_learn/typora/performanceTesting.assets/image-20200918192119406.png)

添加多个`query`后的显示效果：

![image-20200918200553996](../../../Testing_learn/typora/performanceTesting.assets/image-20200918200553996.png)



##### telegraf plugins

查看`telegraf`支持的`plugins`: https://docs.influxdata.com/telegraf/v1.15/plugins/#input-plugins

打开上面的网页后，可以看到`telegraf`支持的`input`和`output`等各种`plugins`，举个例子：

![image-20200919175518833](../../../Testing_learn/typora/performanceTesting.assets/image-20200919175518833.png)

可以看到，`CPU`的`pugin id`为`inputs.cpu`，点击`View`：

![image-20200919175634583](../../../Testing_learn/typora/performanceTesting.assets/image-20200919175634583.png)

因此，如果我们要使用`telegraf`收集`cpu`相关指标时，就直接在`telegraf.conf`配置文件中的`input plugins`下添加一下内容即可：

![image-20200919175840579](../../../Testing_learn/typora/performanceTesting.assets/image-20200919175840579.png)

所以说，**修改要收集的指标时，有两种方法：**

1. 直接`vi`修改配置文件`telegraf.conf`，想要添加收集什么指标，直接把`[[inputs.xxx]]`等内容追加到`input plugins`里面即可。
2. 使用配置命令来修改`telegraf.conf`配置文件：`telegraf -sample-config -input-filter cpu:mem:disk -output-filter influxdb > telegraf.conf`

常见的`plugins`：

- `mem` ：系统内存信息，如物理、虚拟、交换内存量等等。
- `disk` ：磁盘占用信息。
- `diskio` ：磁盘`IO`性能。
- `net`和`netstat` ：网卡和网络信息。
- `system` ：当前系统负载信息，类似`uptime`信息。
- `file` ：每个时间周期读取文件所有信息。
- `cpu`
- `swap`
- `processes`

#### 我自己的dashboard

我自己在`grafana`官网上`id`为`dashboard`模板的基础上，修改完善了后，个人认为还不错的，大致效果如下，可以监控`linux`系统的`cpu`、内存、磁盘、负载、网络等系统运行信息。

![image-20200919192813846](../../../Testing_learn/typora/performanceTesting.assets/image-20200919192813846.png)

完成`dashboard`修改后，可点击分享：

![image-20200919192929440](../../../Testing_learn/typora/performanceTesting.assets/image-20200919192929440.png)

分享方法1：别人可根据我的`link`链接来创建`dashboard`：http://192.168.239.135:3000/d/OrznxYOMk/influxdb_centos7_info?orgId=1&refresh=5s&from=1600511376487&to=1600514976487&var-host=Jimmy

![image-20200919192948563](../../../Testing_learn/typora/performanceTesting.assets/image-20200919192948563.png)

分享方法2：`snapshot`:

![image-20200919193300707](../../../Testing_learn/typora/performanceTesting.assets/image-20200919193300707.png)

分享方法3：`json`信息，别人可以根据我分享的`json`信息创建`dashboard`（将`dashboard`的`json`信息文件下载保存到本地可以是我们保存`dashboard`的一种方法）

![image-20200919193339404](../../../Testing_learn/typora/performanceTesting.assets/image-20200919193339404.png)

![image-20200919193530124](性能测试.assets/image-20200919193530124.png)

`pannel`的查看技巧：

单击某个指标名称可仅查看单个指标信息：

![image-20200919202908700](../../../Testing_learn/typora/performanceTesting.assets/image-20200919202908700.png)

双击其它名称时又可查看全部指标信息：

![image-20200919203039672](../../../Testing_learn/typora/performanceTesting.assets/image-20200919203039672.png)

### 









