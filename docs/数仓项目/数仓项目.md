## 知识点回顾

> hadoop：
> 	hdfs ：namenode两个是一种什么样的架构  HA  元数据管理 托付给了journalnode
> 		namenode + secondarynamenode  单节点故障问题
> 			使用snn来合并fsimage与edits文件	
> 	yarn：分布式资源管理系统  主要用于管理集群的资源 CPU  内存
> 	mapreduce：分布式文件计算系统  但是分布式的思想，非常经典
> 		天龙八部
>
> zookeeper：分布式服务协调框架
> 	注册中心
> 	服务发现
> 	
>
> hive：数据仓库工具
> 	主要用于来构建数据仓库
> 	四种表模型：内部表，外部表，分区表，分桶表
> 	数据加载：
> 	数据导出保存
> 	动态分区，静态分区
> 	数据存储格式 + 压缩方式
> 	hive调优
> 	
>
> HBase：分布式nosql数据库  主要用于存储海量的数据的
> 	列式数据库：主键rowkey，列族，列，多版本，表名，namespace命名空间
> 	基本增删改查的操作，创建预分区，二级索引，rowkey的设计
> 	
> ETL辅助框架
> 	sqoop：数据导入导出工具
> 	flume：数据采集工具
> 		如何自定义source以及sink
> 		如何保证事务问题
> 		flume采集的频率，如何实现高可用等等
> 	azkaban：定时任务，会用即可
> 	impala + hue + oozie
>
> kafka的课程：分布式的消息队列
> 	生产者
> 	消费者
> 	broker
> 	offset管理
> 	数据的查找策略：使用二分法查找
> 	数据的分区方式：指定分区号，通过key进行hashCode，随机的方式
> 	ISR,LEO,HW等等
> 	数据的有序性：每一个分区当中的数据都是有序的
>
> spark课程：
> 	sparkCore：算子调优
> 	sparkSQL：用于处理结构化的数据 sparkSQL  on  hive 替代hive底层的mr的执行引擎
> 	sparkStreaming：
> 		sparkStreaming整合kafka的实现  direct方式 +  receiver方式
> 		如何实现自定offset的维护，如何实现exactly_once语义的消费

## :rainbow:游戏日志数据仓库之数据采集

## 数据仓库的基本概念

#### 什么是数据仓库

W.H.Inmon在《 Building the Data Warehouse 》一书中，对数据仓库的定义为：

数据仓库是一个

**面向主题的**：可理解为面向不同的业务

**集成的**：将散落在不同业务库中的数据收集起来，打通数据之间的隔阂

**非易失的**：保存的一直都是历史数据，不会轻易做改动

**随时间变化的**：反映历史变化，通过分析历史的数据，看到历史变化的趋势=》饼状图、折线图、地图..

用来**支持管理人员决策的**数据集合。

##### 面向主题

操作型数据库的数据组织面向事务处理任务，各个业务系统之间各自分离，而**数据仓库中的数据是按照一定的主题域进行组织。**    

主题是一个抽象的概念，是数据归类的标准，是指用户使用数据仓库进行决策时所关心的重点方面，一个主题通常与多个操作型信息系统相关。每一个主题基本对应一个宏观的分析领域。

例如，银行的数据仓库的主题：客户

客户数据来源：从银行储蓄数据库、信用卡数据库、贷款数据库等几个数据库中抽取的数据整理而成。这些客户信息有可能是一致的，也可能是不一致的，这些信息需要统一整合才能完整体现客户。

##### 集成

面向事务处理的操作型数据库通常与某些特定的应用相关，数据库之间相互独立，并且往往是异构的。而数据仓库中的数据是在**对原有分散的数据库数据抽取、清理的基础上经过系统加工、汇总和整理得到的，必须消除源数据中的不一致性**，以保证数据仓库内的信息是关于整个企业的一致的全局信息。

具体如下：

1. 数据进入数据仓库后、使用之前，必须经过加工与集成。
2. 对不同的数据来源进行统一数据结构和编码。统一原始数 据中的所有矛盾之处，如字段的同名异义，异名同义，单位不统一，字长不一致等。
3. 将原始数据结构做一个从面向应用到面向主题的大转变。

##### 非易失即相对稳定的

操作型数据库中的数据通常实时更新，数据根据需要及时发生变化。数据仓库的数据主要供企业决策分析之用，所涉及的数据操作主要是数据查询**，一旦某个数据进入数据仓库以后，一般情况下将被长期保留，**也就是数据仓库中一般有大量的查询操作，但修改和删除操作很少，通常只需要定期的加载、刷新。

数据仓库中包括了大量的历史数据。

数据经集成进入数据仓库后是极少或根本不更新的。

##### 随时间变化即反映历史变化

操作型数据库主要关心当前某一个时间段内的数据，而数据仓库中的数据通常包含历史信息，系统记录了企业从过去某一时点(如开始应用数据仓库的时点)到目前的各个阶段的信息，通过这些信息，可以对企业的发展历程和未来趋势做出定量分析和预测。企业数据仓库的建设，是以现有企业业务系统和大量业务数据的积累为基础。数据仓库不是静态的概念，只有把信息及时交给需要这些信息的使用者，供他们做出改善其业务经营的决策，信息才能发挥作用，信息才有意义。而把信息加以整理归纳和重组，并及时提供给相应的管理决策人员，是数据仓库的根本任务。因此，从产业界的角度看，**数据仓库建设是一个工程，是一个过程**

数据仓库内的数据时限一般在５-10年以上，甚至永不删除，这些数据的键码都包含时间项，标明数据的历史时期，方便做时间趋势分析。

 

#### 数据仓库的演进

![image-20200425030104111](数仓项目.assets/image-20200425030104111.png)

#### 数据库与数据仓库的对比

![img](数仓项目.assets/clip_image004.png)

#### 数据仓库的主要作用

###### 1、支持数据提取

数据提取可以支撑来自企业各业务部门的数据需求。

由之前的不同业务部门给不同业务系统提需求转变为不同业务系统统一给数据仓库提需求

![img](数仓项目.assets/clip_image006.jpg)

###### 2、支持报表系统

基于企业的数据仓库，向上支撑企业的各部门的统计报表需求，辅助支撑企业日常运营决策。

![1-11101519444S20.jpg](数仓项目.assets/clip_image008.jpg)

###### 3、支持数据分析（BI）

BI（**商业智能**）是一种解决方案：

从许多来自不同的企业业务系统的数据中提取出有用的数据并进行清理，以保证数据的正确性，然后经过抽取、转换和装载,即ETL过程，合并到一个企业级的数据仓库里，从而得到企业数据的一个全局视图；

在此基础上利用合适的查询和分析工具、数据挖掘工具、OLAP工具等对其进行分析和处理（这时信息变为辅助决策的知识）；

最后将知识呈现给管理者，为管理者的决策过程提供支持 。  

![u=663018156,2699099922&fm=23&gp=0.jpg](数仓项目.assets/clip_image010.jpg)

###### 4、支持数据挖掘

数据挖掘也称为数据库知识发现（Knowledge Discovery in Databases, KDD），就是将高级智能计算技术应用于大量数据中，让计算机在有人或无人指导的情况下从海量数据中发现潜在的，有用的模式（也叫知识）。

Jiawei Han在《数据挖掘概念与技术》一书中对数据挖掘的定义：数据挖掘是从大量数据中挖掘有趣模式和知识的过程，数据源包括数据库、数据仓库、Web、其他信息存储库或动态地流入系统的数据。

![img](数仓项目.assets/clip_image012.jpg)

###### 5、支持数据应用

电信基于位置数据的旅游客流分析及人群画像

电信基于位置数据的人流监控和预警

银行基于用户交易数据的金融画像应用

电商根据用户浏览和购买行为的用户标签体系及推荐系统

征信机构根据用户信用记录的信用评估

![img](数仓项目.assets/clip_image014.jpg)

![img](数仓项目.assets/clip_image016.jpg)



#### 离线数据仓库的大致构建流程（重要）

1. 收集各类数据，**进行ETL过程，存到HDFS**
2. 将存到HDFS的数据**映射到Hive表中，构建数据仓库**
3. 将数据仓库分层，一般分为3/4层，以4层为例：
   1. ODS层：贴源层
      DWD层：ods层的数据进行**清洗**的操作   **》将数据变成结构化的数据
      DWS层：对ods层的数据进行**聚合**操作   **》 将多张表合并成为一张表
      ADS层：数据展示层
4. 进行数据展示的操作。

![image-20200425031735191](数仓项目.assets/image-20200425031735191.png)

## 游戏行业背景介绍

#### 游戏行业分类

现在的市面上很多游戏公司主要分类两大类：**一是游戏研发类，二是游戏运营类。**

研发类公司就是自己开发游戏，有自己的游戏策划师，美术团队，有原画师，美术设计师，设计场景和角色，有3D建模师，有渲染师，程序员游戏开发工程师。

 

![IMG_256](数仓项目.assets/clip_image017.jpg)

韩国的NEXON游戏公司开发的，由盛大代理的。后来盛大出钱成立了启航工作室，开发出了《龙之谷手游》。还有天美工作室，开发过《QQ飞车》、《逆战》，由腾讯运营。

这种团队一般是以工作室的形式存在，不是有限责任公司的形式。他们没有自己的运营和客服团队，他们一般是把自己的游戏卖给专门做运营的公司，**由运营公司负责游戏的运营、客服服务、玩家服务、游戏活动策划等工作**。他们再根据流水来分成，就是用户充值的钱，叫做流水。

而这部分流水，不仅仅由游戏研发公司和游戏运营公司分钱，还有第三方渠道也要参与分钱。比如一款游戏是通过苹果的AppStore下载的，那苹果就要分钱，通过腾讯应用宝的，那腾讯也要分。

所以游戏行业的数据分析和数据仓库的建设中，渠道是很重要的一个维度。

老师笔记：

```
游戏是这样给你玩的，
一类是游戏研发公司**》工作室的形式存在   =》写代码开发游戏的
游戏运营公司：做推广的    **》也有一部分自己的数据分析人员  **》做大数据的分析  **》
分析哪些关卡比较难，分析哪些人比较有钱，分析哪些人物角色战斗力该怎么设置   **> 促进充值流水
```

#### 数据仓库总体架构概览

![数据仓库总体架构](数仓项目.assets/clip_image023.png)

 

老师笔记：

```
游戏行业数据仓库的设计：
玩游戏：让你选择区服：
每个地方都有好多区服：
湖北省区服  1区  2区   3区  4区
游戏并发量太大了，服务器顶不住

各个区服之间的数据不同步的

在游戏区服下面，进一步做分库分表的操作
分表
ods_user_login_2020-03-26
ods_user_login_2020-03-27

分表：按照id进行hash取模的操作
ods_user_login_1
ods_user_login_2
ods_user_login_3
ods_user_login_4

数据仓库的构建大致流程：

1. 多线程java程序去每个服务器上面拉取数据
2. 拉取回来的数据，写入到linux的本次磁盘
3. 本地磁盘的数据  **》 加载到hdfs上面去  **》 load 到hive的ods层表里面去了
4. dw层：构建数据的分析
5. res层：数据集市，对外提供数据的展示，数据提取等等各种功能
```

所有的大数据平台都是一样的功能设计，虽然组件和模块各有不同。但是**主要的功流程还是3个板块，数据采集，数据存储与计算，数据产出与展示。**

![3个板块](数仓项目.assets/clip_image024.png)

采集的方式很多，组件也很多。主要根据业务场景来选择实时采集和离线采集的方式

**实时采集： 比如常见的flume + kafka + 流计算框架**。这种业务场景就要大数据团队能够有权限实时的去获得数据，或者由对方能够实时的输出出来，才可以。

**离线采集：**不能实时获取数据的，有些是采集日志，有些公司是采集关系型数据库。比如采集mysql，离线的有人用**sqoop去采集**，实时的有些公司是通过flume或者**解析binlog日志**。

但是**一般的游戏公司不能做到实时采集，因为很多游戏是代理的，自己的开发人员不能左右游戏研发团队**，而且往往采集日志时游戏已经开发完毕，这时候只能通过读取原来游戏日志存储的位置来获取。

**获取到数据之后，要进行存储和计算。存储和计算都是在数据仓库中进行的**，数据仓库是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。

数据仓库也是大数据的核心所在。因为只有做好了数据仓库，才能做好数据分析和后续的数据挖掘工作，才能让数据产生价值。



## 游戏日志集群服务器分表分库数据采集

#### 游戏日志采集架构：

![游戏日志采集架构](数仓项目.assets/clip_image030.png)

 

#### 日志分表分库

游戏数据分析数据的采集一般是采集日志，就是每个玩家的行为日志，包括注册、登录、充值、关卡、基本信息这些。**而这些数据都是存在多个服务器和数据库中的**，比如很多游戏登录的时候就分了四川一服、广东一服、重庆一服等等，就是为了缓解服务器的压力，而这些相应的日志数据，也是根据服务器所在的区进行了分散，同样是为了减少服务器的压力。

**日志数据一般是存放在mysql里面，做主从备份。由于数据量很大，一般会采用分表的形式来存储，一般是一个月一个表。**

比如《热血精灵王》的游戏用户登录日志，这款游戏日志的存储分了30多个服区，**每个服区的登录用户都是存进了相应服区的数据库，而每个库里面都是采用分表的形式，每个月一个表**，即：

一个服务区---》一个database？

一个月的日志数据---》database中的一个表？

结构如下：

| 平台id | 区服id |    Ip地址     | 端口号 | 用户名 | 密码   | 数据库名 |       表名        |
| :----: | :----: | :-----------: | ------ | ------ | ------ | :------: | :---------------: |
|   1    |   1    | 192.168.1.101 | 3306   | root   | 123456 |  rxjjw   | user_login_201901 |
|   1    |   1    | 192.168.1.101 | 3306   | root   | 123456 |  rxjjw   | user_login_201902 |
|   1    |   1    | 192.168.1.101 | 3306   | root   | 123456 |  rxjjw   | user_login_201903 |
|   1    |   1    | 192.168.1.101 | 3306   | root   | 123456 |  rxjjw   | user_login_201904 |
|   1    |   1    | 192.168.1.101 | 3306   | root   | 123456 |  rxjjw   | user_login_201905 |
|   1    |   2    | 192.168.1.102 | 3306   | root   | 123456 |  rxjjw   | user_login_201901 |
|   1    |   2    | 192.168.1.102 | 3306   | root   | 123456 |  rxjjw   | user_login_201902 |
|   1    |   2    | 192.168.1.102 | 3306   | root   | 123456 |  rxjjw   | user_login_201903 |
|   1    |   2    | 192.168.1.102 | 3306   | root   | 123456 |  rxjjw   | user_login_201904 |
|   1    |   2    | 192.168.1.102 | 3306   | root   | 123456 |  rxjjw   | user_login_201905 |
|   1    |   3    | 192.168.1.103 | 3306   | root   | 123456 |  rxjjw   | user_login_201901 |
|   1    |   3    | 192.168.1.103 | 3306   | root   | 123456 |  rxjjw   | user_login_201902 |
|   1    |   3    | 192.168.1.103 | 3306   | root   | 123456 |  rxjjw   | user_login_201903 |
|   1    |   3    | 192.168.1.103 | 3306   | root   | 123456 |  rxjjw   | user_login_201904 |
|   1    |   3    | 192.168.1.103 | 3306   | root   | 123456 |  rxjjw   | user_login_201905 |
|   1    |   4    | 192.168.1.104 | 3306   | root   | 123456 |  rxjjw   | user_login_201901 |
|   1    |   4    | 192.168.1.104 | 3306   | root   | 123456 |  rxjjw   | user_login_201902 |
|   1    |   4    | 192.168.1.104 | 3306   | root   | 123456 |  rxjjw   | user_login_201903 |
|   1    |   4    | 192.168.1.104 | 3306   | root   | 123456 |  rxjjw   | user_login_201904 |
|   1    |   4    | 192.168.1.104 | 3306   | root   | 123456 |  rxjjw   | user_login_201905 |
|   1    |   5    | 192.168.1.105 | 3306   | root   | 123456 |  rxjjw   | user_login_201901 |
|   1    |   5    | 192.168.1.105 | 3306   | root   | 123456 |  rxjjw   | user_login_201902 |
|   1    |   5    | 192.168.1.105 | 3306   | root   | 123456 |  rxjjw   | user_login_201903 |
|   1    |   5    | 192.168.1.105 | 3306   | root   | 123456 |  rxjjw   | user_login_201904 |
|   1    |   5    | 192.168.1.105 | 3306   | root   | 123456 |  rxjjw   | user_login_201905 |

**【注：** **服务器地址.xlsx** **文件中】**

 这里的**平台id表示服务器所属的项目，一般表示游戏id**，比如1代表《热血精灵王》、2代表《梦幻西游》、3代表《斗罗大陆》。

user_login就是游戏用户登录日志表，由于是分表分库，所以在每个区服上会生成一个以后面月份为后缀的表，格式可能不一定是这样的，可能还有 `name_2015_1 n_yyyym, name_201501 n_yyyymm, name n`等多种格式。

这就意味着，**每个游戏，每个区服的数据是分散的，如果要查询某个游戏下某个用户的登录详情，就得查询所有的库，而且不同游戏不同服区的数据彼此之间是相互独立的**，每个游戏都是一个独立的小王国。

比如我们要查询《热血精灵王》这个游戏里面某一个用户的登录记录，我们就要挨个服区的数据库去遍历查询，因为这个用户可能在任何一个服务区上玩，遍历查询出来之后我们在汇总这个用户的所有结果。但是这样查询一次就非常的麻烦而且要消耗很长的时间。

#### 分表分库的采集方式

而**大数据平台就是要解决这个问题，消除各个信息孤岛，整合所有数据形成数据中心，提供海量数据的存储与快速查询。**

我们要先将数据采集到hive中，然后才能进行数据仓库的建设，才能进行数据分析和挖掘。

**本项目采用Java多线程的方式，并行读取所有的数据库数据，然后统一写入文件。**

##### 第一步：新建Java Maven项目 

**【注：Gamedata.zip**】

pom.xml：

```xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<groupId>gamedata</groupId>
	<artifactId>Gamedata</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<packaging>jar</packaging>

	<name>Gamedata</name>
	<url>http://maven.apache.org</url>

	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
	</properties>

	<repositories>
		<repository>
			<id>aliyun</id>
			<url>http://maven.aliyun.com/nexus/content/groups/public/</url>
		</repository>
		<repository>
			<id>central</id>
			<!-- This should be at top, it makes maven try the central repo first 
				and then others and hence faster dep resolution -->
			<name>Maven Repository</name>
			<url>https://repo1.maven.org/maven2</url>
			<releases>
				<enabled>true</enabled>
			</releases>
			<snapshots>
				<enabled>false</enabled>
			</snapshots>
		</repository>
	</repositories>

	<pluginRepositories>
		<pluginRepository>
			<id>aliyun</id>
			<url>http://maven.aliyun.com/nexus/content/groups/public/</url>
		</pluginRepository>
		<pluginRepository>
			<id>central</id>
			<url>https://repo1.maven.org/maven2</url>
			<releases>
				<enabled>true</enabled>
			</releases>
			<snapshots>
				<enabled>false</enabled>
			</snapshots>
		</pluginRepository>
	</pluginRepositories>

	<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>3.8.1</version>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.jvnet.hudson</groupId>
			<artifactId>ganymed-ssh-2</artifactId>
			<version>build260</version>
		</dependency>

		<dependency>
			<groupId>mysql</groupId>
			<artifactId>mysql-connector-java</artifactId>
			<version>5.1.32</version>
		</dependency>

		<!-- https://mvnrepository.com/artifact/log4j/log4j -->
		<dependency>
			<groupId>log4j</groupId>
			<artifactId>log4j</artifactId>
			<version>1.2.17</version>
		</dependency>

	</dependencies>

	<build>
		<sourceDirectory>src/main/java</sourceDirectory>
		<testSourceDirectory>src/main/test</testSourceDirectory>

		<plugins>
			<plugin>
				<artifactId>maven-assembly-plugin</artifactId>
				<configuration>
					<descriptorRefs>
						<descriptorRef>jar-with-dependencies</descriptorRef>
					</descriptorRefs>
					<archive>
						<manifest>
							<mainClass></mainClass>
						</manifest>
					</archive>
				</configuration>
				<executions>
					<execution>
						<id>make-assembly</id>
						<phase>package</phase>
						<goals>
							<goal>single</goal>
						</goals>
					</execution>
				</executions>
			</plugin>

			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>exec-maven-plugin</artifactId>
				<version>1.2.1</version>
				<executions>
					<execution>
						<goals>
							<goal>exec</goal>
						</goals>
					</execution>
				</executions>
				<configuration>
					<executable>java</executable>
					<includeProjectDependencies>true</includeProjectDependencies>
					<includePluginDependencies>false</includePluginDependencies>
					<classpathScope>compile</classpathScope>
					<mainClass>com.gamedata.App</mainClass>
				</configuration>
			</plugin>

			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-compiler-plugin</artifactId>
				<configuration>
					<source>1.8</source>
					<target>1.8</target>
				</configuration>
			</plugin>

		</plugins>
	</build>

</project>

```

 

##### 第二步：新建一个线程类，用来专门数据库地址连接，然后下载数据

**【项目中com.gamedata.data.TaskDataThread**】

```java
package com.gamedata.data;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.CountDownLatch;

import com.gamedata.utils.ExecShell;

/**
 * 线程类，用来处理缓存到队列里面的数据库地址的类
 * @author hutao
 *
 */
public class TaskDataThread implements Runnable {

	/**
	 * 接收的 mysql 地址队列
	 */
	private ConcurrentLinkedQueue<String> taskCountQueue = null;
	/**
	 * CountDownLatch，一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。
	 */
	private CountDownLatch latch = null;
	
	private static List<String> successUrl = new ArrayList<>(); 
	private static List<String> failUrl = new ArrayList<>(); 
	
	private String mysqlTable = null;
	private String fields = null;
	private String where = null;
	private String shellPath = null;
	private String tmpDataPath = null;
	private Set<String> fixTables = null;
	
	public ConcurrentLinkedQueue<String> getTaskCountQueue() {
		return taskCountQueue;
	}

	public void setTaskCountQueue(ConcurrentLinkedQueue<String> taskCountQueue, CountDownLatch latch, 
			String mysqlTable, String fields, String where, String shellPath, String tmpDataPath, Set<String> fixTables) {
		this.taskCountQueue = taskCountQueue;
		this.latch = latch;
		this.mysqlTable = mysqlTable;
		this.fields = fields;
		this.where = where;
		this.shellPath = shellPath;
		this.tmpDataPath = tmpDataPath;
		this.fixTables = fixTables;
	}

	public void run() {
		while (true) {
//			synchronized (TaskDataThread.class) {
				String poll = taskCountQueue.poll();
				if (poll ** null) {
					break;
				}
				String[] split = poll.split("\t");
				String plat = split[0].trim();
				String area_id = split[1].trim();
				String ip = split[2].trim();
				String port = split[3].trim();
				String username = split[4].trim();
				String password = split[5].trim();
				String database = split[6].trim();
				
				Connection conn = null;
				Statement stmt = null;
				String myDriver = "com.mysql.jdbc.Driver";
				String url = "jdbc:mysql://" + ip + ":" + port + "/" + database
						+ "?connectTimeout=20000&socketTimeout=20000&autoReconnect=true&failOverReadOnly=false&maxReconnects=6";
				
				try {
					Class.forName(myDriver);
					conn = DriverManager.getConnection(url, username, password);
					
//					System.out.println(Thread.currentThread().getName() + "连接成功:   " + poll);
					
					successUrl.add(poll);
					stmt = conn.createStatement();
					ResultSet rs = stmt.executeQuery("show tables like '%" + mysqlTable + "%'");
					
//					System.out.println(Thread.currentThread().getName() + "连接:" + poll + ": " + "show tables like '%" + mysqlTable + "%'");
					
					while (rs.next()) {
						String table = rs.getString(1);
						
						if(fixTables.contains(table)){
							String sql = "select "+ fields + " , " + area_id + " , " + plat + " from " + table +" where " + where;
							
//							System.out.println(Thread.currentThread().getName() + "连接成功并且要拉取数据的地址:   " + poll);
//							System.out.println(Thread.currentThread().getName() + "要执行的sql为： " + sql);
							
							String[] cmd = new String[9];
							cmd[0] = "sh";  		// sh 或者  bash
							cmd[1] = shellPath;  	// 要执行的脚本
							
							cmd[2] = ip; 
							cmd[3] = port;
							cmd[4] = username;
							cmd[5] = password;
							cmd[6] = database;
							
							cmd[7] = sql;  			// 要执行的sql
							// 把执行到sql的结果重定的文件名 tmp_plat_area_id_ip_database_table
							cmd[8] = tmpDataPath + "tmp" + "_" + plat + "_" + area_id + "_" + ip + "_" + database + "_" + table + ".successed"; 	
							
							ExecShell.callShell(cmd);
						}
					}
					
					rs.close();
					stmt.close();
					conn.close();
				} catch (Exception e) {
//					e.printStackTrace();
					System.err.println("连接失败:  " + poll);
					System.out.println("连接失败的原因: " + e.getMessage());
					failUrl.add(poll);
				} finally {
					if (conn != null) {
						try {
							conn.close();
						} catch (SQLException e) {
							e.printStackTrace();
						}
					}
				}
//			}

		}
		latch.countDown();
	}
	
	public static List<String> getSuccessUrl(){
		return successUrl;
	}
	
	public static List<String> getFailUrl(){
		return failUrl;
	}
}

```

##### 第三步：在主类里面运行

【项目中 com.gamedata.data.TaskDownloadData】

```java
package com.gamedata.data;

import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import com.gamedata.utils.DateUtil;

public class TaskDownloadData {

	public static void main(String[] args) {

		int threadCount = Integer.parseInt(args[0]); // 线程个数
		String filePath = args[1];

		String mysqlTable = args[2];
		String rule = args[3]; // 表名后缀规则，name_2015_1 n_yyyym, name_201501 n_yyyymm, name n

		String fields = args[4];
		String where = args[5];
		String shellPath = args[6];
		String tmpDataPath = args[7]; // 下载的数据的存放目录，这里建议写明游戏名称
		
		// 获取最终需要查询数据的表
		System.out.println("filePath: " + filePath);
		System.out.println("mysqlTable: " + mysqlTable);
		System.out.println("rule: " + rule);
		System.out.println("fields: " + fields);
		System.out.println("where: " + where);
		System.out.println("shellPath: " + shellPath);
		System.out.println("tmpDataPath: " + tmpDataPath);

		Set<String> fixTables = fixTables(mysqlTable, rule, where);
		System.out.println("要拉取数据的表为:  " + fixTables);

		long downloadStart = System.currentTimeMillis();
		System.out.println("开始拉数据...............");

		CountDownLatch latch = new CountDownLatch(threadCount);
		ConcurrentLinkedQueue<String> taskCounts = TaskGetUrl.getTaskCount(filePath);
		System.out.println("要拉取的数据库有: " + taskCounts.size());

		ExecutorService esDown = Executors.newFixedThreadPool(threadCount);
		TaskDataThread taskDataThread = new TaskDataThread();
		taskDataThread.setTaskCountQueue(taskCounts, latch, mysqlTable, fields, where, shellPath, tmpDataPath, fixTables);

		for (int i = 0; i < threadCount; i++) {
			esDown.submit(taskDataThread);
		}
		try {
			latch.await(); // 使得主线程(main)阻塞直到latch.countDown()为零才继续执行
		} catch (InterruptedException e) {
			e.printStackTrace();
		}
		System.out.println("拉取数据时间共消耗 ： " + (System.currentTimeMillis() - downloadStart) / 1000 + " 秒");
		esDown.shutdown();

		System.out.println("拉取数据成功的地址有 ： " + taskDataThread.getSuccessUrl().size());
		System.out.println("拉取连接失败的地址有 ： " + taskDataThread.getFailUrl().size());
		if(taskDataThread.getFailUrl().size()!=0){
			System.out.println(DateUtil.getCurrentTime() + " 连接失败的地址有 ： ");
			for (String string : taskDataThread.getFailUrl()) {
				System.out.println(string);
				System.out.println();
			}
			System.exit(-1);
		}
	}

	public static Set<String> fixTables(String mysqlTable, String rule, String where) {
		
		Set<String> tables = new HashSet<String>();
		
		if(where.equals("1=1")){
			tables.add(mysqlTable);
			return tables;
		}
		
		// 根据正则获取到日期，然后计算日期之间的月份，并获取加一个月和减一个月的月份
		Pattern pattern = Pattern.compile(" .*(.*)>='(.*)' and (.*)<='(.*)'.* ");
		Matcher matcher = pattern.matcher(where);
		String startDate = null;
		String endDate = null;
		while (matcher.find()) {
			startDate = matcher.group(2);
			endDate = matcher.group(4);
		}

		List<String> monthBetween = DateUtil.getMonthBetween(DateUtil.monthLess1(startDate),DateUtil.monthAdd1(endDate));

		if (rule.equals("n")) {
			tables.add(mysqlTable);
		} else if (rule.equals("n_yyyymm")) {
			for (String string : monthBetween) {
				String table = mysqlTable + "_" + string.replace("-", "");
				tables.add(table);
			}
		} else if (rule.equals("n_yyyym")) {
			for (String string : monthBetween) {
				String[] split = string.split("-");
				String month = split[0] + "_" + Integer.parseInt(split[1]);
				String table = mysqlTable + "_" + month;
				tables.add(table);
			}
		}

		return tables;
	}
}

```

 

##### 第四步：运行脚本

【项目com.gamedata.down中的 run.sh】

```sh
#!/bin/sh

startDate=$1
endDate=$2
today=`date -d "" +%Y-%m-%d`

## 线程数
threadCount=20
## MySQL 连接超时时间设置
timeOut=6000;
#使用的游戏
game="hqg";
## 要连接的文件地址，注意： 文件必须以\t制表符分割， 格式为： 平台id	区服id	Ip地址	端口号	用户名	密码	数据库名称
filePath=/home/bigdata/hadoop-project/hqg/tmp/hqg_url_new.txt
## 要拉取数据的mysql 表名
mysqlTable="log_recharge"
## 表名后缀规则，name_2015_1 n_yyyym, name_201501 n_yyyymm, name n
rule="n"
## 要拉取的字段, 使用注意事项，当拉取字段为 * 时，要把area_id,plat字段定义在hive表的最前面
fields="\\*";
## 拉取的条件, 注意，前后要加空格，这个时间要决定分表的月份的，所以如果是分月表必须知道时间，但是可以使用 '2017-01-01'>='${startDate}' and '2001-01-01'<='${endDate}' 这种形式
where=" date(time)>='${startDate}' and date(time)<='${endDate}' "
## 下载的数据的存放目录，不要 / 结尾, 如果路径不存在，要创建
tmpDataPath="/data1/tmp_down"

## 要加载的hive表
hiveTable="ods_log_recharge"
## 要加载的hive临时表
tmphiveTable="tmp_ods_log_recharge"
## 要加载的hive数据库
hiveDatabase="hqg_ods"


java -cp /home/bigdata/hadoop-project/download/mysql-connector-java-5.1.10.jar:/home/bigdata/hadoop-project/download/Skymoons-0.0.1-SNAPSHOT-jar-with-dependencies.jar com.skymoons.down.DownloadData ${threadCount} ${timeOut} ${game} ${filePath} ${mysqlTable} ${rule} ${fields} "${where}" ${tmpDataPath}


if [[ $? -eq 0 ]]
then
echo "执行:  cat ${tmpDataPath}/${game}/${mysqlTable}/data/*.data > ${tmpDataPath}/${game}/${hiveTable}.txt "
cat ${tmpDataPath}/${game}/${mysqlTable}/data/*.data > ${tmpDataPath}/${game}/${hiveTable}.txt
else
exit 1
fi

if [[ $? -eq 0 ]]
then
echo "执行:   hive -e load data local inpath '${tmpDataPath}/${game}/${hiveTable}.txt' overwrite into table ${hiveDatabase}.${tmphiveTable};"
hive -e "load data local inpath '${tmpDataPath}/${game}/${hiveTable}.txt' overwrite into table ${hiveDatabase}.${tmphiveTable};"
else
exit 1
fi


if [[ $? -eq 0 ]]
then
echo "执行:   insert overwrite table ${hiveDatabase}.${hiveTable} partition(date)
select area_id,plat,time,device_id,platform_id,platform_account,server_id,player_id,account,role_id,role_name,recharge_from,money,diamond,item_id,item_num,order_no,cp_order_no,vip_level_pre,role_level_pre,ip,vPaymentGateway,vPayingTypeID,iGrossRevenue_User,iGrossRevenue,iTopupCoin,iCashCoin,iBonusCoin,iTopupCoinAfter,iCashCoinAfter,iBonusCoinAfter,vDescription,iResult,iLevel,iIsFirstPaying,times,online_long_total,tmp1,tmp2,tmp3,tmp4,  to_date(time) as date from ${hiveDatabase}.${tmphiveTable};"

hive -e "insert overwrite table ${hiveDatabase}.${hiveTable} partition(date)
select area_id,plat,time,device_id,platform_id,platform_account,server_id,player_id,account,role_id,role_name,recharge_from,money,diamond,item_id,item_num,order_no,cp_order_no,vip_level_pre,role_level_pre,ip,vPaymentGateway,vPayingTypeID,iGrossRevenue_User,iGrossRevenue,iTopupCoin,iCashCoin,iBonusCoin,iTopupCoinAfter,iCashCoinAfter,iBonusCoinAfter,vDescription,iResult,iLevel,iIsFirstPaying,times,online_long_total,tmp1,tmp2,tmp3,tmp4,  to_date(time) as date from ${hiveDatabase}.${tmphiveTable};"
else
exit 1
fi


if [[ $? -eq 0 ]]
then
echo "执行:  rm -rf ${tmpDataPath}/${game}/${mysqlTable} "
rm -rf ${tmpDataPath}/${game}/${mysqlTable}

echo "执行:  rm ${tmpDataPath}/${game}/${hiveTable}.txt "
rm ${tmpDataPath}/${game}/${hiveTable}.txt
else
exit 1
fi

```

## 游戏业务日志分类

游戏日志有很多，按照常规的业务可以分为3大类： 

### 4.1 基础信息类

1、 游戏账号创建日志

2、 角色创建日志

3、 角色升级日志

4、 角色vip升级日志

5、 角色技能升级日志

6、 角色转职日志

7、 角色组队日志

8、 角色改名日志

9、 好友添加日志

10、夫妻组建日志

11、帮会创建

12、加入退出帮会日志

13、帮会更改

14、帮会职位日志

15、角色属性变化日志

16、角色附属品升级日志

 

这类信息就是关于到用户的基本条件，所属的，是可变化的，但是是单一的属性。

### 4.2 行为属性类

1、 角色登录登出日志

2、 背包扩充日志

3、 仓库扩充日志

4、 账号封停日志

5、 死亡日志

6、 任务日志

7、 副本开始日志

8、 副本结束日志

9、 Boss挑战开始日志

10、Boss挑战结束日志

11、战斗力变化日志

12、战斗模式切换

13、活动日志

14、自定义监控

15、自定义埋点

16、游戏启动日志

17、游戏分享日志

18、游戏关卡开始日志

19、游戏关卡结束日志

20、游戏通关日志

21、聊天日志

 

这类信息就是用户每天都会产生的行为变化，是可变的，要根据不同的日期做不同的变化。

 

### 4.3 交易属性类

1、 角色充值日志

2、 道具产出日志

3、 道具交易日志

4、 道具锻造日志

5、 道具消耗日志

6、 装备操作日志

7、 货币产出日志

8、 货币消耗日志

9、 坐骑产出日志

10、坐骑出战日志

11、坐骑升级日志

12、武将产出日志

13、武将升级日志

14、武将出战日志

15、道具销毁日志

16、广告展示日志

 

这类就是核心业务，涉及到公司收入和分成，涉及到业务和运营的考核指标。

 

## 游戏日志数仓表结构介绍

以后做数据仓库的时候，很重的一部分都是在梳理业务逻辑  --》其实就是在看各个表的各个字段代表什么含义

我们这个数仓：**如果以后出去面试写这个游戏数仓的项目  --》最少记住5张表名   --》每张表名最少背下来五个关键的字段**

### 1、游戏登录日志表

| 字段名称          | 字段类型 | 字段说明               |
| ----------------- | -------- | ---------------------- |
| plat_id           | string   | 平台id                 |
| server_id         | int      | 区服id                 |
| channel_id        | string   | 渠道                   |
| user_id           | string   | 用户ID                 |
| role_id           | string   | 角色ID                 |
| role_name         | string   | 角色名称               |
| client_ip         | string   | 客户端IP               |
| event_time        | int      | 事件时间               |
| op_type           | string   | 操作类型(1:登录-1登出) |
| online_time       | int      | 在线时长(s)            |
| operating_system  | string   | 操作系统名称           |
| operating_version | string   | 操作系统版本           |
| device_brand      | string   | 设备型号               |
| device_type       | string   | 设备品牌               |

 

### 2、角色创建日志表

| 字段名称   | 字段类型 | 字段说明           |
| ---------- | -------- | ------------------ |
| plat_id    | string   | 平台id             |
| server_id  | int      | 角色创建服         |
| channel_id | string   | 渠道(平台)ID       |
| user_id    | string   | 用户ID             |
| role_id    | string   | 角色ID             |
| role_name  | string   | 角色名称           |
| client_ip  | string   | 客户端IP           |
| event_time | int      | 事件时间           |
| job_name   | string   | 职业名称           |
| role_sex   | int      | 角色性别(0:男1:女) |

 

### 3、游戏创建日志表

| 字段名称          | 字段类型 | 字段说明     |
| ----------------- | -------- | ------------ |
| plat_id           | string   | 平台id       |
| server_id         | int      | 服务器ID     |
| channel_id        | string   | 渠道id       |
| user_id           | string   | 用户ID       |
| client_ip         | string   | 客户端IP     |
| event_time        | int      | 事件时间     |
| device_brand      | string   | 设备品牌     |
| device_type       | string   | 设备型号     |
| operating_system  | string   | 操作系统名称 |
| operating_version | string   | 操作系统版本 |

 

### 4、战斗力日志表

| 字段名称     | 字段类型 | 字段说明   |
| ------------ | -------- | ---------- |
| plat_id      | string   | 平台id     |
| server_id    | int      | 服务器ID   |
| channel_id   | string   | 渠道ID     |
| user_id      | string   | 用户ID     |
| role_id      | string   | 角色ID     |
| role_name    | string   | 角色名称   |
| client_ip    | string   | 客户端IP   |
| event_time   | int      | 事件时间   |
| change_count | bigint   | 战斗力变化 |
| combat_power | bigint   | 当前战斗力 |

 

### 5、角色升级日志表

| 字段名称    | 字段类型 | 字段说明   |
| ----------- | -------- | ---------- |
| plat_id     | string   | 平台id     |
| server_id   | int      | 服务器ID   |
| channel_id  | string   | 渠道ID     |
| user_id     | string   | 用户ID     |
| role_id     | string   | 角色ID     |
| role_name   | string   | 角色名称   |
| client_ip   | string   | 客户端IP   |
| event_time  | int      | 事件时间   |
| level_befor | int      | 战斗力变化 |
| level_after | int      | 当前战斗力 |

### 6、角色充值日志表

| 字段名称         | 字段类型 | 字段说明                                                     |
| ---------------- | -------- | ------------------------------------------------------------ |
| plat_id          | string   | 平台id                                                       |
| server_id        | int      | 服务器ID                                                     |
| channel_id       | string   | 渠道ID                                                       |
| user_id          | string   | 用户ID                                                       |
| role_id          | string   | 角色ID                                                       |
| role_name        | string   | 角色名称                                                     |
| event_time       | int      | 事件时间                                                     |
| order_id         | string   | 订单ID                                                       |
| acer_count       | int      | 获得的元宝数量                                               |
| recharge_amount  | double   | 充值金额                                                     |
| order_status     | int      | 订单状态                                                     |
| recharge_purpose | int      | 充值用途（0:商城充值充值获得元宝的形式   1:月卡充值   2:礼包充值） |

 

 

 

### 7、任务日志表

| 字段名称        | 字段类型 | 字段说明 |
| --------------- | -------- | -------- |
| plat_id         | string   | 平台id   |
| server_id       | int      | 服务器ID |
| channel_id      | string   | 渠道ID   |
| user_id         | string   | 用户ID   |
| role_id         | string   | 角色ID   |
| role_name       | string   | 角色名称 |
| event_time      | int      | 事件时间 |
| task_type       | int      | 任务类型 |
| task_id         | int      | 任务ID   |
| cost_time       | int      | 任务耗时 |
| op_type         | int      | 操作类型 |
| level_limit     | int      | 等级约束 |
| award_exp       | bigint   | 奖励经验 |
| award_monetary  | string   | 奖励货币 |
| award_item      | string   | 奖励道具 |
| death_count     | int      | 死亡次数 |
| award_attribute | string   | 奖励属性 |

 

### 8、游戏按钮操作日志表

| 字段名称   | 字段类型 | 字段说明 |
| ---------- | -------- | -------- |
| plat_id    | string   | 平台id   |
| server_id  | int      | 服务器ID |
| channel_id | string   | 渠道ID   |
| user_id    | string   | 用户ID   |
| role_id    | string   | 角色ID   |
| role_name  | string   | 角色名称 |
| client_ip  | string   | 客户端IP |
| event_time | int      | 事件时间 |
| log_id     | string   | 日志ID   |
| button_id  | string   | 按钮ID   |

 

### 9、游戏按钮字典表

| 字段名称    | 字段类型 | 字段说明     |
| ----------- | -------- | ------------ |
| plat_id     | string   | 游戏         |
| button_id   | int      | 按钮ID       |
| button_name | string   | 按钮名称     |
| system_type | string   | 按钮类型名称 |

 

### 10、虚拟币产出日志表

| 字段名称       | 字段类型 | 字段说明                      |
| -------------- | -------- | ----------------------------- |
| plat_id        | string   | 平台id                        |
| server_id      | int      | 服务器ID                      |
| channel_id     | string   | 渠道ID                        |
| user_id        | string   | 用户ID                        |
| role_id        | string   | 角色ID                        |
| role_name      | string   | 角色名称                      |
| client_ip      | string   | 客户端IP                      |
| event_time     | int      | 事件时间                      |
| log_id         | string   | 日志id                        |
| monetary_type  | int      | 货币类型(1:元宝2:金币3:礼金 ) |
| output_type    | int      | 产出类型ID                    |
| output_count   | bigint   | 产出数量                      |
| monetary_stock | bigint   | 货币存量                      |

 

### 11、虚拟币消费日志表

| 字段名称       | 字段类型 | 字段说明                      |
| -------------- | -------- | ----------------------------- |
| plat_id        | string   | 平台id                        |
| server_id      | int      | 服务器ID                      |
| channel_id     | string   | 渠道ID                        |
| user_id        | string   | 用户ID                        |
| role_id        | string   | 角色ID                        |
| role_name      | string   | 角色名称                      |
| client_ip      | string   | 客户端IP                      |
| event_time     | int      | 事件时间                      |
| log_id         | string   | 日志id                        |
| monetary_type  | int      | 货币类型(1:元宝2:金币3:礼金 ) |
| consume_type   | int      | 产出类型ID                    |
| consume_count  | bigint   | 产出数量                      |
| monetary_stock | bigint   | 货币存量                      |

 

### 12、 玩家死亡日志表

| 字段名称       | 字段类型 | 字段说明               |
| -------------- | -------- | ---------------------- |
| plat_id        | string   | 平台id                 |
| server_id      | int      | 服务器ID               |
| channel_id     | string   | 渠道ID                 |
| user_id        | string   | 用户ID                 |
| role_id        | string   | 角色ID                 |
| role_name      | string   | 角色名称               |
| client_ip      | string   | 客户端IP               |
| event_time     | int      | 事件时间               |
| log_id         | string   | 日志ID                 |
| map_id         | int      | 地图ID                 |
| attacker_uid   | string   | 凶手用户ID/怪物id      |
| attacker_rid   | string   | 凶手角色ID/怪物id      |
| attacker_rname | string   | 凶手/怪物名称          |
| attacker_level | int      | 凶手/怪物等级          |
| death_level    | int      | 死亡等级               |
| hero_power     | bigint   | 当前玩家战斗力或者伤害 |
| death_detail   | string   | 死亡详细               |

 

### 13、玩家分享日志表

| 字段名称         | 字段类型 | 字段说明              |
| ---------------- | -------- | --------------------- |
| plat_id          | string   | 平台id                |
| channel_id       | int      | 渠道ID                |
| game_channel_id  | int      | 子渠道id              |
| advertisement_id | int      | 广告ID                |
| material_id      | int      | 素材ID                |
| material_number  | int      | 素材编号              |
| template_id      | int      | 模板ID                |
| package_id       | int      | 包ID                  |
| package_name     | string   | 包名称                |
| user_id          | string   | 用户ID                |
| timestamp        | int      | 分享时间              |
| share_id         | string   | 分享ID                |
| share_content    | string   | 分享自定义内容        |
| log_id           | string   | 日志id                |
| share_result     | int      | 分享结果:0:失败1:成功 |

 

 

### 14、游戏关卡日志表

| 字段名称     | 字段类型 | 字段说明           |
| ------------ | -------- | ------------------ |
| plat_id      | string   | 平台id             |
| server_id    | int      | 服务器ID           |
| channel_id   | string   | 渠道ID             |
| user_id      | string   | 用户ID             |
| role_id      | string   | 角色ID             |
| role_name    | string   | 角色名称           |
| client_ip    | string   | 客户端IP           |
| event_time   | int      | 事件时间           |
| log_id       | string   | 日志ID             |
| level_before | int      | 玩家通关之前的关卡 |
| level_after  | int      | 玩家通关后的关卡   |

 

 

## 数据仓库的分层设计

### PPT

> P1:数据仓库的生态系统：
>
> <img src="数仓项目.assets/image-20200425042454213.png" alt="image-20200425042454213" style="zoom:50%;" />
>
> P2:数据仓库的相关技术：
>
> <img src="数仓项目.assets/image-20200425042419516.png" alt="image-20200425042419516" style="zoom: 40%;" />
>
> P3：数据仓库的常见体系结构:
>
> <img src="数仓项目.assets/image-20200425042532579.png" alt="image-20200425042532579" style="zoom: 42%;" />
>
> P4:数据仓库的体系架构大致可以分为以下几层：
>
> <img src="数仓项目.assets/image-20200425042723806.png" alt="image-20200425042723806" style="zoom:38%;" />
>
> P5:数据整合层
>
> <img src="数仓项目.assets/image-20200425042838030.png" alt="image-20200425042838030" style="zoom:42%;" />
>
> P6:数据仓库各层中对应的数据类别：
>
> <img src="数仓项目.assets/image-20200425043014746.png" alt="image-20200425043014746" style="zoom: 42%;" />
>
> P7:数据仓库架构——依赖数据集市型
>
> **数据集市 -- 小型的，面向部门或工作组级数据仓库**。即”小数据仓库”。如果说数据仓库是建立在企业级的数据模型之上的话。那么数据集市就是企业级数据仓库的一个子集，他主要面向部门级业务，并且只是面向某个特定的主题。数据集市可以在一定程度上缓解访问数据仓库的瓶颈。
>
> <img src="数仓项目.assets/image-20200425043122513.png" alt="image-20200425043122513" style="zoom:42%;" />
>
> <img src="数仓项目.assets/image-20200425043154384.png" alt="image-20200425043154384" style="zoom:42%;" />
>
> P8:数据仓库架构——依赖数据仓库型
>
> <img src="数仓项目.assets/image-20200425043254648.png" alt="image-20200425043254648" style="zoom:42%;" />
>
> <img src="数仓项目.assets/image-20200425043319320.png" alt="image-20200425043319320" style="zoom:42%;" />
>
> ——
>
> 

在我们实际工作当中，对数仓的设计一般都免不了要做分层的设计，一般的数据仓库可以分为三层或者四层等，根据自己公司的业务逻辑的负责，数据仓库的层次可能也是不一样的

### 为什么要分层

![image-20200425043540704](数仓项目.assets/image-20200425043540704.png)

 

### 本项目的数仓分层

本项目采用三层结构：不涉及到数据清洗的操作

1. ODS层：贴源层
2. DW层：对ODS层的数据进行汇总
3. RES层：结果输出展示层

如果想要加层的话，可以按照下面的来设计：

```
数据仓库的分层设计：
	三层：不用数据清洗过程  
		ods层：贴源层
		dw层：对ods层数据进行轻度的汇总
		res层  结果层  数据集市层
	四层：
		ods层  贴源层
		dwd层：对ods层的数据进行清洗操作
		dws层  对dwd层的数据进行轻度的汇总
		res层    结果层  数据集市层
	五层：
		ods层  贴源层
		dwd层：对ods层的数据进行清洗操作
		dws层  对dwd层的数据进行轻度的汇总
		dwm层：数据集市层
		res层    结果层  
```

![image-20200425043623721](数仓项目.assets/image-20200425043623721.png)

 

### 数据仓库建设规范

##### 数据日志采集

1、数据传输与协议，日志格式化存储在游戏服务器上，一份表分库形式存在，每个月一个表。

2、字段规范与约束，日志数据的属性key尽量复用。比如：用户id，都统一使用user_id，而不是有player_id、id、等不同key来表示用户id。字段命名格式统一以：小写、数字、下划线连接。比如：app_name（反面：appName或者AppName）。严格遵守value可选值。比如：channel可选值有appstore，但是不允许埋成app store等；

##### 数据层次规范

![数据仓库三层架构](数仓项目.assets/clip_image038.png)

 

数据仓库采用三层架构模式：ods、dw、res，对应各层的缩写分别：

**ods：Operation Data Store**

原始层，是数据仓库第一层数据，直接从原始数据过来的，这一层的数据跟我们采集到mysql中的数据是一样的，这个主要是为了和原始日志做同步，而且也可以被认作是原始日志的一个备份，如果后续计算数据有问题，可以通过原始数据核对来排查。

**dw：Data Warehouse**

中间层，存放数据仓库明细层的数据，这一层主要用于对数据进行整合和ETL处理之后存放的，可以在各个业务场景下共用的。因此，这一层整合的数据较多，因为在hive中，性能消耗最大的就是在于两个地方。

一、大表查询。这种情况就需要提前每天做一次规约聚合操作，将数据按照相应的维度进行一次细粒度的聚合，减少数据量。

二、多表关联。在hive中，多表之间的关联是性能消耗最厉害的地方。因此我们要在做一个大表的聚合关联，在这个时段期间给予这个聚合任务的大量资源来完成，这样后续结果集查询的数据就会很简单，少去很多关联，直接查询这个大表即可，提高查询效率。

**res: Result**

结果层，存放应用层的数据，直接提供给业务人员或报表使用。**这一层一般是从中间层查询计算好之后将数据输出到关系型数据库中**，由PHP或者Java web和前端合作展示到页面上。到了这一步，展示的工作就是PHP、web、前端的了，但是数仓工程师要在在后续继续维护数据的准确性。

##### 命名规范

database和table的命名等设计：

1. 整个数仓可以建立多个库，如create database ods; create database dw; create database res; 不同的database对应不同的层。
2. 也可以就使用一个库，数仓的层次是通过表前缀命名来区分的。比如：ods_ 开头表示原始层、dw_ 开头表示中间层、res_ 开头表示结果层。

****本项目采用第二种方式，即只创建一个database。****

对应的术语单元做语义翻译，可以采用拼音、英文、含义数字，但避免英文，拼音混用。拼音、英文尽量在不失原意的情况下采用缩写形式，一般采用行业常用术语、英文重点发音字母、避免数字开头。比如：

角色创建 ods_role_create、战斗力日志 ods_fight_power、角色充值  ods_role_recharge。

建表语句规范：

（1）**注意字段要加描述**。comment

（2）**指定分隔符**。

（3）**判断是否需要分区，如果是增量表就要分区**，并且所有分区字段都为 yyyy-MM-dd格式的date类型，字段名称都必须统一为part_date。如果这个表要全量更新，就不需要做分区。

规范语句如下：

```sql
CREATE TABLE ods_game_create(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID',
channel_id         string     comment '渠道id',
user_id            string     comment '用户ID',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
device_brand       string     comment '设备品牌 ',
device_type        string     comment '设备型号 ',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本'
)
comment '游戏创建创建日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

 

## ODS数据原始层设计

### 1、上传日志数据

node03创建目录：

```sh
mkdir -p /kkb/datas/gamecenter/
```

将我们通过java多线程采集到的日志数据上传到node03服务器的/kkb/datas/gamecenter这个路径下。

```sh
cd /kkb/datas/gamecenter/
rz

[hadoop@node03 gamecenter]$ ll
total 4085524
-rw-r--r-- 1 hadoop hadoop    7686315 Feb 10 15:34 ods_death_log.txt
-rw-r--r-- 1 hadoop hadoop     137268 Feb  8 17:09 ods_dim_game_button.txt
-rw-r--r-- 1 hadoop hadoop 2205476848 Dec 26 15:05 ods_fight_power.txt
-rw-r--r-- 1 hadoop hadoop   10482616 Dec 26 14:17 ods_game_create.txt
-rw-r--r-- 1 hadoop hadoop    4759133 Feb 10 15:34 ods_game_levelup.txt
-rw-r--r-- 1 hadoop hadoop    5797282 Feb  8 17:35 ods_monetary_consume.txt
-rw-r--r-- 1 hadoop hadoop    5829941 Feb  8 17:35 ods_monetary_output.txt
-rw-r--r-- 1 hadoop hadoop    5138799 Feb  8 17:35 ods_panel_op.txt
-rw-r--r-- 1 hadoop hadoop   14047817 Dec 26 14:18 ods_role_create.txt
-rw-r--r-- 1 hadoop hadoop  926068852 Dec 26 14:40 ods_role_level_up.txt
-rw-r--r-- 1 hadoop hadoop   22429171 Dec 26 14:40 ods_role_recharge.txt
-rw-r--r-- 1 hadoop hadoop  731548283 Feb  8 17:16 ods_task_log.txt
-rw-r--r-- 1 hadoop hadoop  243245810 Dec 26 14:48 ods_user_login.txt
-rw-r--r-- 1 hadoop hadoop     900062 Feb 10 15:34 ods_user_share.txt
```

当我们通过Java程序去采集数据的时候，我们要把采集下来的数据原封不动的存入hive，要在hive建表，这些最原始没有经过任何加工的数据就是原始数据，我们统一用ods_ 开头来命名表。

比如：游戏登录日志 user_login_201901

| 字段名称          | 字段类型 | 描述                    |
| ----------------- | -------- | ----------------------- |
| plat_id           | string   | 平台id                  |
| server_id         | int      | 区服id                  |
| channel_id        | string   | 渠道                    |
| user_id           | string   | 用户ID                  |
| role_id           | string   | 角色ID                  |
| role_name         | string   | 角色名称                |
| client_ip         | string   | 客户端IP                |
| event_time        | int      | 事件时间                |
| op_type           | string   | 操作类型(1:登录,-1登出) |
| online_time       | int      | 在线时长(s)             |
| operating_system  | string   | 操作系统名称            |
| operating_version | string   | 操作系统版本            |
| device_brand      | string   | 设备型号                |
| device_type       | string   | 设备品牌                |

我们将每个服区的表中的字段拉取出来，**其中平台id和区服id在MySQL数据库的表结构中是没有的**，因为游戏研发存储日志的时候他们是根据数据库地址来存储的。

我们要将所有分区分库分表中的数据集中到一个数据表中ods_user_login 中，**为了区分数据来源加入这两个字段。**

****下面，我们开始建表，通过动态分区的方式加载数据。****

### 2、建表——游戏登录登出

##### 建表准备

启动hiveserver2服务端

```sh
cd /kkb/install/hive-1.1.0-cdh5.14.2/
bin/hive --service hiveserver2
```

重新开启会话，然后进入hive客户端

```sh
cd /kkb/install/hive-1.1.0-cdh5.14.2/

bin/beeline
```

创建database game_center

```sql
create database if not exists  game_center;
use game_center;
```

##### 直接建表

创建游戏登录登出表——目标表

```sql
CREATE TABLE ods_user_login(
plat_id            string     comment '平台id',
server_id          int        comment '区服id',
channel_id         string     comment '渠道',
user_id            string     comment '用户ID',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
op_type            string     comment '操作类型(1:登录,-1登出)',
online_time        int        comment '在线时长(s)',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本',
device_brand       string     comment '设备型号',
device_type        string     comment '设备品牌'
)
comment '游戏登录登出'
PARTITIONED BY(part_date date)  #from_unixtime(event_time,'yyyy-MM-dd') as part_date
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

 创建游戏登录登出表——临时表

```sql
CREATE TABLE tmp_ods_user_login(
plat_id            string     comment '平台id',
server_id          int        comment '区服id',
channel_id         string     comment '渠道',
user_id            string     comment '用户ID',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
op_type            string     comment '操作类型(1:登录,-1登出)',
online_time        int        comment '在线时长(s)',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本',
device_brand       string     comment '设备型号',
device_type        string     comment '设备品牌'
)
comment '游戏登录登出-临时表，用于将数据通过动态分区载入ods_user_login中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

 加载数据到临时表：

```sql
load data local inpath '/kkb/datas/gamecenter/ods_user_login.txt' overwrite into table tmp_ods_user_login;
```

使用select方式，将数据通过动态分区载入ods_user_login中

```sql
set hive.exec.dynamic.partition=true; #【开启动态分区】
set hive.exec.dynamic.partition.mode=nostrict; #【动态分区模式为非严格模式】
set hive.exec.max.dynamic.partitions.pernode=1000; #【最大动态分区数量设置为1000】
insert overwrite table ods_user_login partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,op_type,online_time,operating_system,operating_version,device_brand,device_type,from_unixtime(event_time,'yyyy-MM-dd') as part_date
from tmp_ods_user_login;
```

##### 脚本建表

node03创建shell脚本

```sh
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_user_login.sh
```

大体思路：sql="xxx" ----> /kkb/install/hive-1.1.0-cdh5.14.2/bin/hive -e "$sql"

```sh
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive

sql="
create database if not exists  game_center;
use game_center;

CREATE TABLE if not exists  "$DBNAME".ods_user_login(
plat_id            string     comment '平台id',
server_id          int        comment '区服id',
channel_id         string     comment '渠道',
user_id            string     comment '用户ID',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
op_type            string     comment '操作类型(1:登录,-1登出)',
online_time        int        comment '在线时长(s)',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本',
device_brand       string     comment '设备型号',
device_type        string     comment '设备品牌'
)
comment '游戏登录登出'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists tmp_ods_user_login(
plat_id            string     comment '平台id',
server_id          int        comment '区服id',
channel_id         string     comment '渠道',
user_id            string     comment '用户ID',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
op_type            string     comment '操作类型(1:登录,-1登出)',
online_time        int        comment '在线时长(s)',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本',
device_brand       string     comment '设备型号',
device_type        string     comment '设备品牌'
)
comment '游戏登录登出-临时表，用于将数据通过动态分区载入ods_user_login中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_user_login.txt' overwrite into table tmp_ods_user_login;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;
insert overwrite table ods_user_login partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,op_type,online_time,operating_system,operating_version,device_brand,device_type,from_unixtime(event_time,'yyyy-MM-dd') as part_date 
from tmp_ods_user_login;

"

$hive_home -e "$sql"

```

### 3、建表——角色创建日志

##### 直接建表

目标表：

```sql
CREATE TABLE ods_role_create(
plat_id            string     comment '平台id',
server_id         int         comment '角色创建服          ',
channel_id        string      comment '渠道(平台)ID        ',
user_id           string      comment '用户ID              ',
role_id           string      comment '角色ID              ',
role_name         string      comment '角色名称            ',
client_ip         string      comment '客户端IP            ',
event_time        int         comment '事件时间            ',
job_name          string      comment '职业名称            ',
role_sex          int         comment '角色性别(0:男,1:女) '
)
comment '角色创建日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

 临时表：

```sql
CREATE TABLE tmp_ods_role_create(
plat_id            string     comment '平台id',
server_id         int         comment '角色创建服          ',
channel_id        string      comment '渠道(平台)ID        ',
user_id           string      comment '用户ID              ',
role_id           string      comment '角色ID              ',
role_name         string      comment '角色名称            ',
client_ip         string      comment '客户端IP            ',
event_time        int         comment '事件时间            ',
job_name          string      comment '职业名称            ',
role_sex          int         comment '角色性别(0:男,1:女) '
)
comment '角色创建日志-临时表，用于将数据通过动态分区载入ods_role_create中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

加载数据到临时表：

```sql
load data local inpath '/kkb/datas/gamecenter/ods_role_create.txt' overwrite into table tmp_ods_role_create;
```

使用动态分区加载数据到目标表

```sh
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_role_create partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,job_name,role_sex,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_role_create;
```

##### 脚本建表

node03:

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_role_create.sh
```

```sh
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
sql="
create database if not exists  game_center;
use game_center;

CREATE TABLE if not exists  ods_role_create(
plat_id            string     comment '平台id',
server_id         int         comment '角色创建服          ',
channel_id        string      comment '渠道(平台)ID        ',
user_id           string      comment '用户ID              ',
role_id           string      comment '角色ID              ',
role_name         string      comment '角色名称            ',
client_ip         string      comment '客户端IP            ',
event_time        int         comment '事件时间            ',
job_name          string      comment '职业名称            ',
role_sex          int         comment '角色性别(0:男,1:女) '
)
comment '角色创建日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists  tmp_ods_role_create(
plat_id            string     comment '平台id',
server_id         int         comment '角色创建服          ',
channel_id        string      comment '渠道(平台)ID        ',
user_id           string      comment '用户ID              ',
role_id           string      comment '角色ID              ',
role_name         string      comment '角色名称            ',
client_ip         string      comment '客户端IP            ',
event_time        int         comment '事件时间            ',
job_name          string      comment '职业名称            ',
role_sex          int         comment '角色性别(0:男,1:女) '
)
comment '角色创建日志-临时表，用于将数据通过动态分区载入ods_role_create中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_role_create.txt' overwrite into table tmp_ods_role_create;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_role_create partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,job_name,role_sex,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_role_create;


"

$hive_home -e "$sql"
```



### 4、建表——游戏创建创建日志

##### 直接建表

```sql
CREATE TABLE ods_game_create(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID',
channel_id         string     comment '渠道id',
user_id            string     comment '用户ID',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
device_brand       string     comment '设备品牌 ',
device_type        string     comment '设备型号 ',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本'
)
comment '游戏创建创建日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_game_create(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID',
channel_id         string     comment '渠道id',
user_id            string     comment '用户ID',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
device_brand       string     comment '设备品牌 ',
device_type        string     comment '设备型号 ',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本'
)
comment '游戏创建创建日志-临时表，用于将数据通过动态分区载入ods_game_create中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_game_create.txt' overwrite into table tmp_ods_game_create;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_game_create partition(part_date)
select plat_id,server_id,channel_id,user_id,client_ip,event_time,device_brand,device_type,operating_system,operating_version,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_game_create;

```



##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_game_create.sh
```

```sh
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
sql="
create database if not exists  game_center;
use game_center;

CREATE TABLE if not exists  ods_game_create(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID',
channel_id         string     comment '渠道id',
user_id            string     comment '用户ID',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
device_brand       string     comment '设备品牌 ',
device_type        string     comment '设备型号 ',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本'
)
comment '游戏创建创建日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE  if not exists  tmp_ods_game_create(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID',
channel_id         string     comment '渠道id',
user_id            string     comment '用户ID',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
device_brand       string     comment '设备品牌 ',
device_type        string     comment '设备型号 ',
operating_system   string     comment '操作系统名称',
operating_version  string     comment '操作系统版本'
)
comment '游戏创建创建日志-临时表，用于将数据通过动态分区载入ods_game_create中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_game_create.txt' overwrite into table tmp_ods_game_create;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_game_create partition(part_date)
select plat_id,server_id,channel_id,user_id,client_ip,event_time,device_brand,device_type,operating_system,operating_version,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_game_create;
"
$hive_home -e "$sql"

```

 

### 5、建表——战斗力日志

##### 直接建表

```sql
CREATE TABLE ods_fight_power(
plat_id         string     comment '平台id',
server_id       int        comment '服务器ID  ',
channel_id      string     comment '渠道ID    ',
user_id         string     comment '用户ID    ',
role_id         string     comment '角色ID    ',
role_name       string     comment '角色名称  ', 
client_ip       string     comment '客户端IP  ',
event_time      int        comment '事件时间  ', 
change_count    bigint     comment '战斗力变化',  
combat_power    bigint     comment '当前战斗力'
)
comment '战斗力日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_fight_power(
plat_id         string     comment '平台id',
server_id       int        comment '服务器ID  ',
channel_id      string     comment '渠道ID    ',
user_id         string     comment '用户ID    ',
role_id         string     comment '角色ID    ',
role_name       string     comment '角色名称  ', 
client_ip       string     comment '客户端IP  ',
event_time      int        comment '事件时间  ', 
change_count    bigint     comment '战斗力变化',  
combat_power    bigint     comment '当前战斗力'
)
comment '战斗力日志-临时表，用于将数据通过动态分区载入ods_fight_power中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_fight_power.txt' overwrite into table tmp_ods_fight_power;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_fight_power partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,change_count,combat_power,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_fight_power;

```

#####  脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_fight_power.sh
```

```sh
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive

sql="
create database if not exists  game_center;
use game_center;
CREATE TABLE if not exists  ods_fight_power(
plat_id         string     comment '平台id',
server_id       int        comment '服务器ID  ',
channel_id      string     comment '渠道ID    ',
user_id         string     comment '用户ID    ',
role_id         string     comment '角色ID    ',
role_name       string     comment '角色名称  ', 
client_ip       string     comment '客户端IP  ',
event_time      int        comment '事件时间  ', 
change_count    bigint     comment '战斗力变化',  
combat_power    bigint     comment '当前战斗力'
)
comment '战斗力日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists tmp_ods_fight_power(
plat_id         string     comment '平台id',
server_id       int        comment '服务器ID  ',
channel_id      string     comment '渠道ID    ',
user_id         string     comment '用户ID    ',
role_id         string     comment '角色ID    ',
role_name       string     comment '角色名称  ', 
client_ip       string     comment '客户端IP  ',
event_time      int        comment '事件时间  ', 
change_count    bigint     comment '战斗力变化',  
combat_power    bigint     comment '当前战斗力'
)
comment '战斗力日志-临时表，用于将数据通过动态分区载入ods_fight_power中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_fight_power.txt' overwrite into table tmp_ods_fight_power;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_fight_power partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,change_count,combat_power,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_fight_power;


"

$hive_home -e "$sql"
```



### 6、建表——角色升级日志

##### 直接建表

```sql
CREATE TABLE ods_role_level_up(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID     ',
channel_id         string     comment '渠道(平台)ID ',
user_id            string     comment '用户ID       ',
role_id            string     comment '角色ID       ',
role_name          string     comment '角色名称     ',
client_ip          string     comment '客户端IP     ',
event_time         int        comment '事件时间     ',
level_befor        int        comment '升级前等级   ',
level_after        int        comment '升级后等级   '
)
comment '角色升级日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_role_level_up(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID     ',
channel_id         string     comment '渠道(平台)ID ',
user_id            string     comment '用户ID       ',
role_id            string     comment '角色ID       ',
role_name          string     comment '角色名称     ',
client_ip          string     comment '客户端IP     ',
event_time         int        comment '事件时间     ',
level_befor        int        comment '升级前等级   ',
level_after        int        comment '升级后等级   '
)
comment '角色升级日志-临时表，用于将数据通过动态分区载入ods_role_level_up中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_role_level_up.txt' overwrite into table tmp_ods_role_level_up;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_role_level_up partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,level_befor,level_after,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_role_level_up;

```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_role_level_up.sh
```

```sh
#!/bin/bash

DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive

sql="
create database if not exists  game_center;
use game_center;
CREATE TABLE ods_role_level_up(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID     ',
channel_id         string     comment '渠道(平台)ID ',
user_id            string     comment '用户ID       ',
role_id            string     comment '角色ID       ',
role_name          string     comment '角色名称     ',
client_ip          string     comment '客户端IP     ',
event_time         int        comment '事件时间     ',
level_befor        int        comment '升级前等级   ',
level_after        int        comment '升级后等级   '
)
comment '角色升级日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_role_level_up(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID     ',
channel_id         string     comment '渠道(平台)ID ',
user_id            string     comment '用户ID       ',
role_id            string     comment '角色ID       ',
role_name          string     comment '角色名称     ',
client_ip          string     comment '客户端IP     ',
event_time         int        comment '事件时间     ',
level_befor        int        comment '升级前等级   ',
level_after        int        comment '升级后等级   '
)
comment '角色升级日志-临时表，用于将数据通过动态分区载入ods_role_level_up中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_role_level_up.txt' overwrite into table tmp_ods_role_level_up;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_role_level_up partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,level_befor,level_after,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_role_level_up;

"
$hive_home -e "$sql"

```

 

### 7、建表——角色充值日志

##### 直接建表

```sql
CREATE TABLE ods_role_recharge(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID       ',
channel_id         string     comment '渠道ID         ',
user_id            string     comment '用户ID         ',
role_id            string     comment '角色ID         ',
role_name          string     comment '角色名称       ',
event_time         int        comment '事件时间       ',
order_id           string     comment '订单ID         ',
acer_count         int        comment '获得的元宝数量 ',
recharge_amount    double     comment '充值金额       ',
order_status       int        comment '订单状态       ',
recharge_purpose   int        comment '充值用途（0:商城充值,充值获得元宝的形式1:月卡充值2:礼包充值）'
)
comment '角色充值日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_role_recharge(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID       ',
channel_id         string     comment '渠道ID         ',
user_id            string     comment '用户ID         ',
role_id            string     comment '角色ID         ',
role_name          string     comment '角色名称       ',
event_time         int        comment '事件时间       ',
order_id           string     comment '订单ID         ',
acer_count         int        comment '获得的元宝数量 ',
recharge_amount    double     comment '充值金额       ',
order_status       int        comment '订单状态       ',
recharge_purpose   int        comment '充值用途（0:商城充值,充值获得元宝的形式1:月卡充值2:礼包充值）'
)
comment '角色充值日志-临时表，用于将数据通过动态分区载入ods_role_recharge中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


load data local inpath '/kkb/datas/gamecenter/ods_role_recharge.txt' overwrite into table tmp_ods_role_recharge;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_role_recharge partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,event_time,order_id,acer_count,recharge_amount,order_status,recharge_purpose,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_role_recharge;
```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_role_recharge.sh
```

```sh
#!/bin/bash


DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive


sql="
create database if not exists  game_center;
use game_center;

CREATE TABLE if not exists  ods_role_recharge(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID       ',
channel_id         string     comment '渠道ID         ',
user_id            string     comment '用户ID         ',
role_id            string     comment '角色ID         ',
role_name          string     comment '角色名称       ',
event_time         int        comment '事件时间       ',
order_id           string     comment '订单ID         ',
acer_count         int        comment '获得的元宝数量 ',
recharge_amount    double     comment '充值金额       ',
order_status       int        comment '订单状态       ',
recharge_purpose   int        comment '充值用途（0:商城充值,充值获得元宝的形式1:月卡充值2:礼包充值）'
)
comment '角色充值日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists  tmp_ods_role_recharge(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID       ',
channel_id         string     comment '渠道ID         ',
user_id            string     comment '用户ID         ',
role_id            string     comment '角色ID         ',
role_name          string     comment '角色名称       ',
event_time         int        comment '事件时间       ',
order_id           string     comment '订单ID         ',
acer_count         int        comment '获得的元宝数量 ',
recharge_amount    double     comment '充值金额       ',
order_status       int        comment '订单状态       ',
recharge_purpose   int        comment '充值用途（0:商城充值,充值获得元宝的形式1:月卡充值2:礼包充值）'
)
comment '角色充值日志-临时表，用于将数据通过动态分区载入ods_role_recharge中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


load data local inpath '/kkb/datas/gamecenter/ods_role_recharge.txt' overwrite into table tmp_ods_role_recharge;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_role_recharge partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,event_time,order_id,acer_count,recharge_amount,order_status,recharge_purpose,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_role_recharge;


"

$hive_home -e "$sql"
```

 

 

### 8、建表——任务日志

##### 直接建表

```sql
CREATE TABLE ods_task_log(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID       ',
channel_id         string     comment '渠道ID         ',
user_id            string     comment '用户ID         ',
role_id            string     comment '角色ID         ',
role_name          string     comment '角色名称       ',
event_time         int        comment '事件时间       ',
task_type          int        comment '任务类型  ',
task_id            int        comment '任务ID    ',
cost_time          int        comment '任务耗时  ',
op_type            int        comment '操作类型  ',
level_limit        int        comment '等级约束  ',
award_exp          bigint     comment '奖励经验  ',
award_monetary     string     comment '奖励货币  ',
award_item         string     comment '奖励道具  ',
death_count        int        comment '死亡次数  ',
award_attribute    string     comment '奖励属性  '
)
comment '任务日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


CREATE TABLE tmp_ods_task_log(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID       ',
channel_id         string     comment '渠道ID         ',
user_id            string     comment '用户ID         ',
role_id            string     comment '角色ID         ',
role_name          string     comment '角色名称       ',
event_time         int        comment '事件时间       ',
task_type          int        comment '任务类型  ',
task_id            int        comment '任务ID    ',
cost_time          int        comment '任务耗时  ',
op_type            int        comment '操作类型  ',
level_limit        int        comment '等级约束  ',
award_exp          bigint     comment '奖励经验  ',
award_monetary     string     comment '奖励货币  ',
award_item         string     comment '奖励道具  ',
death_count        int        comment '死亡次数  ',
award_attribute    string     comment '奖励属性  '
)
comment '任务日志-临时表，用于将数据通过动态分区载入ods_task_log中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_task_log.txt' overwrite into table tmp_ods_task_log;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_task_log partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,event_time,task_type,task_id,cost_time,op_type,level_limit,award_exp,award_monetary,award_item,death_count,award_attribute,
from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_task_log;


```

##### 脚本建表

```sh
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_task_log.sh
```

```sh
#!/bin/bash


DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive


sql="
create database if not exists  game_center;
use game_center;

CREATE TABLE if not exists ods_task_log(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID       ',
channel_id         string     comment '渠道ID         ',
user_id            string     comment '用户ID         ',
role_id            string     comment '角色ID         ',
role_name          string     comment '角色名称       ',
event_time         int        comment '事件时间       ',
task_type          int        comment '任务类型  ',
task_id            int        comment '任务ID    ',
cost_time          int        comment '任务耗时  ',
op_type            int        comment '操作类型  ',
level_limit        int        comment '等级约束  ',
award_exp          bigint     comment '奖励经验  ',
award_monetary     string     comment '奖励货币  ',
award_item         string     comment '奖励道具  ',
death_count        int        comment '死亡次数  ',
award_attribute    string     comment '奖励属性  '
)
comment '任务日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


CREATE TABLE if not exists tmp_ods_task_log(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID       ',
channel_id         string     comment '渠道ID         ',
user_id            string     comment '用户ID         ',
role_id            string     comment '角色ID         ',
role_name          string     comment '角色名称       ',
event_time         int        comment '事件时间       ',
task_type          int        comment '任务类型  ',
task_id            int        comment '任务ID    ',
cost_time          int        comment '任务耗时  ',
op_type            int        comment '操作类型  ',
level_limit        int        comment '等级约束  ',
award_exp          bigint     comment '奖励经验  ',
award_monetary     string     comment '奖励货币  ',
award_item         string     comment '奖励道具  ',
death_count        int        comment '死亡次数  ',
award_attribute    string     comment '奖励属性  '
)
comment '任务日志-临时表，用于将数据通过动态分区载入ods_task_log中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_task_log.txt' overwrite into table tmp_ods_task_log;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_task_log partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,event_time,task_type,task_id,cost_time,op_type,level_limit,award_exp,award_monetary,award_item,death_count,award_attribute,
from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_task_log;

"

$hive_home -e "$sql"
```

 

### 9、建表——游戏按钮操作日志

##### 直接建表

```sql
CREATE TABLE ods_panel_op(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志ID',
button_id          string     comment '按钮ID'
)
comment '游戏按钮操作日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_panel_op(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志ID',
button_id          string     comment '按钮ID'
)
comment '游戏按钮操作日志-临时表，用于将数据通过动态分区载入ods_panel_op中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_panel_op.txt' overwrite into table tmp_ods_panel_op;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_panel_op partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,button_id,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_panel_op;

```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_panel_op.sh
```

```
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
sql="
create database if not exists  game_center;
use game_center;
CREATE TABLE if not exists ods_panel_op(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志ID',
button_id          string     comment '按钮ID'
)
comment '游戏按钮操作日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists tmp_ods_panel_op(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志ID',
button_id          string     comment '按钮ID'
)
comment '游戏按钮操作日志-临时表，用于将数据通过动态分区载入ods_panel_op中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_panel_op.txt' overwrite into table tmp_ods_panel_op;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_panel_op partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,button_id,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_panel_op;



"

$hive_home -e "$sql"

```

 



### 10、建表——游戏按钮字典，手动维护，load数据

##### 直接建表

```sql
create table ods_dim_game_button(
plat_id              string       comment '游戏',
button_id            int          comment '按钮ID',
button_name          string       comment '按钮名称',
system_type          string       comment '按钮类型名称'
) comment '游戏按钮字典'
row format delimited fields terminated by '\t'
collection items terminated by ','
map keys terminated by ':'
lines terminated by '\n'
stored as textfile;

load data local inpath '/kkb/datas/gamecenter/ods_dim_game_button.txt' overwrite into table ods_dim_game_button;

```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_dim_game_button.sh
```

```sh
#!/bin/bash


DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive


sql="
create database if not exists  game_center;
use game_center;

create table if not exists ods_dim_game_button(
plat_id              string       comment '游戏',
button_id            int          comment '按钮ID',
button_name          string       comment '按钮名称',
system_type          string       comment '按钮类型名称'
) comment '游戏按钮字典'
row format delimited fields terminated by '\t'
collection items terminated by ','
map keys terminated by ':'
lines terminated by '\n'
stored as textfile;

load data local inpath '/kkb/datas/gamecenter/ods_dim_game_button.txt' overwrite into table ods_dim_game_button;




"

$hive_home -e "$sql"

```

 

 

 

### 11、建表——虚拟币产出日志

##### 直接建表

```sql
CREATE TABLE ods_monetary_output(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志id ',
monetary_type      int        comment '货币类型(1:元宝,2:金币,3:礼金 )',
output_type        int        comment '产出类型ID ',
output_count       bigint     comment '产出数量 ',
monetary_stock     bigint     comment '货币存量 '
)
comment '虚拟币产出日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_monetary_output(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志id ',
monetary_type      int        comment '货币类型(1:元宝,2:金币,3:礼金 )',
output_type        int        comment '产出类型ID ',
output_count       bigint     comment '产出数量 ',
monetary_stock     bigint     comment '货币存量 '
)
comment '虚拟币产出日志-临时表，用于将数据通过动态分区载入ods_monetary_output中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_monetary_output.txt' overwrite into table tmp_ods_monetary_output;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_monetary_output partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,monetary_type,output_type,output_count,monetary_stock,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_monetary_output;

```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_monetary_output.sh
```

```sh
#!/bin/bash


DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive


sql="
create database if not exists  game_center;
use game_center;

CREATE TABLE  if not exists ods_monetary_output(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志id ',
monetary_type      int        comment '货币类型(1:元宝,2:金币,3:礼金 )',
output_type        int        comment '产出类型ID ',
output_count       bigint     comment '产出数量 ',
monetary_stock     bigint     comment '货币存量 '
)
comment '虚拟币产出日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists tmp_ods_monetary_output(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志id ',
monetary_type      int        comment '货币类型(1:元宝,2:金币,3:礼金 )',
output_type        int        comment '产出类型ID ',
output_count       bigint     comment '产出数量 ',
monetary_stock     bigint     comment '货币存量 '
)
comment '虚拟币产出日志-临时表，用于将数据通过动态分区载入ods_monetary_output中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_monetary_output.txt' overwrite into table tmp_ods_monetary_output;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_monetary_output partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,monetary_type,output_type,output_count,monetary_stock,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_monetary_output;


"

$hive_home -e "$sql"

```

 



### 12、建表——虚拟币消费日志

##### 直接建表

```sql
CREATE TABLE ods_monetary_consume(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志id ',
monetary_type      int        comment '货币类型(1:元宝,2:金币,3:礼金 )',
consume_type        int        comment '产出类型ID ',
consume_count       bigint     comment '产出数量 ',
monetary_stock     bigint     comment '货币存量 '
)
comment '虚拟币消费日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_monetary_consume(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志id ',
monetary_type      int        comment '货币类型(1:元宝,2:金币,3:礼金 )',
consume_type        int        comment '产出类型ID ',
consume_count       bigint     comment '产出数量 ',
monetary_stock     bigint     comment '货币存量 '
)
comment '虚拟币消费日志-临时表，用于将数据通过动态分区载入ods_monetary_consume中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_monetary_consume.txt' overwrite into table tmp_ods_monetary_consume;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_monetary_consume partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,monetary_type,consume_type,consume_count,monetary_stock,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_monetary_consume;

```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_monetary_consume.sh
```

```sh
#!/bin/bash

DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive


sql="
create database if not exists  game_center;
use game_center;

CREATE TABLE if not exists ods_monetary_consume(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志id ',
monetary_type      int        comment '货币类型(1:元宝,2:金币,3:礼金 )',
consume_type        int        comment '产出类型ID ',
consume_count       bigint     comment '产出数量 ',
monetary_stock     bigint     comment '货币存量 '
)
comment '虚拟币消费日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists  tmp_ods_monetary_consume(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志id ',
monetary_type      int        comment '货币类型(1:元宝,2:金币,3:礼金 )',
consume_type        int        comment '产出类型ID ',
consume_count       bigint     comment '产出数量 ',
monetary_stock     bigint     comment '货币存量 '
)
comment '虚拟币消费日志-临时表，用于将数据通过动态分区载入ods_monetary_consume中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_monetary_consume.txt' overwrite into table tmp_ods_monetary_consume;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_monetary_consume partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,monetary_type,consume_type,consume_count,monetary_stock,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_monetary_consume;



"

$hive_home -e "$sql"

```

  

### 13、建表——玩家死亡日志

##### 直接建表

```sql
CREATE TABLE ods_death_log(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称 ',
client_ip          string     comment '客户端IP ',
event_time         int        comment '事件时间 ',
log_id             string     comment '日志ID',
map_id             int        comment '地图ID',
attacker_uid       string     comment '凶手用户ID/怪物id ',
attacker_rid       string     comment '凶手角色ID/怪物id ',
attacker_rname     string     comment '凶手/怪物名称',
attacker_level     int        comment '凶手/怪物等级',
death_level        int        comment '死亡等级',
hero_power         bigint     comment '当前玩家战斗力或者伤害',
death_detail       string     comment '死亡详细'
)
comment '玩家死亡日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_death_log(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称 ',
client_ip          string     comment '客户端IP ',
event_time         int        comment '事件时间 ',
log_id             string     comment '日志ID',
map_id             int        comment '地图ID',
attacker_uid       string     comment '凶手用户ID/怪物id ',
attacker_rid       string     comment '凶手角色ID/怪物id ',
attacker_rname     string     comment '凶手/怪物名称',
attacker_level     int        comment '凶手/怪物等级',
death_level        int        comment '死亡等级',
hero_power         bigint     comment '当前玩家战斗力或者伤害',
death_detail       string     comment '死亡详细'
)
comment '玩家死亡日志-临时表，用于将数据通过动态分区载入ods_death_log中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_death_log.txt' overwrite into table tmp_ods_death_log;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_death_log partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,map_id,attacker_uid,attacker_rid,attacker_rname,attacker_level,death_level,hero_power,death_detail,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_death_log;

```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_death_log.sh
```

```sh
#!/bin/bash

DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive


sql="
create database if not exists  game_center;
use game_center;
CREATE TABLE if not exists ods_death_log(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称 ',
client_ip          string     comment '客户端IP ',
event_time         int        comment '事件时间 ',
log_id             string     comment '日志ID',
map_id             int        comment '地图ID',
attacker_uid       string     comment '凶手用户ID/怪物id ',
attacker_rid       string     comment '凶手角色ID/怪物id ',
attacker_rname     string     comment '凶手/怪物名称',
attacker_level     int        comment '凶手/怪物等级',
death_level        int        comment '死亡等级',
hero_power         bigint     comment '当前玩家战斗力或者伤害',
death_detail       string     comment '死亡详细'
)
comment '玩家死亡日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE  if not exists tmp_ods_death_log(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID ',
role_id            string     comment '角色ID ',
role_name          string     comment '角色名称 ',
client_ip          string     comment '客户端IP ',
event_time         int        comment '事件时间 ',
log_id             string     comment '日志ID',
map_id             int        comment '地图ID',
attacker_uid       string     comment '凶手用户ID/怪物id ',
attacker_rid       string     comment '凶手角色ID/怪物id ',
attacker_rname     string     comment '凶手/怪物名称',
attacker_level     int        comment '凶手/怪物等级',
death_level        int        comment '死亡等级',
hero_power         bigint     comment '当前玩家战斗力或者伤害',
death_detail       string     comment '死亡详细'
)
comment '玩家死亡日志-临时表，用于将数据通过动态分区载入ods_death_log中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_death_log.txt' overwrite into table tmp_ods_death_log;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_death_log partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,map_id,attacker_uid,attacker_rid,attacker_rname,attacker_level,death_level,hero_power,death_detail,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_death_log;

"

$hive_home -e "$sql"
```

 

### 14、建表——玩家分享行为日志

##### 直接建表

```sql
CREATE TABLE ods_user_share(
plat_id            string     comment '平台id',
channel_id         int        comment '渠道ID',
game_channel_id    int        comment '子渠道id',
advertisement_id   int        comment '广告ID',
material_id        int        comment '素材ID',
material_number    int        comment '素材编号',
template_id        int        comment '模板ID',
package_id         int        comment '包ID',
package_name       string     comment '包名称',
user_id            string     comment '用户ID',
event_time          int        comment '分享时间',
share_id           string     comment '分享ID',
share_content      string     comment '分享自定义内容',
log_id             string     comment '日志id',
share_result       int        comment '分享结果:0:失败,1:成功'
)
comment '玩家分享行为日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_user_share(
plat_id            string     comment '平台id',
channel_id         int        comment '渠道ID',
game_channel_id    int        comment '子渠道id',
advertisement_id   int        comment '广告ID',
material_id        int        comment '素材ID',
material_number    int        comment '素材编号',
template_id        int        comment '模板ID',
package_id         int        comment '包ID',
package_name       string     comment '包名称',
user_id            string     comment '用户ID',
event_time          int        comment '分享时间',
share_id           string     comment '分享ID',
share_content      string     comment '分享自定义内容',
log_id             string     comment '日志id',
share_result       int        comment '分享结果:0:失败,1:成功'
)
comment '玩家分享行为日志-临时表，用于将数据通过动态分区载入ods_user_share中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_user_share.txt' overwrite into table tmp_ods_user_share;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_user_share partition(part_date)
select plat_id,channel_id,game_channel_id,advertisement_id,material_id,material_number,template_id,package_id,package_name,user_id,event_time,share_id,share_content,log_id,share_result,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_user_share;

```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_user_share.sh
```

```sh
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive

sql="

create database if not exists  game_center;
use game_center;

CREATE TABLE if not exists ods_user_share(
plat_id            string     comment '平台id',
channel_id         int        comment '渠道ID',
game_channel_id    int        comment '子渠道id',
advertisement_id   int        comment '广告ID',
material_id        int        comment '素材ID',
material_number    int        comment '素材编号',
template_id        int        comment '模板ID',
package_id         int        comment '包ID',
package_name       string     comment '包名称',
user_id            string     comment '用户ID',
event_time          int        comment '分享时间',
share_id           string     comment '分享ID',
share_content      string     comment '分享自定义内容',
log_id             string     comment '日志id',
share_result       int        comment '分享结果:0:失败,1:成功'
)
comment '玩家分享行为日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists tmp_ods_user_share(
plat_id            string     comment '平台id',
channel_id         int        comment '渠道ID',
game_channel_id    int        comment '子渠道id',
advertisement_id   int        comment '广告ID',
material_id        int        comment '素材ID',
material_number    int        comment '素材编号',
template_id        int        comment '模板ID',
package_id         int        comment '包ID',
package_name       string     comment '包名称',
user_id            string     comment '用户ID',
event_time          int        comment '分享时间',
share_id           string     comment '分享ID',
share_content      string     comment '分享自定义内容',
log_id             string     comment '日志id',
share_result       int        comment '分享结果:0:失败,1:成功'
)
comment '玩家分享行为日志-临时表，用于将数据通过动态分区载入ods_user_share中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_user_share.txt' overwrite into table tmp_ods_user_share;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_user_share partition(part_date)
select plat_id,channel_id,game_channel_id,advertisement_id,material_id,material_number,template_id,package_id,package_name,user_id,event_time,share_id,share_content,log_id,share_result,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_user_share;



"

$hive_home -e "$sql"

```

 

### 15、建表——游戏关卡日志

##### 直接建表

```sql
CREATE TABLE ods_game_levelup(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志ID',
level_before       int        comment '玩家通关之前的关卡',
level_after        int        comment '玩家通关后的关卡'
)
comment '游戏关卡日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE tmp_ods_game_levelup(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志ID',
level_before       int        comment '玩家通关之前的关卡',
level_after        int        comment '玩家通关后的关卡'
)
comment '游戏关卡日志-临时表，用于将数据通过动态分区载入ods_game_levelup中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_game_levelup.txt' overwrite into table tmp_ods_game_levelup;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_game_levelup partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,level_before,level_after,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_game_levelup;


```

##### 脚本建表

```
mkdir -p /home/hadoop/bin/
cd /home/hadoop/bin/
vim ods_game_levelup.sh
```

```sh
#!/bin/bash

DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive

sql="
create database if not exists  game_center;
use game_center;

CREATE TABLE if not exists ods_game_levelup(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志ID',
level_before       int        comment '玩家通关之前的关卡',
level_after        int        comment '玩家通关后的关卡'
)
comment '游戏关卡日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

CREATE TABLE if not exists tmp_ods_game_levelup(
plat_id            string     comment '平台id',
server_id          int        comment '服务器ID ',
channel_id         string     comment '渠道ID ',
user_id            string     comment '用户ID',
role_id            string     comment '角色ID',
role_name          string     comment '角色名称',
client_ip          string     comment '客户端IP',
event_time         int        comment '事件时间',
log_id             string     comment '日志ID',
level_before       int        comment '玩家通关之前的关卡',
level_after        int        comment '玩家通关后的关卡'
)
comment '游戏关卡日志-临时表，用于将数据通过动态分区载入ods_game_levelup中'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '/kkb/datas/gamecenter/ods_game_levelup.txt' overwrite into table tmp_ods_game_levelup;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions.pernode=1000;

insert overwrite table ods_game_levelup partition(part_date)
select plat_id,server_id,channel_id,user_id,role_id,role_name,client_ip,event_time,log_id,level_before,level_after,from_unixtime(event_time,'yyyy-MM-dd') as part_date from tmp_ods_game_levelup;



"

$hive_home -e "$sql"

```

 

 

### 16、一键执行所有的ods层的数据库表

开发shell脚本，一键执行ods层所有表的建表脚本。

node03开发以下shell脚本

```
cd /home/hadoop/bin
vim execute_ods.sh
```

```sh
#!/bin/bash
odsshell=`ls ods*`
#echo "$odsshell"

for m in $odsshell
do
 echo $m
 /usr/bin/sh $m
done
```

 执行脚本

```
cd /home/hadoop/bin	
sh execute_ods.sh
```

查看效果

```sh
show tables;
+---------------------------+--+
|         tab_name          |
+---------------------------+--+
| ods_death_log             |
| ods_dim_game_button       |
| ods_fight_power           |
| ods_game_create           |
| ods_game_levelup          |
| ods_monetary_consume      |
| ods_monetary_output       |
| ods_panel_op              |
| ods_role_create           |
| ods_role_level_up         |
| ods_role_recharge         |
| ods_task_log              |
| ods_user_login            |
| ods_user_share            |
| tmp_ods_death_log         |
| tmp_ods_fight_power       |
| tmp_ods_game_create       |
| tmp_ods_game_levelup      |
| tmp_ods_monetary_consume  |
| tmp_ods_monetary_output   |
| tmp_ods_panel_op          |
| tmp_ods_role_create       |
| tmp_ods_role_level_up     |
| tmp_ods_role_recharge     |
| tmp_ods_task_log          |
| tmp_ods_user_login        |
| tmp_ods_user_share        |
+---------------------------+--+
```



## :rainbow:游戏数据仓库架构设计以及中间DW层

## 1.常见数据仓库架构体系

#### 数据仓库的特点和意义

现在互联网时代的电子商务，门户网站都会产生海量的数据，并且都可以通过对数据的挖掘和分析来提升转化率和转化收益。

国内外都有专门提供数据分析服务的公司，比如国内最早深入游戏领域的TalkingData 的 Game Analytics，国外还有 Kontagent、Localytics等等这些。

这些产品的优势在于能获取到行业内比较通用的指标来判断游戏的运营状况，节省开发成本，运营简单上手。缺点就在于无法满足复杂的定制化数据需求，而且这些公司还会收集自己游戏的数据，导致自己的数据流失风险增大。

因此我们要建立自己的数据仓库，**让各个游戏数据都集中到这个仓库里面，打破信息孤岛，形成统一的数据中心，实现第三方工具实现不了的作用**：

**数据理解：**                           

 ![image-20200426005050694](数仓项目.assets/image-20200426005050694.png)

数据仓库是面向主题的，所以其自身与业务结合就相对紧密和完善，更方便数据分析师基于数据理解业务。而数据仓库是有很多的主题组成，包括了很多的数据。当我们需要对数据进行分析的时候，如果理解数据仓库的模型，数据理解也就水到渠成了。

**数据质量：**

 ![image-20200426005118989](数仓项目.assets/image-20200426005118989.png)

 我们在做数据分析的时候要求数据是干净、完整的，而**数据仓库已经对源系统的数据进行了业务契合的转换，以及脏数据的清洗，这就为数据分析的数据质量做了较好的保障**。

**数据跨系统关联：**

 ![image-20200426005130096](数仓项目.assets/image-20200426005130096.png)

跨系统关联问题，进行数据整合时，总是需要找到共同点来关联来自不同系统的信息，而**数据仓库在数据采集和ETL过程中就会整合相关用户信息，完美解决跨系统关联问题，我们就可以根据相应的规则将数据进行关联查询**。

比如**游戏跨服务器的分表分库，在不同的游戏之间，可以通过游戏用户的手机号，Mac地址，IP地址，设备唯一键等信息进行关联，从而分析不同游戏之间的相互关联性和共通性**。

 

#### hadoop平台数据仓库架构

 

 ![image-20200426005140397](数仓项目.assets/image-20200426005140397.png)

##### 数据采集层：

我们前面提到的分表分库，采用Java多线程的方式去采集mysql中的日志。**数据采集层的任务就是把数据从各种数据源中采集和存储到数据库上，期间有可能会做一些ETL（抽取extra，转化transfer，装载load ）操作**。数据源种类可以有多种， 日志所占份额最大，存储在备份服务器上的数据库，如Mysql、Oracle。还有来自HTTP/FTP的数据，比如合作伙伴提供的接口所获取的数据，还有一些其他数据源：如Excel等需要手工录入的数据。

##### 数据存储与分析：

HDFS是大数据环境下数据仓库/数据平台最完美的数据存储解决方案。**离线数据分析与计算，也就是对实时性要求不高的部分，Hive是不错的选择**。 我们通过hive作为元信息的表结构化管理，可以通过spark sql或者 impala，或者presto sql 去操作查询。

##### 数据输出：

前面使用Hive、MR、Spark、SparkSQL分析和计算的结果，还是在HDFS上，但**大多业务和应用不可能直接从HDFS上获取数据，那么就需要一个数据共享的地方**，使得各业务和产品能方便的获取数据。 这里的数据共享，其实指的是前面数据分析与计算后的结果存放的地方，**其实就是关系型数据库mysql或者其他NOSQL的数据库。一般都是采用mysql**

##### 数据应用：

**报表，业务系统，运营系统，等公司系统所使用的数据，通常是数据集市层直接查询**，一般也是已经统计汇总好的，存放于数据集市层中通过直接操作SQL得到。

#### 数据仓库之维度建模

##### 1、两种不同的建模思想

要建设数据仓库，就不得不提到**数仓建模**。**Inmon和Kimball是最常见的两种架构**。

**Inmon主张自上而下的架构，它将数据仓库定义为整个企业级的集中存储**。数据仓库存放着最低的详细级别的原子数据。维度数据集市只是在数据仓库完成后才创建的。因此，数据仓库是企业信息工厂（CIF）的中心，它为交付商业智能提供逻辑框架。**通俗解释：先建立数据仓库，然后根据不同业务建立数据集市。**

不同的OLTP数据集中到面向主题、集成的、不易失的和时间变化的结构中，用于以后的分析。且数据可以通过下钻到最细层，或者上卷到汇总层。数据集市应该是数据仓库的子集，每个数据集市是针对独立部门特殊设计的。

 

 

 ![image-20200426005208823](数仓项目.assets/image-20200426005208823.png)

而**Kimball正好与Inmon相反，Kimball架构是一种自下而上的架构，它认为数据仓库是一系列数据集市的集合**。它首先建立最重要的业务单元或部门的数据集市。这些数据集市可以为透视组织数据提供一个较窄的视图，需要的时候，这些数据集市还可以与更大的数据仓库合并在一起。**通俗解释：先确定数据集市需要哪些数据，再建立数据仓库。**

Kimball将数据仓库定义为“一份针对查询和分析做特别结构化的事物数据拷贝。”Kimball的数据仓库结构就是著名的数据仓库总线。**企业可以通过一系列维数相同的数据集市递增地构建数据仓库，通过使用一致的维度，能够共同看到不同数据集市中的信息，这表示它们拥有公共定义的元素。**

 

 ![image-20200426005216316](数仓项目.assets/image-20200426005216316.png)

 

两种模式各有优势，Inmon模式适合开发进度慢，实施成本高，适合对设计科学性和规范性较高的企业，在业务模式较固定的行业应用较好，比如金融和电信等行业。

Kimball 模式适合快速迭代，实施成本低，能够较快交付任务。这种模式非常适应互联网行业的高速发展，也适合中小型企业。

##### 2、事实表，维度表，实体表

**1、事实表**：事实表其实质就是**通过各种维度和一些指标值的组合来确定一个事实**的，比如通过时间维度，地域组织维度，指标值可以去确定在某时某地的一些指标值怎么样的事实。**事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。**

例如说我昨天在去百货商场买了一双鞋子，花了两百块。这就是一整件事情，可以记做一个事实，如果这件事情继续拆分的话，又可以分为以下几个维度：

时间：昨天

地点：百货商场

金额：200

货币单位：人民币

**2、维度表**：维度表可以看成是**用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述**，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，**维度表只能是事实表的一个分析角度。**

**3、实体表**：实体表就是**一个实际对象的表，实体表它放的数据一定是一条条客观存在的事物数据**，比如说设备，它就是客观存在的，所以可以将其设计一个实体表。

Kimball提出了维度建模方法，这个也是企业中最常见的方法，将表分为**事实表**和**维度表**。**维度模型关注的重点是如果使最终用户访问数据仓库更容易，并有较高的性能**。

在维度建模方法体系中，维度是描述事实的角度，如日期、渠道、服区id等，事实是要度量的指标，如注册人数、充值金额、登录次数等。

比如要查看每天每个渠道下每个服区的注册人数、充值金额、登录次数。那这张表的结构如下：

| 字段名称    | 字段类型 | 描述     | 备注 |
| ----------- | -------- | -------- | ---- |
| event_date  | date     | 日期     | 维度 |
| server_id   | int      | 区服id   | 维度 |
| channel_id  | int      | 渠道     | 维度 |
| reg_users   | int      | 注册人数 | 指标 |
| pay_money   | int      | 充值金额 | 指标 |
| login_times | int      | 登录次数 | 指标 |

```sql
CREATE TABLE table1(
event_date       date    comment '日期',
server_id        int     comment '区服ID',
channel_id       int     comment '渠道ID',
reg_users        int     comment '注册人数',
pay_money        int     comment '充值金额',
login_times      int     comment '登录次数'
)
comment '维度建模表'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

```

其实简单来讲，**建模并不高大上，就是设计表结构**。而这个表里面的数据内容来源，就分为事实表和维度表。

**事实表**就是发生在现实世界中的操作型事件，其所产生的可度量数值，存储在事实表中。从最低的粒度级别来看，事实表行对应一个度量事件，反之亦然。

比如上面表中的注册、充值、登录。当一个用户每次发生一次这样的行为，日志记录一条，这个就是一个事实，而每个用户都有产生行为的日期、所属的渠道和所在的区服。但是有写用户有登录不一定有充值，有注册但是注册和登录不是在同一天，而且这3个日志也是在不同的表里面。因此我们要将这些共同的属性日期、渠道、区服所关联集合产生，就是事实表，因为这些是真实产生在日志里面的。

**维度表**是**对业务过程的上下文描述，主要包含代理键、文本信息和离散的数字。它是进入事实表的入口，丰富的维度属性给出了对事实表的分析切割能力，它一般是行少列多**。如果属性值是离散的，用于过滤和标记的，就放到维度表里，如果是属性值是连续取值，可用于计算的，就放到事实表中。

#### 雪花模型、星型模型和星座模型

在维度建模的基础上又分为三种模型：**星型模型、雪花模型、星座模型**。

######  星型模型

![image-20200426031953090](数仓项目.assets/image-20200426031953090.png)

######  雪花模型

 

![image-20200426032123292](数仓项目.assets/image-20200426032123292.png)

###### 星座模型



![image-20200426032416699](数仓项目.assets/image-20200426032416699.png)

**星座模型与前两种情况的区别是事实表的数量，星座模型是基于多个事实表。**

基本上是很多数据仓库的常态，因为很多数据仓库都是多个事实表的。所以星座不星座只反映是否有多个事实表，它们之间是否共享一些维度表。

所以，星座模型并不和前两个模型冲突。

###### 模型的选择

首先是**星座不星座这个只跟数据和需求有关系，跟设计没关系，不用选择。**

星型还是雪花，却决于性能优先，还是灵活更优先。

目前实际企业开发中，不会绝对选择一种，根据情况灵活组合，甚至并存（一层维度和多层维度都保存）。但是整体来看，更倾向于维度更少的星型模型。尤其是hadoop体系，减少join就是减少shuffle,性能差距很大。（关系型数据可以依靠强大的主键索引）

#### 数据仓库之范式建模

范式建模法主要由Inmon 所提倡，主要运用于传统数仓之中，一般传统的数仓是建立在关系型数据库之上的，不是大数据平台下。它解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前范式建模法大部分采用的是三范式建模法。

三范式具备以下三个条件：

第一范式（1NF）：属性都是原子性的，即数据库表的每一列都是不可分割的原子数据项。比如：

| ID   | 玩家购买道具 | 店铺ID | 玩家ID   |
| ---- | ------------ | ------ | -------- |
| 1    | 10颗宝石     | 10089  | 57655202 |

“玩家购买道具”字段就不是原子性的，可以分割成“10颗”和“宝石”。

![image-20200426005452899](数仓项目.assets/image-20200426005452899.png) 

第二范式（2NF）：在1NF的基础上，实体的属性完全依赖于主关键字，不能存在仅依赖主关键字一部分的属性，也就是不存在局部依赖。比如：

| 玩家ID   | 渠道   | 服区 | 创建角色 | 角色职业 |
| -------- | ------ | ---- | -------- | -------- |
| 57655202 | 应用宝 | 2    | 地狱修罗 | 法师     |
| 57655202 | 应用宝 | 2    | 撒旦使者 | 守卫     |

主键ID为“玩家ID，创建角色”，但是字段“渠道”只依赖于“玩家ID”，不符合2NF。

 ![image-20200426005531276](数仓项目.assets/image-20200426005531276.png)



第三范式（3NF）：在2NF的基础上，任何非主属性不依赖于其它非主属性，也就是不存在传递依赖。比如：

| 订单ID     | 道具ID | 道具经验值 | 商城ID | 玩家ID   |
| ---------- | ------ | ---------- | ------ | -------- |
| order78512 | 8956   | 100        | 6      | 57655202 |

主键为“订单ID”，但是字段“道具经验值”依赖于“道具ID”，不符合3NF。

 ![image-20200426005607470](数仓项目.assets/image-20200426005607470.png)

**虽然范式建模一般没有用在大数据的数仓中，但是我们要采集关系型数据库中的数据，了解这种模型的结构，有利于当我们采集到关系型数据库中的数据到hive里面之后，我们对数据之间的血缘关系的梳理比较有清晰的思路。**

因为我们往往需要大数据平台来采集的关系型数据库里面的数据和表都是海量的。 

#### 数据仓库之实体建模

**实体建模：主要就是确定数据库表之间的关系**

在数据系统中，将数据抽象为“实体”、“属性”、“关系”来表示数据关联和事物描述，这种对数据的抽象建模通常被称为实体关系模型。

实体：通常为参与到过程中的主体，客观存在的，比如游戏道具、坐骑、等级、武器装备，此实体非数据库表的实体表。 

属性：对主体的描述、修饰即为属性，比如游戏道具的属性有道具名称、道具类型、经验值、价值金币的额度、解封的等级等。

关系：现实的物理事件是依附于实体的，比如获得道具并放入背包事件，依附实体商品、货位，就会有“库存”的属性产生；用户购买道具，依附实体用户、道具，就会有“购买数量”、“金额”的属性产品。

**当这些实体建立关系的时候需要根据主键来进行关联，关联的时候就会产生1对1、1对多、多对多的关系。**

**1对1的产生如下：**

| 玩家id   | 注册时间   |      | 玩家id   | 注册区服ID |
| -------- | ---------- | ---- | -------- | ---------- |
| 57655202 | 2019-10-01 |      | 57655202 | 1          |

这两个表我们想要查询一个玩家的注册时间和注册区服时，就要关联主键都玩家id，他们分别在各自的表中都只有一条记录，这样关联的结果就是1对1的。

**1对多的产生如下：**

| 玩家id   | 注册时间   |      | 玩家id   | 登录时间         |
| -------- | ---------- | ---- | -------- | ---------------- |
| 57655202 | 2019-10-01 |      | 57655202 | 2019-10-01 12:00 |
|          |            |      | 57655202 | 2019-10-02 10:00 |
|          |            |      | 57655202 | 2019-10-03 16:00 |

当我们要查在2019-10-01注册的用户的登录轨迹时，我们就要用注册表去关联登录表，而一个玩家的注册都是唯一的，他只能注册一次，而登录可以产生多次，他每天都要登录，因此就会产生1对多的关联。

**多对多的产生如下：**

| 玩家id   | 登录时间         |      | 玩家id   | 升级记录 |
| -------- | ---------------- | ---- | -------- | -------- |
| 57655202 | 2019-10-01 12:00 |      | 57655202 | 50       |
| 57655202 | 2019-10-01 13:00 |      | 57655202 | 60       |
| 57655202 | 2019-10-01 14:00 |      | 57655202 | 70       |

当我们要查询玩家每天登陆之后的升级轨迹，当我们用登录日志表和升级日志表关联的时候，玩家可以在一天之中登录多次，所以会产生多条记录，而他在同一天中升级多次，也会产生多条记录，因此关联就会产生多对多的关系。

**但是要注意：****这个多对多的关系要产生笛卡尔积（两个数据量相乘），数据计算量会增加很多倍，大大的降低计算性能，如果主键分布不均匀还会产生数据倾斜，导致任务一直跑不出来。所以在hive或者spark sql中要尽量的避免这种多对多的情况。**

#### 数据仓库之ETL概念

ETL，Extraction-Transformation-Loading的缩写，中文名称为数据抽取、转换和加载

 ![image-20200426005649884](数仓项目.assets/image-20200426005649884.png)

 

**构建数据仓库的核心是建模，在数据仓库的构建中，ETL贯穿于项目始终，它是整个数据仓库的生命线**。从数据源中抽取数据，然后对这些数据进行转化，最终加载到目标数据库或者数据仓库中去，这也就是我们通常所说的 ETL 过程(Extract,Transform,Load)。

通常数据抽取工作分抽取、清洗、转换、装载几个步骤:

 

 ![image-20200426005656263](数仓项目.assets/image-20200426005656263.png)

**抽取：**主要是针对各个业务系统及不同服务器的分散数据，充分理解数据定义后，规划需要的数据源及数据定义，制定可操作的数据源，制定增量抽取和缓慢渐变的规则。**我们的抽取主要是在前面的Java项目多线程读取游戏日志分表分库的操作。**

**清洗：**主要是针对系统的各个环节可能出现的数据二义性、重复、不完整、违反业务规则等数据质量问题，允许通过数据抽取设定的数据质量规则，将有问题的记录先剔除出来，根据实际情况调整相应的清洗操作。

**转换：**主要是针对数据仓库建立的模型，通过一系列的转换来实现将数据从业务模型到分析模型，通过ETL工具可视化拖拽操作可以直接使用标准的内置代码片段功能、自定义脚本、函数、存储过程以及其他的扩展方式，实现了各种复杂的转换，并且支持自动分析日志，清楚的监控数据转换的状态并优化分析模型。

**装载：**主要是将经过转换的数据装载到数据仓库里面，可以通过直连数据库的方式来进行数据装载，可以充分体现高效性。在应用的时候可以随时调整数据抽取工作的运行方式，可以灵活的集成到其他管理系统中。

目前有很多的ETL工具，比如SEDWA、kettle、OracleGoldengate、informatica、talend等等。这些工具呢主要是用于传统数仓，基于大数据平台的，少部分公司有用kettle的，其他的工具就很少有公司在用。

因为实际上**对于ETL这个流程概念来讲，不同的业务场景有不同的操作流程，有些公司是在数据载入大数据平台之前进行这一步的，有些公司是在数据载入大数据平台之后才有ETL这个操作的，这个就要根据具体的实际场景，看在哪个阶段使用比较合适。**

比如有的公司，他在采集数据的时候得到的数据很多脏数据或者是非结构化的，比如通过接口获取的数据、excel表格数据、json格式的数据，这种情况他要载入hive就要提前把这些数据解析形成和hive ODS层表结构对应的数据才可以。

像游戏行业的日志采集，就已经是结构化的，它们存在数据库中，一条一条的记录，每一条都有相应的字段做对应。这种情况就直接把数据采集进hive ODS层的表中，这样就在数据仓库中进行ETL。

## 2 游戏业务调研

我们采集到数据，也了解了数据仓库模型的设计，那我们就要根据自己实际的业务情况和行业内的业务情况来建设适合我们自己业务的数据仓库。

我们的数据通常是给游戏运营和市场来做数据分析的，运营人员和市场人员把数据作为基础，玩家作为中心，市场作为导向。通过从玩家的行为日志中去发现解决问题，找到最优解方案，来进行最有效的管理。

 ![image-20200426005707333](数仓项目.assets/image-20200426005707333.png)

图1

#### 游戏运营数据

对于游戏而言，从运营的角度理解，他们重点关注用户的营销，用户的经营。他们从三个方面来分析数据。

 ![image-20200426005715680](数仓项目.assets/image-20200426005715680.png)

1)   基础统计

解决用户从哪里来、活跃度、收入等情况，是对于宏观质量和运营情况的描述，这一点也是我们耗费时间和精力最多的部分。

 

图2

 ![image-20200426005743034](数仓项目.assets/image-20200426005743034.png)

2)   行为方式

如何针对目标用户群，根据用户的行为进行分析，扩展及保留用户群，提供服务满足用户需要，刺激收益和提升活跃度。

 ![image-20200426005802071](数仓项目.assets/image-20200426005802071.png)

图3

 

3）用户价值

高价值用户群，是重点运营的目标用户群，将用户作为运营的中心，尽可能的挖掘用户潜在的价值，通过对用户的维系，提升用户规模和收益。

 ![image-20200426005808500](数仓项目.assets/image-20200426005808500.png)

图4

#### 游戏反馈数据

上面是从运营的角度来分析数据，下面是结合游戏反馈确定数据的方式来关注用户对游戏的体验。

 ![image-20200426005815737](数仓项目.assets/image-20200426005815737.png)

  

 

1)   数值

游戏本身是一个通过数值构建的虚拟社会，整体的运算逻辑是基于数值的，因此游戏内容和相关的数据都属于数值反馈数据，比如用户的关卡、等级、注册转化等就是属于此类数值反馈数据。而这类数据的优化和改善从根本上提升游戏的体验，进而降低用户流失率，提升用户量。

 ![image-20200426005822545](数仓项目.assets/image-20200426005822545.png)

图5

 

2）      需求

在构建的虚拟社会中，通过游戏为用户创造很多的需求，典型的就是消费需求，尤其是目前免费游戏盛行的情况下，最大限度激发用户的消费能力和游戏内容的透支能力，因此掌握用户的需求反馈数据会帮助开发着优化游戏，进一步提升游戏的收入。

 ![image-20200426005828396](数仓项目.assets/image-20200426005828396.png)

图6

 

 

 

## 3.   中间层体系设计原则

#### 如何分中间层

结合Inmon和Kimball架构的思想，我们采用三层架构来处理分层，即原始层、中间层、结果层。

**中间层是根据主题域来区分的比较大的集合，所以每一个中间层的表，都是比较大的表，都是一张宽表**。因此，每一张中间层的表都是要集合比较全面的信息，而且这个表还是可以变化扩展的。

有了这个中间层，可以一次性的将原始数据中的脏数据、错误数据、关键缺失信息都补足进来，后面所有的查询都变得简单。因此**中间层的结构比较复杂，表关联很多，而且性能消耗最大**。中间层依赖于原始层，而结果层又依赖于中间层，中间层的资源在执行时要给得充分一点。

 ![image-20200426005835873](数仓项目.assets/image-20200426005835873.png)

#### 中间层的作用

**空间换时间。**通过建设多层次的数据模型供用户使用，避免用户直接使用操作型数据，可以更高效的访问数据。

**把复杂问题简单化。**讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。

**便于处理业务的变化。**随着业务的变化，只需要调整底层的数据，对应用层对业务的调整零感知。

**可以横向扩展。**中间层都是宽表，包含很多信息，但是这些信息在不同时期有着不同的业务需求，所以要可以增加，但是原则上不能删除字段，即便某个统计指标不在参与计算，但是不可删除。

 

 ![image-20200426005842986](数仓项目.assets/image-20200426005842986.png)

#### 中间层的价值

**易维护。**中间层高效的数据组织形式，面向主题的特性决定了数据仓库拥有业务数据库所无法拥有的高效的数据组织形式，更加完整的数据体系，清晰的数据分类和分层机制。因为所有数据在进入数据仓库之前都经过清洗和过滤，使原始数据不再杂乱无章，基于优化查询的组织形式，有效提高数据获取、统计和分析的效率。

**高性能。**中间层时间价值可以让数据仓库的构建将大大缩短获取信息的时间，数据仓库作为数据的集合，所有的信息都可以从数据仓库直接获取，数据仓库的最大优势在于一旦底层从各类数据源到数据仓库的ETL流程构建成型，那么每天就会有来自各方面的信息通过自动任务调度的形式流入数据仓库，从而使一切基于这些底层信息的数据获取的效率达到迅速提升。从应用来看，使用数据仓库可以大大提高数据的查询效率，尤其对于海量数据的关联查询和复杂查询，所以数据仓库有利于实现复杂的统计需求，提高数据统计的效率。

**简单化。**中间层的集成价值在于数据仓库是所有数据的集合，包括日志信息、数据库数据、文本数据、外部数据等都集成在数据仓库中，对于应用来说，实现各种不同数据的关联并使多维分析更加方便，为从多角度多层次地数据分析和决策制定提供的可能。

**历史性。**中间层集合了所有的历史数据。记录历史是数据仓库的特性之一，数据仓库能够还原历史时间点上的产品状态、用户状态、用户行为等，以便于能更好的回溯历史，分析历史，跟踪用户的历史行为，更好地比较历史和总结历史，同时根据历史预测未来。

 

 ![image-20200426005850439](数仓项目.assets/image-20200426005850439.png)

## 4.用户基础属性中间层

#### 整合角色创建与角色登陆

dw_user_basic_info基础信息是来源于ods_game_create 表，这个是游戏创建日志，也是游戏玩家的注册日志。而用户基础属性应该包含每个用户以及他们的每个角色信息，因此要整合 ods_role_create 角色创建信息。

而**角色创建的日志记录可能是不完整的**，有些时候有些玩家有登录日志，但是却没有创建日志，因此我们要整合进来，**保证这个用户基础属性表，是最完整的，真正拥有所有用户的信息。**

###### 1、表来源关系

 ![image-20200426005906856](数仓项目.assets/image-20200426005906856.png)

###### 2、hive当中创建表结构并插入数据

结构如下【中间层结构.xlsx】：

| 字段名           | 字段类型 | 含义                   |
| ---------------- | -------- | ---------------------- |
| plat_id          | int      | 游戏                   |
| server_id        | int      | 服区                   |
| channel_id       | int      | 渠道                   |
| user_id          | string   | user_id                |
| role_id          | string   | 角色ID                 |
| role_name        | string   | 角色名称               |
| job_name         | string   | 角色名称               |
| role_create_date | date     | 角色创建日期yyyy-MM-dd |
| role_create_time | int      | 角色创建时间戳         |

```sql
create table dw_role_login_merge(
plat_id                 int        comment '游戏',
server_id               int        comment '服区',
channel_id              int        comment '渠道',
user_id                 string     comment 'user_id',
role_id                 string     comment '角色ID',
role_name               string     comment '角色名称',
job_name                string     comment '角色名称',
role_create_date        date       comment '角色创建日期yyyy-MM-dd',
role_create_time        int        comment '角色创建时间戳'
) 
comment '整合角色创建与角色登陆'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS orc tblproperties ("orc.compress"="SNAPPY");

```

整合角色创建与角色登陆代码如下：

使用动态分区功能实现将ods层的数据动态添加到dw层

```sql
set hive.exec.dynamic.partition=true; 
set hive.exec.dynamic.partition.mode=nonstrict; 
开启本地模式
set hive.exec.mode.local.auto=true;  
#设置输入数据小于该值，使用本地模式
set hive.exec.mode.local.auto.inputbytes.max=524288;
#设置maptask个数小于5使用本地模式
set hive.exec.mode.local.auto.input.files.max=5;

insert overwrite table dw_role_login_merge partition(part_date) 
select plat_id,server_id,channel_id,user_id,role_id,role_name,job_name,role_create_date,
min(role_create_time) as role_create_time,part_date
from  
(
  select 
  case when t1.channel_id is null then t.channel_id else t1.channel_id end as channel_id,
  case when t1.plat_id is null then t.plat_id else t1.plat_id end as plat_id,
  case when t1.server_id is null then t.server_id else t1.server_id end as server_id,
  case when t1.user_id is null then t.user_id else t1.user_id end as user_id,
  case when t1.role_id is null then t.role_id else t1.role_id end as role_id,
  case when t1.role_name is null then t.role_name else t1.role_name end as role_name,
  case when t1.job_name is null then '' else t1.job_name end as job_name,
  case when from_unixtime(t1.event_time,'yyyy-MM-dd') is null then '2019-01-01' else from_unixtime(t1.event_time,'yyyy-MM-dd') end as role_create_date,
  case when t1.event_time is null then 1546300800 else t1.event_time  end as role_create_time,
  t.part_date
  from
  (
    select channel_id,plat_id,server_id,user_id,role_id,role_name,part_date
    from ods_user_login 
    group by channel_id,plat_id,server_id,user_id,role_id,role_name,part_date
  ) t 
  full outer join ods_role_create t1 on (t1.plat_id=t.plat_id and t1.user_id=t.user_id and t1.role_id=t.role_id and t.part_date = t1.part_date)
) t
group by channel_id,plat_id,server_id,user_id,role_id,role_name,job_name,role_create_date,part_date;

```

这里**以 ods_user_login 为主表，按照每日新增数据，和ods_role_create进行全关联**。这样如果由用户存在于 ods_user_login 中而不存在 ods_role_create 也可以关联出来。

要全关联，是因为在登录中按照每日新增数据取，但是当天登录的玩家不一定是当天创建角色的，因此，无法确定登录中所有的玩家是在哪一天创建角色的，角色创建日志只能选取全量的进行关联。

其中，每个用户在每个 plat_id 下是唯一键，但是一个用户可以创建多个角色，而每个角色在不同的时段都是可以登录的，因此凡是在拥有角色字段的日志信息中，**plat_id + user_id + role_id是唯一键**。

当角色创建日志找不到时，就取登录的信息，如果创建角色的时间也没有，就取 2019-01-01 ，因为我们假设这一天是游戏正式发布的时间。

后面的 where role_create_date in ('2019-10-01','2019-01-01') 这个条件是为了做增加数据到dw_role_login_merge，过滤掉历史创建角色的玩家进入，只选取当天的角色和未产生关联的角色创建信息。

###### 3、使用shell脚本来创建dw层角色登录大表

```sh
cd /home/hadoop/bin/
vim dw_role_login_merge.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive

## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 

echo "**=日志日期为 $part_date**="
sql="
create database if not exists  "$DBNAME";
use "$DBNAME";

set hive.exec.dynamic.partition=true; 
set hive.exec.dynamic.partition.mode=nonstrict; 
set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=524288;
set hive.exec.mode.local.auto.input.files.max=5;


drop table if exists "$DBNAME".dw_role_login_merge;

create table if not exists  "$DBNAME".dw_role_login_merge(
plat_id                 int        comment '游戏',
server_id               int        comment '服区',
channel_id              int        comment '渠道',
user_id                 string     comment 'user_id',
role_id                 string     comment '角色ID',
role_name               string     comment '角色名称',
job_name                string     comment '角色名称',
role_create_date        date       comment '角色创建日期yyyy-MM-dd',
role_create_time        int        comment '角色创建时间戳'
) 
comment '整合角色创建与角色登陆'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS orc tblproperties ('orc.compress'='SNAPPY');

insert overwrite table dw_role_login_merge partition(part_date) 
select plat_id,server_id,channel_id,user_id,role_id,role_name,job_name,role_create_date,
min(role_create_time) as role_create_time,part_date
from  
(
  select 
  case when t1.channel_id is null then t.channel_id else t1.channel_id end as channel_id,
  case when t1.plat_id is null then t.plat_id else t1.plat_id end as plat_id,
  case when t1.server_id is null then t.server_id else t1.server_id end as server_id,
  case when t1.user_id is null then t.user_id else t1.user_id end as user_id,
  case when t1.role_id is null then t.role_id else t1.role_id end as role_id,
  case when t1.role_name is null then t.role_name else t1.role_name end as role_name,
  case when t1.job_name is null then '' else t1.job_name end as job_name,
  case when from_unixtime(t1.event_time,'yyyy-MM-dd') is null then '2019-01-01' else from_unixtime(t1.event_time,'yyyy-MM-dd') end as role_create_date,
  case when t1.event_time is null then 1546300800 else t1.event_time  end as role_create_time,
  t.part_date
  from
  (
    select channel_id,plat_id,server_id,user_id,role_id,role_name,part_date
    from ods_user_login 
    group by channel_id,plat_id,server_id,user_id,role_id,role_name,part_date
  ) t 
  full outer join ods_role_create t1 on (t1.plat_id=t.plat_id and t1.user_id=t.user_id and t1.role_id=t.role_id and t.part_date = t1.part_date)
) t
group by channel_id,plat_id,server_id,user_id,role_id,role_name,job_name,role_create_date,part_date;
"

$hive_home -e "$sql"
```

执行shell脚本 

```
sh dw_role_login_merge.sh
```

#### 整合基础属性表结构

###### 1、表来源关系

 ![image-20200426005952009](数仓项目.assets/image-20200426005952009.png)

基础属性中间层，这一层主要是集合用户的基础属性，表名为 dw_user_basic_info，中间层的表都是以 dw_ 开头，结构如下【中间层结构.xlsx】：

###### 2、创建hive表

| 表名               | 字段名  | 字段类型               | 含义 |
| ------------------ | ------- | ---------------------- | ---- |
| dw_user_basic_info | plat_id | string                 | 平台 |
| server_id          | int     | 服区                   |      |
| channel_id         | string  | 渠道                   |      |
| user_id            | string  | 用户ID                 |      |
| device_brand       | string  | 设备品牌               |      |
| device_type        | string  | 设备型号               |      |
| client_ip          | string  | 注册ip                 |      |
| role_id            | string  | 角色id                 |      |
| role_name          | string  | 角色名称               |      |
| job_name           | string  | 角色职业               |      |
| role_create_date   | date    | 角色创建日期yyyy-MM-dd |      |
| role_create_time   | int     | 角色创建时间戳         |      |
| reg_date           | date    | 注册日期               |      |
| reg_time           | int     | 注册日期               |      |
| operating_system   | string  | 操作系统(小写)         |      |

 建表语句如下：

```sh
create table dw_user_basic_info(
plat_id             int       comment '游戏',
server_id           int       comment '服区',
channel_id          int       comment '渠道',
user_id             string    comment '用户ID',
device_brand        string    comment '设备品牌',
device_type         string    comment '设备型号',
client_ip          string    comment '注册ip',
role_id             string    comment '角色id',
role_name           string    comment '角色名称',
job_name            string    comment '角色职业',
role_create_date    date      comment '角色创建日期yyyy-MM-dd',
role_create_time    int       comment '角色创建时间戳',
reg_date            date      comment '注册日期',
reg_time            int       comment '注册日期',
operating_version   String    comment '操作系统(小写)'
) comment '获取所有注册用户的创建角色信息'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'  
STORED AS orc tblproperties ("orc.compress"="SNAPPY");
```

 获取所有注册用户的创建角色信息，整合所有数据

```sql
insert overwrite table dw_user_basic_info
select plat_id, server_id, channel_id, user_id, device_brand, device_type, client_ip, 
role_id, role_name, job_name, role_create_date, role_create_time, reg_date,reg_time, operating_system
from 
(
  select case when channel_id=0 then 1 
         when plat_id=0 and channel_id=0 then 1
         else channel_id end as channel_id,
  plat_id, server_id, user_id, device_brand, device_type, client_ip,
  role_id, role_name, job_name, role_create_date, role_create_time, reg_date,reg_time, operating_system
  from 
  (
    select t.channel_id,t.plat_id, t.server_id, t.user_id, t.device_brand, t.device_type, t.client_ip,
    t.role_id, t.role_name, t.job_name, t.role_create_date, t.role_create_time, t.reg_date, t.reg_time, t.operating_system
    from 
    (
      select 
      case when t.channel_id is null then t1.channel_id else t.channel_id end as channel_id,
      case when t.plat_id is null then t1.plat_id else t.plat_id end as plat_id,
      case when t1.server_id is null then t.server_id else t1.server_id end as server_id,
      case when t.user_id is null then t1.user_id else t.user_id end as user_id,
      case when t.device_brand is null then '' else t.device_brand end as device_brand,
      case when t.device_type is null then '' else t.device_type end as device_type,
      case when t.client_ip is null then '' else t.client_ip end as client_ip,
      t1.role_id,t1.role_name,t1.job_name,t1.role_create_date,t1.role_create_time,
      case when t.reg_date is null then '2019-01-01' else t.reg_date end as reg_date, 
      case when t.reg_time is null then 1546300800 else t.reg_time end as reg_time,
      case when t.operating_system is null then '' else t.operating_system end as operating_system
      from
      (
        select t.channel_id, server_id,t.plat_id,t.user_id,t.device_brand,t.device_type,t.client_ip,
        from_unixtime(t.event_time,'yyyy-MM-dd') as reg_date,t.event_time as reg_time,t.operating_system
        from 
        (
          select channel_id, plat_id, user_id, server_id, device_brand, device_type, client_ip, event_time,operating_system
          from 
          (
           select channel_id, plat_id, user_id, server_id, device_brand, device_type, client_ip, event_time,operating_system,
           ROW_NUMBER() OVER(PARTITION BY channel_id, plat_id, user_id ORDER BY event_time asc) AS rn 
           from ods_game_create
          ) t where rn=1
        ) t  
      ) t
      full outer join 
      (
        select channel_id,plat_id,server_id,user_id,role_id,role_name,job_name,role_create_date,role_create_time
        from
        (
          select channel_id,plat_id,server_id,user_id,role_id,role_name,job_name,role_create_date,role_create_time,
          ROW_NUMBER() OVER(PARTITION BY plat_id,server_id,user_id,role_id ORDER BY role_create_time asc) AS rn
          from dw_role_login_merge 
        ) t where rn=1
      ) t1 on (t.plat_id=t1.plat_id and t.user_id=t1.user_id)
    ) t 
  ) t 
) t 
group by plat_id, server_id, channel_id, server_id, user_id, device_brand, device_type, client_ip, 
role_id, role_name, job_name, role_create_date, role_create_time, reg_date, reg_time, operating_system;

```

 以 ods_game_create 为主表，全量关联 dw_role_login_merge 角色登录整合表，其中 ods_game_create 以 channel_id + plat_id + user_id 为主，根据日志生成时间取最早的一条记录作为唯一值，这是因为每个用户只属于一款游戏，但是每款游戏在推广的时候会在多个渠道进行推广，比如应用宝、爱奇艺、优酷、哔哩哔哩等，但是玩家只能分属一个渠道，游戏研发在记录的时候可能会因为服务器的问题记录重复，导致一个用户注册来源有多个，因此我们只按照他最早创建游戏那个渠道来获取这个用户的真正来源。

 全量关联 dw_role_login_merge 时，是根据 plat_id + server_id + user_id + role_id为主，根据角色创建时间取最早的一条记录作为唯一值，这里的每个用户会有多个角色，但是每个用户在每个服区上只能有一个角色，他可以在不同的服区上登录，但是他登录的时候选择登录的角色就是唯一的，但是为了避免在日志记录事产生重复，因此也根据时间排序取最早一条记录。

 这里全量关联的目的也是为了整合游戏创建日志和角色创建日志的缺失，让有些有角色创建日志但是没有游戏注册数据整合进来，这样 dw_user_basic_info 这个表中的数据，就是最完整的用户信息数据，包括了丢失的用户数据。

 对其中部分脏数据进行修正，比如 case when channel_id=0 then 1 when plat_id=0 and channel_id=0 then 1 else channel_id end as channel_id 这一步就是修正渠道和平台日志记录错误的情况给予默认值。

###### 3、使用shell脚本来创建dw层基础属性大表整合

node03执行以下命令创建用户基本属性表脚本

```
cd /home/hadoop/bin/

vim dw_user_basic_info.sh
```

```sql
#!/bin/bash

DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive

## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 

echo "**=日志日期为 $part_date**="

sql="
create database if not exists  "$DBNAME";
use "$DBNAME";

set hive.exec.dynamic.partition=true; 
set hive.exec.dynamic.partition.mode=nonstrict; 
set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=524288;
set hive.exec.mode.local.auto.input.files.max=5;

drop table if exists "$DBNAME".dw_user_basic_info;

create table  if not exists  "$DBNAME".dw_user_basic_info(
plat_id             int       comment '游戏',
server_id           int       comment '服区',
channel_id          int       comment '渠道',
user_id             string    comment '用户ID',
device_brand        string    comment '设备品牌',
device_type         string    comment '设备型号',
client_ip          string    comment '注册ip',
role_id             string    comment '角色id',
role_name           string    comment '角色名称',
job_name            string    comment '角色职业',
role_create_date    date      comment '角色创建日期yyyy-MM-dd',
role_create_time    int       comment '角色创建时间戳',
reg_date            date      comment '注册日期',
reg_time            int       comment '注册日期',
operating_version   String    comment '操作系统(小写)'
) comment '获取所有注册用户的创建角色信息'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'  
STORED AS orc tblproperties ('orc.compress'='SNAPPY');

insert overwrite table dw_user_basic_info
select plat_id, server_id, channel_id, user_id, device_brand, device_type, client_ip, 
role_id, role_name, job_name, role_create_date, role_create_time, reg_date,reg_time, operating_system
from 
(
  select case when channel_id=0 then 1 
         when plat_id=0 and channel_id=0 then 1
         else channel_id end as channel_id,
  plat_id, server_id, user_id, device_brand, device_type, client_ip,
  role_id, role_name, job_name, role_create_date, role_create_time, reg_date,reg_time, operating_system
  from 
  (
    select t.channel_id,t.plat_id, t.server_id, t.user_id, t.device_brand, t.device_type, t.client_ip,
    t.role_id, t.role_name, t.job_name, t.role_create_date, t.role_create_time, t.reg_date, t.reg_time, t.operating_system
    from 
    (
      select 
      case when t.channel_id is null then t1.channel_id else t.channel_id end as channel_id,
      case when t.plat_id is null then t1.plat_id else t.plat_id end as plat_id,
      case when t1.server_id is null then t.server_id else t1.server_id end as server_id,
      case when t.user_id is null then t1.user_id else t.user_id end as user_id,
      case when t.device_brand is null then '' else t.device_brand end as device_brand,
      case when t.device_type is null then '' else t.device_type end as device_type,
      case when t.client_ip is null then '' else t.client_ip end as client_ip,
      t1.role_id,t1.role_name,t1.job_name,t1.role_create_date,t1.role_create_time,
      case when t.reg_date is null then '2019-01-01' else t.reg_date end as reg_date, 
      case when t.reg_time is null then 1546300800 else t.reg_time end as reg_time,
      case when t.operating_system is null then '' else t.operating_system end as operating_system
      from
      (
        select t.channel_id, server_id,t.plat_id,t.user_id,t.device_brand,t.device_type,t.client_ip,
        from_unixtime(t.event_time,'yyyy-MM-dd') as reg_date,t.event_time as reg_time,t.operating_system
        from 
        (
          select channel_id, plat_id, user_id, server_id, device_brand, device_type, client_ip, event_time,operating_system
          from 
          (
           select channel_id, plat_id, user_id, server_id, device_brand, device_type, client_ip, event_time,operating_system,
           ROW_NUMBER() OVER(PARTITION BY channel_id, plat_id, user_id ORDER BY event_time asc) AS rn 
           from ods_game_create
          ) t where rn=1
        ) t  
      ) t
      full outer join 
      (
        select channel_id,plat_id,server_id,user_id,role_id,role_name,job_name,role_create_date,role_create_time
        from
        (
          select channel_id,plat_id,server_id,user_id,role_id,role_name,job_name,role_create_date,role_create_time,
          ROW_NUMBER() OVER(PARTITION BY plat_id,server_id,user_id,role_id ORDER BY role_create_time asc) AS rn
          from dw_role_login_merge 
        ) t where rn=1
      ) t1 on (t.plat_id=t1.plat_id and t.user_id=t1.user_id)
    ) t 
  ) t 
) t 
group by plat_id, server_id, channel_id, server_id, user_id, device_brand, device_type, client_ip, 
role_id, role_name, job_name, role_create_date, role_create_time, reg_date, reg_time, operating_system;
"
$hive_home -e "$sql"
```

执行shell脚本

```sh
sh dw_user_basic_info.sh 
```

## 5 用户登录属性整合

#### 整合登录属性表

##### 1、来源表关系

 ![image-20200426010013779](数仓项目.assets/image-20200426010013779.png)

整合登录日志，**整合最全的登陆操作日志**。数据来源于**登录日志、充值日志、战斗力日志、角色升级日志**。这样做的目的跟用户基础属性一样，是因为在其他充值日志、战斗力日志、角色升级日志等采集过程中产生了数据丢失，因此要整合进来。比如有玩家有充值记录，但是没有登录记录，按道理来讲应该是先登录后充值的，其他的信息也是一样的。

##### 2、创建hive表

结构如下【中间层结构.xlsx】：

| 表名                   | 字段名     | 字段类型                                    | 含义               |
| ---------------------- | ---------- | ------------------------------------------- | ------------------ |
| dw_user_behavior_login | login_date | date                                        | 登录日期yyyy-MM-dd |
| plat_id                | string     | 游戏                                        |                    |
| server_id              | int        | 区服                                        |                    |
| user_id                | string     | user_id                                     |                    |
| role_id                | string     | 角色ID                                      |                    |
| role_name              | string     | 角色名称                                    |                    |
| online_times           | int        | 在线时长                                    |                    |
| login_times            | int        | 登陆次数                                    |                    |
| source                 | int        | 整合来源1登录,2角色充值，3战斗力，4升级日志 |                    |

```sql
create table dw_user_behavior_login (
login_date      date    comment  '登录日期yyyy-MM-dd',
plat_id         string  comment  '游戏',
server_id       int     comment  '区服',
user_id         string  comment  'user_id',
role_id         string  comment  '角色ID',
role_name       string  comment  '角色名称',
online_times    int     comment  '在线时长',
login_times     int     comment  '登陆次数' ,
source          int     comment  '整合来源1登录,2角色充值，3战斗力，4升级日志'
)
comment '整合登录日志，整合最全的登陆操作日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'  
STORED AS orc tblproperties ("orc.compress"="SNAPPY"); 


向dw_user_behavior_login表当中插入聚合数据
insert overwrite table dw_user_behavior_login partition(part_date) 
select part_date as login_date, plat_id, server_id, user_id, role_id, role_name, online_times, login_times, source,part_date
from (
select plat_id, server_id, user_id, role_id, role_name, online_times, source, login_times,
ROW_NUMBER() OVER(PARTITION BY plat_id, server_id, user_id, role_id, role_name ORDER BY source asc) AS rn ,part_date
from 
(
  select plat_id, server_id, user_id, role_id, role_name, 
  sum(case when online_time>86400 then 0 else online_time end) as online_times, 1 as source, sum(case when op_type=1 then 1 else 0 end) as login_times,part_date
  from 
  (
    select event_time, plat_id, server_id, user_id, role_id, role_name, online_time,op_type,part_date
    from ods_user_login  
    group by event_time, plat_id, server_id, user_id, role_id, role_name, online_time,op_type,part_date
  ) t 
  group by plat_id, server_id, user_id, role_id, role_name,part_date
  union all 
  select plat_id, server_id, user_id, role_id, role_name, 86400 as online_times, 2 as source, 1 as login_times,part_date
  from ods_role_recharge 
  union all 
  select plat_id, server_id,  user_id, role_id, role_name, 86400 as online_times, 3 as source, 1 as login_times,part_date
  from ods_fight_power 
  union all 
  select plat_id, server_id, user_id, role_id, role_name, 86400 as online_times, 4 as source, 1 as login_times,part_date
  from ods_role_level_up 
) t 
) t where rn=1;
```

 整合登录日志、充值日志、战斗力日志、升级日志。并且要注明数据整合进来的来源，这样是为了好追溯源头，便于后期排查问题。

在线时长 online_times如果在登录数据中没有，取其他日志的数据就给默认值 86400秒，如果登录中有，但是由于记录错误，导致超过这么多的，就认为无效。

 根据 ROW_NUMBER开窗函数取最值是因为 union 多个日志数据后，为避免重复提取，因此设置来源 source 的优先级，优先取登录日志，然后是充值、战斗力、升级日志等。

##### 3、使用shell脚本来创建DW层用户行为属性表

node03执行以下命令创建用户基本属性表脚本

```
cd /home/hadoop/bin/

vim dw_user_behavior_login.sh
```

 

```sql
#!/bin/bash
DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive

## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="
sql="
create database if not exists  "$DBNAME";
use "$DBNAME";

set hive.exec.dynamic.partition=true; 
set hive.exec.dynamic.partition.mode=nonstrict; 
set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=524288;
set hive.exec.mode.local.auto.input.files.max=5;


drop table if exists "$DBNAME".dw_user_behavior_login;

create table if not exists  "$DBNAME".dw_user_behavior_login (
login_date      date    comment  '登录日期yyyy-MM-dd',
plat_id         string  comment  '游戏',
server_id       int     comment  '区服',
user_id         string  comment  'user_id',
role_id         string  comment  '角色ID',
role_name       string  comment  '角色名称',
online_times    int     comment  '在线时长',
login_times     int     comment  '登陆次数' ,
source          int     comment  '整合来源1登录,2角色充值，3战斗力，4升级日志'
)
comment '整合登录日志，整合最全的登陆操作日志'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'  
STORED AS orc tblproperties ('orc.compress'='SNAPPY'); 

insert overwrite table dw_user_behavior_login partition(part_date) 
select part_date as login_date, plat_id, server_id, user_id, role_id, role_name, online_times, login_times, source,part_date
from (
select plat_id, server_id, user_id, role_id, role_name, online_times, source, login_times,
ROW_NUMBER() OVER(PARTITION BY plat_id, server_id, user_id, role_id, role_name ORDER BY source asc) AS rn ,part_date
from 
(
  select plat_id, server_id, user_id, role_id, role_name, 
  sum(case when online_time>86400 then 0 else online_time end) as online_times, 1 as source, sum(case when op_type=1 then 1 else 0 end) as login_times,part_date
  from 
  (
    select event_time, plat_id, server_id, user_id, role_id, role_name, online_time,op_type,part_date
    from ods_user_login  
    group by event_time, plat_id, server_id, user_id, role_id, role_name, online_time,op_type,part_date
  ) t 
  group by plat_id, server_id, user_id, role_id, role_name,part_date
  union all 
  select plat_id, server_id, user_id, role_id, role_name, 86400 as online_times, 2 as source, 1 as login_times,part_date
  from ods_role_recharge 
  union all 
  select plat_id, server_id,  user_id, role_id, role_name, 86400 as online_times, 3 as source, 1 as login_times,part_date
  from ods_fight_power 
  union all 
  select plat_id, server_id, user_id, role_id, role_name, 86400 as online_times, 4 as source, 1 as login_times,part_date
  from ods_role_level_up 
) t 
) t where rn=1;

"
$hive_home -e "$sql"
```



## 6 超大表历史快照整合层

### 6.1 战斗力大表整合

#### 1、来源表关系

 ![image-20200426010030011](数仓项目.assets/image-20200426010030011.png)

角色战斗力日志是非常大的一张表，因为每个玩家在玩游戏的过程中，战斗力是不断的在变化的，比如杀一个怪兽、小兵、或者被敌人砍一下都会减少战斗力，而且这个过程中玩家又不断的捡到道具来补充战斗力，战斗力每发生一次变化，就会记录一条日志，因此这个表就非常大。一般的游戏一天就会产生上百G的数据。

所以我们如果要查询这个表，就会非常消耗性能。如果再把这个表整合到用户行为属性中间层的话，关联起来，会形成多对多的情况而产生笛卡尔积。因此，我们的处理方式就是每天单表与计算聚合一次，减少数据量，聚合到每个游戏每个用户每个角色一天只有一条记录，这样在和用户行为属性整合时就减少关联数，达到1对1的关联效果，使中间层的聚合任务速度最快，效率最高。

#### 2、创建hive表

 

| 表名           | 字段名  | 字段类型           | 含义 |
| -------------- | ------- | ------------------ | ---- |
| dw_fight_power | plat_id | string             | 游戏 |
| server_id      | int     | 服区               |      |
| user_id        | string  | 用户ID             |      |
| role_id        | string  | 角色id             |      |
| role_name      | string  | 角色名称           |      |
| combat_power   | int     | 玩家变化后的战斗力 |      |

```sql
use game_center;
create table dw_fight_power(
plat_id       int comment '游戏',
server_id     int    comment '服区',
user_id       string comment '用户ID',
role_id       string comment '角色id',
role_name     string comment '角色名称',
combat_power  int    comment '玩家变化后的战斗力'
) 
comment '获取所有用户的最大战斗力信息'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
 STORED AS orc tblproperties ("orc.compress"="SNAPPY");
```

插入表数据

```sql
insert overwrite table dw_fight_power partition(part_date) 
select plat_id, server_id, user_id, role_id, role_name, max(combat_power) as combat_power,part_date
from ods_fight_power 
group by plat_id, server_id, user_id, role_id, role_name,part_date;
```

这里聚合战斗力变化日志，每天执行一次任务聚合，聚合每个游戏每个用户每个角色当天最大的战斗力。当然这个表也是可以扩展的，比如后期还想同时查到每个玩家每天战斗力变化的次数，最小的战斗力，平均每次战斗力变化的值等等。

#### 3、使用shell脚本来创建dw层战斗力大表整合

node03执行以下命令创建用户基本属性表脚本

```
cd /home/hadoop/bin/

vim dw_fight_power.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="
sql="

set hive.exec.dynamic.partition=true; 
set hive.exec.dynamic.partition.mode=nonstrict; 
set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=524288;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists  "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".dw_fight_power;
create table if not exists dw_fight_power(
plat_id       int comment '游戏',
server_id     int    comment '服区',
user_id       string comment '用户ID',
role_id       string comment '角色id',
role_name     string comment '角色名称',
combat_power  int    comment '玩家变化后的战斗力'
) 
comment '获取所有用户的最大战斗力信息'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
 STORED AS orc tblproperties ('orc.compress'='SNAPPY');

insert overwrite table dw_fight_power partition(part_date) 
select plat_id, server_id, user_id, role_id, role_name, max(combat_power) as combat_power,part_date
from ods_fight_power 
group by plat_id, server_id, user_id, role_id, role_name,part_date;
"
$hive_home -e "$sql"
```

 执行shell脚本

```
sh dw_fight_power.sh  
```

### 6.2 升级日志大表整合

#### 1、来源表关系

 ![image-20200426010041088](数仓项目.assets/image-20200426010041088.png)

整合所有注册用户的角色升级后最大level信息，角色升级日志的信息量也是比较大的，因为有些角色升级得非常频繁，但是没有战斗力变化得那么快，但是一天也能升几十G，一个用户多个角色，一天也能产生上百条记录。如果和行为中间层整合又会产生多对多的关联关系，因此我们也要像战斗力一样，每天做一次聚合操作。

#### 2、创建hive表

|       表名       | 字段名  |   字段类型   | 含义 |
| :--------------: | :-----: | :----------: | :--: |
|   dw_level_up    | plat_id |    string    | 游戏 |
|    server_id     |   int   |     服区     |      |
|     user_id      | string  |    用户ID    |      |
|     role_id      | string  |    角色id    |      |
|    role_name     | string  |   角色名称   |      |
| role_level_after |   int   | 升级后的等级 |      |

```sql
create table dw_level_up(
plat_id           string  comment '游戏',
server_id         int     comment '服区',
user_id           string  comment '用户ID',
role_id           string  comment '角色id',
role_name         string  comment '角色名称',
role_level_after  int     comment '升级后的等级'
) 
comment '获取所有注册用户的角色升级后最大level信息'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
 STORED AS orc tblproperties ("orc.compress"="SNAPPY");
```

 插入表数据

```sql
insert overwrite table dw_level_up partition(part_date) 
select plat_id, server_id, user_id, role_id, role_name, max(level_after) as role_level_after,part_date
from ods_role_level_up 
group by plat_id, server_id, user_id, role_id, role_name,part_date;
```

 这里聚合角色升级日志，每天执行一次任务聚合，聚合每个游戏每个用户每个角色当天最大的等级。这个表也可以扩展到每个玩家每天级别变化的次数，最小的级别，每次升级的间隔时间等等。

#### 3、使用shell脚本来创建dw层升级日志大表整合

node03执行以下命令创建用户基本属性表脚本

```
cd /home/hadoop/bin/

vim dw_level_up.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 

echo "**=日志日期为 $part_date**="

sql="
create database if not exists  "$DBNAME";
use "$DBNAME";

set hive.exec.dynamic.partition=true; 
set hive.exec.dynamic.partition.mode=nonstrict; 
set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=524288;
set hive.exec.mode.local.auto.input.files.max=5;


drop table if exists "$DBNAME".dw_level_up;

create table if not exists  "$DBNAME".dw_level_up(
plat_id           string  comment '游戏',
server_id         int     comment '服区',
user_id           string  comment '用户ID',
role_id           string  comment '角色id',
role_name         string  comment '角色名称',
role_level_after  int     comment '升级后的等级'
) 
comment '获取所有注册用户的角色升级后最大level信息'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
 STORED AS orc tblproperties ('orc.compress'='SNAPPY');

insert overwrite table dw_level_up partition(part_date) 
select plat_id, server_id, user_id, role_id, role_name, max(level_after) as role_level_after,part_date
from ods_role_level_up 
group by plat_id, server_id, user_id, role_id, role_name,part_date;
"

$hive_home -e "$sql"
```

执行shell脚本

```
sh dw_level_up.sh 
```

### 6.3    用户行为属性大表整合

#### 1、来源关系表

 

 

 ![image-20200426010052043](数仓项目.assets/image-20200426010052043.png)

 

用户行为属性中间层，这一层也是一个宽表，这一层主要就是整合了用户每日的行为，以日期作为了一个重要的维度。

#### 2、创建hive表

结构如下【中间层结构.xlsx】：

| 表名                  | 字段名 | 字段类型               | 含义                                        |
| --------------------- | ------ | ---------------------- | ------------------------------------------- |
| dw_user_behavior_info | source | int                    | 整合来源1登录,2角色充值，3战斗力，4升级日志 |
| plat_id               | int    | 游戏                   |                                             |
| channel_id            | int    | 渠道                   |                                             |
| server_id             | int    | 区服                   |                                             |
| login_date            | date   | 登录日期yyyy-MM-dd     |                                             |
| user_id               | string | user_id                |                                             |
| role_id               | string | 角色ID                 |                                             |
| role_name             | string | 角色名称               |                                             |
| online_times          | int    | 在线时长               |                                             |
| role_create_date      | date   | 角色创建日期yyyy-MM-dd |                                             |
| role_create_time      | int    | 角色创建时间戳         |                                             |
| job_name              | string | 角色职业               |                                             |
| reg_date              | date   | 注册日期               |                                             |
| reg_time              | int    | 注册时间戳             |                                             |
| client_ip             | string | 注册ip                 |                                             |
| device_brand          | string | 设备品牌               |                                             |
| device_type           | string | 设备型号               |                                             |
| recharge_amount       | double | 充值金额               |                                             |
| acer_count            | int    | 充值后获得的元宝数量   |                                             |
| combat_power          | int    | 玩家变化后的战斗力     |                                             |
| role_level_after      | int    | 角色升级后等级         |                                             |
| pay_vip_level         | int    | 充值vip_level          |                                             |
| acer_stock            | int    | 元宝存量               |                                             |
| pay_times             | int    | 充值次数               |                                             |
| login_times           | int    | 登录次数               |                                             |
| operating_system      | string | 操作系统               |                                             |

```sql
create table dw_user_behavior_info (
source                  int             comment      '整合来源1登录,2角色充值，3战斗力，4升级日志',
plat_id                 int             comment      '游戏',
channel_id              int             comment      '渠道',
server_id               int             comment      '区服',
login_date              date            comment      '登录日期yyyy-MM-dd',
user_id                 string          comment      'user_id',
role_id                 string          comment      '角色ID',
role_name               string          comment      '角色名称',
online_times            int             comment      '在线时长',
role_create_date        date            comment      '角色创建日期yyyy-MM-dd',
role_create_time        int             comment      '角色创建时间戳',
job_name                string          comment      '角色职业',
reg_date                date            comment      '注册日期',
reg_time                int             comment      '注册时间戳',
client_ip               string          comment '注册ip',
device_brand            string          comment      '设备品牌',
device_type             string          comment      '设备型号',
recharge_amount         double          comment      '充值金额',
acer_count              int             comment      '充值后获得的元宝数量',
combat_power            int             comment      '玩家变化后的战斗力',
role_level_after        int             comment      '角色升级后等级',
pay_vip_level           int             comment      '充值vip_level',
acer_stock              int             comment      '元宝存量', 
pay_times               int             comment      '充值次数' ,
login_times            int             comment      '登录次数' ,
operating_system      string             comment      '操作系统' 
)
comment '用户行为整合表'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'  
STORED AS orc tblproperties ("orc.compress"="SNAPPY"); 
```

整合了用户登录最全信息表，整合了战斗力和角色升级大表的预聚合，现在就可以把这些数据整合进用户行为属性中间层。

```sql
insert overwrite table dw_user_behavior_info partition(part_date)
select a.source,
a.plat_id,b.channel_id,a.server_id,a.login_date,a.user_id,a.role_id,b.role_name,a.online_times,
b.role_create_date,b.role_create_time,b.job_name,b.reg_date,b.reg_time,b.client_ip,b.device_brand,b.device_type,
e.recharge_amount,e.acer_count,f.combat_power,g.role_level_after,
0 as pay_vip_level, e.acer_stock, pay_times, a.login_times, 0 as operating_system,a.part_date
from 
(
  select login_date, plat_id, server_id, user_id, role_id, sum(online_times) as online_times, source, sum(login_times) as login_times,part_date
  from dw_user_behavior_login 
  group by login_date, plat_id, server_id, user_id, role_id,source,part_date
) a
left outer join dw_user_basic_info b on (b.plat_id=a.plat_id and b.user_id=a.user_id and b.role_id=a.role_id)
left outer join 
(
  select plat_id, user_id, role_id, sum(recharge_amount) as recharge_amount, sum(acer_count) as acer_count,
  max(acer_count) as acer_stock, count(1) as pay_times,part_date
  from 
  (
    select event_time, plat_id, user_id, role_id, role_name, recharge_amount, acer_count,part_date
    from ods_role_recharge 
    group by event_time, plat_id, user_id, role_id, role_name, recharge_amount, acer_count,part_date
  ) e group by plat_id, user_id, role_id,part_date
) e on (e.plat_id=a.plat_id and e.user_id=a.user_id and e.role_id=a.role_id and e.part_date = a.part_date)
left outer join 
(
  select plat_id, user_id, role_id, max(combat_power) as combat_power,part_date
  from dw_fight_power
  group by plat_id, user_id, role_id,part_date
) f on (f.plat_id=a.plat_id and f.user_id=a.user_id and f.role_id=a.role_id and f.part_date = a.part_date)
left outer join 
(
  select plat_id, user_id, role_id, max(role_level_after) as role_level_after,part_date
  from dw_level_up 
  group by plat_id, user_id, role_id,part_date
) g on (g.plat_id=a.plat_id and g.user_id=a.user_id and g.role_id=a.role_id and g.part_date = a.part_date)
```

这里以 dw_user_behavior_login 登录整合表为主表，因为这里主要是整合用户行为，而用户的一切行为的前提就是他一定要先登录才会有后续的闯关、刷副本、刷怪、刷BOSS等一系列操作。

在关联 dw_user_basic_info 表时采用 dw_user_basic_info 的全表信息，因为要获取用户基础属性，但是当他登录的用户不一定是当天注册的，所以基础属性表不能由时间来限定。

充值数据 ods_role_recharge 是可以限定当天日期，因为用户一定是当天登录之后才能充值，不登录就不能充值。

战斗力表 dw_fight_power 和角色升级表 dw_level_up 不能选择限定日期，因为有可能某个用户前一天在玩游戏，有战斗力变化或者有升级变化，但是他在后面的日子里面就只登录游戏，没有玩，没有刷怪发生战斗，因此他的战斗力和级别就应该是之前他有的那个时间的状态，所以他这一天的战斗力和升级就应该是之前最大的值。

至此，中间层完成了建设，但是后续随着需求和业务量的增加，这些中间层的表都会扩展的，而且也可以在增加其他的表进来，数仓不只是一个项目，而是一个要随着业务需求变化而变化的长期工程。

#### 3、使用shell脚本来创建dw层用户行为属性大表

```
cd /home/hadoop/bin/

vim dw_user_behavior_info.sh
```

 

```sql
#!/bin/bash

DBNAME=game_center

hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive


## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 

echo "**=日志日期为 $part_date**="

sql="
create database if not exists  "$DBNAME";
use "$DBNAME";

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.mode.local.auto=true;
set hive.exec.mode.local.auto.inputbytes.max=524288;
set hive.exec.mode.local.auto.input.files.max=5;


drop table if exists "$DBNAME".dw_user_behavior_info;

create table if not exists  "$DBNAME".dw_user_behavior_info (
source                  int             comment      '整合来源1登录,2角色充值，3战斗力，4升级日志',
plat_id                 int             comment      '游戏',
channel_id              int             comment      '渠道',
server_id               int             comment      '区服',
login_date              date            comment      '登录日期yyyy-MM-dd',
user_id                 string          comment      'user_id',
role_id                 string          comment      '角色ID',
role_name               string          comment      '角色名称',
online_times            int             comment      '在线时长',
role_create_date        date            comment      '角色创建日期yyyy-MM-dd',
role_create_time        int             comment      '角色创建时间戳',
job_name                string          comment      '角色职业',
reg_date                date            comment      '注册日期',
reg_time                int             comment      '注册时间戳',
client_ip               string          comment '注册ip',
device_brand            string          comment      '设备品牌',
device_type             string          comment      '设备型号',
recharge_amount         double          comment      '充值金额',
acer_count              int             comment      '充值后获得的元宝数量',
combat_power            int             comment      '玩家变化后的战斗力',
role_level_after        int             comment      '角色升级后等级',
pay_vip_level           int             comment      '充值vip_level',
acer_stock              int             comment      '元宝存量', 
pay_times               int             comment      '充值次数' ,
login_times            int             comment      '登录次数' ,
operating_system      string             comment      '操作系统' 
)
comment '用户行为整合表'
PARTITIONED BY(part_date date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'  
STORED AS orc tblproperties ('orc.compress'='SNAPPY'); 

insert overwrite table dw_user_behavior_info partition(part_date)
select a.source,
a.plat_id,b.channel_id,a.server_id,a.login_date,a.user_id,a.role_id,b.role_name,a.online_times,
b.role_create_date,b.role_create_time,b.job_name,b.reg_date,b.reg_time,b.client_ip,b.device_brand,b.device_type,
e.recharge_amount,e.acer_count,f.combat_power,g.role_level_after,
0 as pay_vip_level, e.acer_stock, pay_times, a.login_times, 0 as operating_system,a.part_date
from 
(
  select login_date, plat_id, server_id, user_id, role_id, sum(online_times) as online_times, source, sum(login_times) as login_times,part_date
  from dw_user_behavior_login 
  group by login_date, plat_id, server_id, user_id, role_id,source,part_date
) a
left outer join dw_user_basic_info b on (b.plat_id=a.plat_id and b.user_id=a.user_id and b.role_id=a.role_id)
left outer join 
(
  select plat_id, user_id, role_id, sum(recharge_amount) as recharge_amount, sum(acer_count) as acer_count,
  max(acer_count) as acer_stock, count(1) as pay_times,part_date
  from 
  (
    select event_time, plat_id, user_id, role_id, role_name, recharge_amount, acer_count,part_date
    from ods_role_recharge 
    group by event_time, plat_id, user_id, role_id, role_name, recharge_amount, acer_count,part_date
  ) e group by plat_id, user_id, role_id,part_date
) e on (e.plat_id=a.plat_id and e.user_id=a.user_id and e.role_id=a.role_id and e.part_date = a.part_date)
left outer join 
(
  select plat_id, user_id, role_id, max(combat_power) as combat_power,part_date
  from dw_fight_power
  group by plat_id, user_id, role_id,part_date
) f on (f.plat_id=a.plat_id and f.user_id=a.user_id and f.role_id=a.role_id and f.part_date = a.part_date)
left outer join 
(
  select plat_id, user_id, role_id, max(role_level_after) as role_level_after,part_date
  from dw_level_up 
  group by plat_id, user_id, role_id,part_date
) g on (g.plat_id=a.plat_id and g.user_id=a.user_id and g.role_id=a.role_id and g.part_date = a.part_date)
;

"

$hive_home -e "$sql"
```

执行shell脚本

```
sh dw_user_behavior_info.sh 2019-10-01
```

## 7 使用shell脚本一键执行dw层所有脚本

node03执行以下命令创建dw层一键执行shell脚本

```
cd /home/hadoop/bin

vim execute_dw.sh
```

```sql
#!/bin/bash

for m in dw_role_login_merge.sh dw_user_basic_info.sh dw_user_behavior_login.sh dw_fight_power.sh dw_level_up.sh dw_user_behavior_info.sh
do
 echo $m
  /usr/bin/sh $m 
done
```

 执行脚本

```
sh execute_dw.sh
```

## :rainbow:游戏行业指标体系以及res层

## 1、用户画像属性指标

### 1.1 什么是用户画像

用户画像这个概念在互联网中已经很普遍的在应用了，每个行业都会有自己的用户画像，电商用户画像、金融用户画像、社交用户画像、游戏用户画像等等。

​       ![image-20200520163606933](数仓项目.assets/image-20200520163606933.png)                        

用户画像是根据用户社会属性、生活习惯和消费行为等信息而抽象出的一个标签化的用户模型。构建用户画像的核心工作即是给用户贴“标签”，而标签是通过对用户信息分析而来的高度精炼的特征标识。

标签可以分为社会属性，生活习惯以及消费行为3个方面，也有分为人口属性、社会属性、行为习惯、兴趣偏好和心理属性5个方面的。我们先概括的说下较为通用的3个方面。

**社会属性**：年龄，性别，地域，学历，职业，婚姻状况，住房车辆等。

**生活习惯：**运动，休闲，旅游，饮食起居， 购物，游戏，体育，文化等。

**消费行为：**消费金额、消费次数、消费时间、消费频次等（基于产品）。

标签的包含的内容不是完全固定的，需要根据行业和产品的属性有所区分，比如社交类的产品会更关注用户社交关系标签；电商类会更关注用户的兴趣和消费能力等；金融行业还会有风险画像，包含征信、违约、还款能力等。游戏行业就会关注玩家的行为轨迹，角色偏好，玩的地图类型，在线时间段等。

 ![image-20200520163622841](数仓项目.assets/image-20200520163622841.png)

 

### 1.2 怎么来构建用户画像

目前行业内的用户画像已经是比较普遍的构建对象了，有很多方法，比如“**七步人物角色法**”、“**十步人物角色法**”等等。

但是通常都是分为三个步骤来实施的。**数据收集、给用户打标签、构建画像。**

 ![image-20200520163703746](数仓项目.assets/image-20200520163703746.png)

**第一步数据收集**

数据收集大致分为网络行为、服务行为、内容偏好、交易行为这四类。

网络行为：登录情况、页面的浏览访问、在页面的停留时长、设备或账号的激活、外部第三方地址的触发和转发、社交行为等。

服务行为：网站内的浏览路径、网站内的停留时间、访问深度、单个页面的浏览次数等。

内容偏好：收藏的内容、评论很好的内容、喜欢互动内容、生活状态、喜欢的品牌等。

交易行为：购买商品、浏览的商品、购买的金额和次数等。

**第二步是给用户打标签**

其实用户画像的构建本质上就是给用户打上各种各样的标签，就像现实中的上班族、宝妈族、月光族、土豪、屌丝等等这些。

这些标签都是带有特定含义的，用于描述真实的用户自身带有的属性特征，方便企业做数据的统计分析。分析出用价值的用户，从而为公司的策略制定方向。

常见的标签分成两大类别：**静止标签、变化标签**。

静止标签：

人口属性标签是用户最基础的信息要素，通常自成标签，不需要过多建模，它构成用户画像的基本框架。包括人的自然属性和社会属性特征：姓名、性别、年龄、身高、体重、职业、地域、受教育程度、婚姻、星座、血型......。

变化标签：

常见的行为包括：搜索、浏览、注册、评论、点赞、收藏、打分、加入购物车、购买、使用优惠券......在不同的时间，不同的场景，这些行为不断发生着变化，它们都属于动态的信息。

还有些公司可以通过爬虫爬到用户的其他，社交网络行为，来自于微博、微信、论坛、社群、贴吧上面的，包括基本的访问行为(搜索、注册、登陆等)、社交行为(邀请/添加/取关好友、加入群、新建群等)、信息发布行为(添加、发布、删除、留言、分享、收藏等)。

课堂笔记：

```
标签怎么存储?

标签需要另外生成表来存储么，标签和用户在数据库里怎么关联 
通过主键来管理，一般都可以使用nosql的数据库，列是可以动态添加的来存储标签库
可以存储到关系型数据库mysql或者oracle等
设计成为单独的一张表来存储标签，设计成为列式存储表
```

**第三步是绘制用户画像**

绘制用户画像要进行数据建模，就是**给标签加上权重**。现在比较通用的是用4w表示来表示用户的行为： **WHO(谁);WHEN(什么时候);WHERE(在哪里);WHAT(做了什么)。**

WHO(谁)：定义用户，明确我们的研究对象。主要是用于做用户分类，划分用户群体。网络上的用户识别，包括但不仅限于用户注册的ID、昵称、手机号、邮箱、身份证、微信微博号等等。

WHEN(时间)：时间跨度，这里的时间包含了时间跨度和时间长度两个方面。时间跨度是以天为单位计算的时长，指某行为发生到现在间隔了多长时间;“时间长度”则为了标识用户在某一页面的停留时间长短。越早发生的行为标签权重越小，越近期权重越大，这就是所谓的“时间衰减因子”。

WHERE(在哪里)：定义来源，就是指用户发生行为的接触点，里面包含有内容+网址。内容是指用户作用于的对象标签，比如小米手机;网址则指用户行为发生的具体地点，比如小米官方网站。权重是加在网址标签上的，比如买小米手机，在小米官网买权重计为1,，在京东买计为0.8，在淘宝买计为0.7。

WHAT(做了什么)：行为结果，就是指的用户发生了怎样的行为，根据行为的深入程度添加权重。比如，用户购买了权重计为1，用户收藏了计为0.85，用户仅仅是浏览了计为0.7。

当上面的单个标签权重确定下来后，就可以利用标签权重公式计算总的用户标签权重：**标签权重=时间衰减因子×行为权重×网址权重。**

### 1.3 游戏行业用户画像

构建用户画像的第一步就是采集数据，这个我们共同建设数据仓库ods层和中间层，已经将用户数据处理好了。

我们的用户数据中间层整理好了两类，**用户基础属性**和**用户行为属性**。其中基础属性属于**静态数据**，行为属性属于**动态数据。**

因此我们的用户画像，其实就已经是做好了的，即 dw_user_basic_info 和 dw_user_behavior_info 两个中间层的表。这两个表的内容，就是我们能获取到的数据的全部信息，并且还可以扩展相应的计算逻辑进去。

dw_user_basic_info：

| dw_user_basic_info | plat_id | string                 | 平台 |
| ------------------ | ------- | ---------------------- | ---- |
| server_id          | int     | 服区                   |      |
| channel_id         | string  | 渠道                   |      |
| user_id            | string  | 用户ID                 |      |
| device_brand       | string  | 设备品牌               |      |
| device_type        | string  | 设备型号               |      |
| client_ip          | string  | 注册ip                 |      |
| role_id            | string  | 角色id                 |      |
| role_name          | string  | 角色名称               |      |
| job_name           | string  | 角色职业               |      |
| role_create_date   | date    | 角色创建日期yyyy-MM-dd |      |
| role_create_time   | int     | 角色创建时间戳         |      |
| reg_date           | date    | 注册日期               |      |
| reg_time           | int     | 注册日期               |      |
| operating_system   | string  | 操作系统(小写)         |      |

dw_user_behavior_info：

| 表名                  | 字段名 | 字段类型               | 含义                                        |
| --------------------- | ------ | ---------------------- | ------------------------------------------- |
| dw_user_behavior_info | source | int                    | 整合来源1登录,2角色充值，3战斗力，4升级日志 |
| plat_id               | int    | 游戏                   |                                             |
| channel_id            | int    | 渠道                   |                                             |
| server_id             | int    | 区服                   |                                             |
| login_date            | date   | 登录日期yyyy-MM-dd     |                                             |
| user_id               | string | user_id                |                                             |
| role_id               | string | 角色ID                 |                                             |
| role_name             | string | 角色名称               |                                             |
| online_times          | int    | 在线时长               |                                             |
| role_create_date      | date   | 角色创建日期yyyy-MM-dd |                                             |
| role_create_time      | int    | 角色创建时间戳         |                                             |
| job_name              | string | 角色职业               |                                             |
| reg_date              | date   | 注册日期               |                                             |
| reg_time              | int    | 注册时间戳             |                                             |
| client_ip             | string | 注册ip                 |                                             |
| device_brand          | string | 设备品牌               |                                             |
| device_type           | string | 设备型号               |                                             |
| recharge_amount       | double | 充值金额               |                                             |
| acer_count            | int    | 充值后获得的元宝数量   |                                             |
| combat_power          | int    | 玩家变化后的战斗力     |                                             |
| role_level_after      | int    | 角色升级后等级         |                                             |
| pay_vip_level         | int    | 充值vip_level          |                                             |
| acer_stock            | int    | 元宝存量               |                                             |
| pay_times             | int    | 充值次数               |                                             |
| login_times           | int    | 登录次数               |                                             |
| operating_system      | string | 操作系统               |                                             |

但是这连个表都是比较大的，是全部的用户信息，dw_user_basic_info里面有很多个游戏的用户数据，数十亿的记录，dw_user_behavior_info 虽然是安装每天分区来增量更新，但是总量也是上百亿了，这样大的数据量在关系型数据中已经无法可视化查询了，因此我们**要把这些数据分多维度进行统计之后，在输出到关系型数据库中。**

### 1.4、统计每日每个区服下的数据情况

**按日期和服区为维度统计**，统计每天每个区服下的**注册人数、登录人数、充值金额、登录次数：**

 ![image-20200520164434735](数仓项目.assets/image-20200520164434735.png)

 ![image-20200520164504586](数仓项目.assets/image-20200520164504586.png)

#### sql语句开发

```sql
select t.reg_date,t.server_id, reg_users, login_users, recharge_amount, login_times
from 
(
  select reg_date,server_id,count(distinct user_id) as reg_users
  from dw_user_basic_info where reg_date = '2019-10-04'
  group by reg_date,server_id
) t 
left outer join 
(
  select login_date,server_id,count(distinct user_id) as login_users,sum(recharge_amount) as recharge_amount,sum(login_times) as login_times
  from dw_user_behavior_info where part_date = '2019-10-04'
  group by login_date,server_id
) t1 on (t1.login_date=t.reg_date and t1.server_id=t.server_id);
```

### 1.5、统计每日每个设备，每个设备型号下的数据情况

按日期和设备品牌、型号为维度统计，统计每天每种设备品牌、型号下的注册人数、登录人数、充值金额、登录次数：

#### sql语句开发

```sql
select t.reg_date,t.device_brand,t.device_type, reg_users, login_users, recharge_amount, login_times
from 
(
  select reg_date,device_brand,device_type,count(distinct user_id) as reg_users
  from dw_user_basic_info where reg_date = '2019-10-04'
  group by reg_date,device_brand,device_type
) t 
left outer join 
(
  select login_date,device_brand,device_type,count(distinct user_id) as login_users,sum(recharge_amount) as recharge_amount,sum(login_times) as login_times
  from dw_user_behavior_info where part_date = '2019-10-04'
  group by login_date,device_brand,device_type
) t1 on (t1.login_date =t.reg_date and t1.device_brand=t.device_brand and t1.device_type=t.device_type);
```



### 1.6、使用shell脚本进行执行

node03执行以下命令创建shell脚本

```
cd /home/hadoop/bin

vim res_user_basic_count.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="
sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;
create database if not exists "$DBNAME";

use "$DBNAME";

drop table if exists "$DBNAME".res_basic_count;
create table if not exists  "$DBNAME".res_basic_count as 
select t.reg_date,t.server_id, reg_users, login_users, recharge_amount, login_times
from 
(
  select reg_date,server_id,count(distinct user_id) as reg_users
  from dw_user_basic_info  where reg_date = '$part_date'
  group by reg_date,server_id
) t 
left outer join 
(
  select login_date,server_id,count(distinct user_id) as login_users,sum(recharge_amount) as recharge_amount,sum(login_times) as login_times
  from dw_user_behavior_info  where part_date = '$part_date'
  group by login_date,server_id
) t1 on (t1.login_date=t.reg_date and t1.server_id=t.server_id);


drop table if exists "$DBNAME".res_date_brand_type;
create table if not exists  "$DBNAME".res_date_brand_type as 
select t.reg_date,t.device_brand,t.device_type, reg_users, login_users, recharge_amount, login_times
from 
(
  select reg_date,device_brand,device_type,count(distinct user_id) as reg_users
  from dw_user_basic_info where reg_date = '$part_date'
  group by reg_date,device_brand,device_type
) t 
left outer join 
(
  select login_date,device_brand,device_type,count(distinct user_id) as login_users,sum(recharge_amount) as recharge_amount,sum(login_times) as login_times
  from dw_user_behavior_info where part_date = '$part_date'
  group by login_date,device_brand,device_type
) t1 on (t1.login_date =t.reg_date and t1.device_brand=t.device_brand and t1.device_type=t.device_type);

"
$hive_home -e "$sql"
```

执行shell脚本

```
sh res_user_basic_count.sh 2019-10-04
```

## 2、用户付费指标

### 2.1 付费指标有哪些

在我们的业务单元数据里面就有选择游戏行业通用的关于付费的指标，结构如下：

|        虚拟币        |                      元宝/绑元产生消耗                       | 每日更新前一天数据 |                           充值元宝                           |            选择时间当天充值元宝数量            |
| :------------------: | :----------------------------------------------------------: | :----------------: | :----------------------------------------------------------: | :--------------------------------------------: |
|       元宝产出       |                   选择时间当天元宝产出数量                   |                    |                                                              |                                                |
|       元宝消耗       |                   选择时间当天元宝消耗数量                   |                    |                                                              |                                                |
|       绑元产出       |                   选择时间当天绑元产出数量                   |                    |                                                              |                                                |
|       绑元消耗       |                   选择时间当天绑元消耗数量                   |                    |                                                              |                                                |
|     商城购买统计     |                      每日更新前一天数据                      |       元宝数       |             选择时间当天用户在商城购买的元宝数量             |                                                |
|         人数         |     选择时间当天用户在商城购买的人数，对用户id去重的结果     |                    |                                                              |                                                |
|        设备数        | 选择时间当天用户在商城购买的设备数，对设备号或者ip号去重的结果 |                    |                                                              |                                                |
|     产销途径统计     |                      每日更新前一天数据                      |      道具数量      |                选择时间当天每种道具的产出数量                |                                                |
|         人数         |             选择时间当天每种道具的有产出的用户数             |                    |                                                              |                                                |
|         次数         |               选择时间当天每种道具的有产出次数               |                    |                                                              |                                                |
|       游戏收入       |                         付费等级统计                         | 每日更新前一天数据 |                           充值次数                           | 选择时间当天充值的每个用户的最大等级的充值次数 |
|       充值人数       |        选择时间当天充值的每个用户的最大等级的充值人数        |                    |                                                              |                                                |
|       充值金额       |        选择时间当天充值的每个用户的最大等级的充值金额        |                    |                                                              |                                                |
|   首次充值等级统计   |                      每日更新前一天数据                      |      充值人数      |          选择时间当天每个用户第一次充值的等级的人数          |                                                |
|     充值分布统计     |                      每日更新前一天数据                      |      充值档位      |              选择时间当天用户单次充值的金额额度              |                                                |
|       充值次数       |         选择时间当天用户单次充值的金额额度的充值次数         |                    |                                                              |                                                |
|      累积充值额      |        选择时间当天用户单次充值的金额额度的充值金额量        |                    |                                                              |                                                |
|       货币数量       |       选择时间当天用户单次充值的金额额度的充值货币数量       |                    |                                                              |                                                |
|      充值排行榜      |                    每日全量更新截止到昨天                    |                    | 每个游戏的充值排前500的用户的充值金额和次数，以及他最后一次充值时的等级 |                                                |
| 新老用户充值情况统计 |                      每日更新前一天数据                      |     用户创建数     | 选择时间当天创建游戏账号并进入游戏的用户人数，对用户id去重的结果 |                                                |
|      独立设备数      | 选择时间当天的创建游戏账号并进入游戏的用户的设备数，对设备号或者ip号去重的结果 |                    |                                                              |                                                |
|        创角数        | 选择时间当天的创建游戏账号并且创建了角色的用户，对用户id去重的结果 |                    |                                                              |                                                |
|   当天总付费账号数   | 新用户选择时间当天是当天创建游戏账号并充值的用户数，老用户选择时间当天是在当天之前创建游戏账号但在当天充值的用户数 |                    |                                                              |                                                |
|   当天渠道付费总额   | 新用户选择时间当天是当天创建游戏账号并充值的金额，老用户选择时间当天是在当天之前创建游戏账号但在当天充值的金额 |                    |                                                              |                                                |

 

### 2.2 游戏常用指标

**DAU**(Daily Active User)日活跃用户数量 ：常用于反映网站、互联网应用或网络游戏的运营情况。DAU通常统计一日（统计日）之内，登录或使用了某个产品的用户数（去除重复登录的用户），这与流量统计工具里的访客（UV）概念相似。

**DNU**（Daily New Users）： 每日游戏中的新登入用户数量

**AU**（Active Users）：活跃用户，统计周期内，登录过游戏的用户数
相应的，根据统计周期，有DAU(日活跃用户)，WAU(周活跃用户),MAU(月活跃用户)等。

**PU** ( Paying User）：付费用户

**APA**（Active Payment Account）：活跃付费用户数。这里要注意“用户”和“付费用户”的区分，这也将影响收入的计算。

**ARPU**(Average Revenue Per User) ：平均每用户收入，即可通过 总收入/AU 计算得出。

**ARPPU** (Average Revenue Per Paying User)： 平均每付费用户收入，可通过 总收入/APA 计算得出。

**PUR**(Pay User Rate)：付费比率，可通过 APA/AU 计算得出。

 

### 2.3 LTV的计算

游戏指标很多，但是并不是每个公司都全部计算了这些指标，主要还是根据自身的业务情况来决定要计算哪些。这些指标通过中间层的表都可以计算出来，**LTV**这个指标几乎是每个公司都会有的，并且计算起来有点复杂。

LTV是衡量玩家价值的一个指标，是计算玩家从他注册那天起，一直到后面对游戏的充值，即**游戏流水的贡献**。因此LTV也可以称之为贡献，但是可能不同的公司对这个LTV有着不同的计算逻辑和理解，但是他们的共同点就是，玩家的充值不等于是游戏的收入，玩家的充值主要是在流水上，而游戏的收入在流水的基础之上提成的，是要和发行商发行渠道分的。

因此一般来讲的LTV就是指用户对流水的贡献。

 ![image-20200520165057110](数仓项目.assets/image-20200520165057110.png)

下面是LTV贡献的计算方式：

按注册日期计算，**1日LTV贡献=1日注册用户在1日充值的金额除于1日的充值人数**，2日LTV贡献=2日注册用户在2日充值的金额除于2日的充值人数....以此类推。

创建一个临时表，获取所有注册用户的创建角色信息和充值信息, 用于计算LTV贡献 ，这个表是以用户基础属性中间表为主表，关联充值日志，获取到所有用户信息。

#### 创建临时表用于计算LTV

```sql
create table tmp_user_ltv as 
select t.channel_id,t.plat_id,t.server_id, 
t.user_id,t.device_brand,t.device_type,t.reg_date,t.reg_time,t.role_id,t.role_name,t.role_create_date,t.role_create_time,
t1.pay_date,case when t1.recharge_amount is null then 0 else t1.recharge_amount end as recharge_amount, 0 as operating_system
from dw_user_basic_info t 
left outer join 
(
  select channel_id,plat_id,user_id,server_id,from_unixtime(event_time,'yyyy-MM-dd') as pay_date,role_id,sum(recharge_amount) as recharge_amount
  from ods_role_recharge  
  group by channel_id,plat_id,user_id,server_id,from_unixtime(event_time,'yyyy-MM-dd'),role_id
) t1 on (t1.channel_id=t.channel_id and t1.plat_id=t.plat_id and t1.server_id=t.server_id and t1.user_id=t.user_id and t1.role_id=t.role_id);
```

这里我们要根据平台，渠道，日期分组，计算每天每个平台每个渠道的LVT，和注册人数，这个要算到最近一年的ltv，就要往前推365天，要计算的指标：

注册人数、1日LTV、2日LTV、3日LTV、4日LTV、5日LTV、6日LTV、7日LTV、10日LTV、14日LTV、30日LTV、2月LTV、3月LTV、6月LTV、9月LTV、1年LTV等。

这里把 tmp_user_ltv 表分成了三部分，进行关联。这是为了优化，因为虽然是单表查询，但是计算的指标过于复杂，因此在计算的时候要达到最优效率，就要拆分。

第一部分是根据平台、渠道、日期来计算**注册人数。**

第二部分是根据平台、渠道、日期来计算**LTV金额。**

第三部分是根据平台、渠道、日期来计算**LTV人数。**

把这三部分分开计算关联，这样避免所有的计算逻辑在一个表区间计算，能很好的分开每个区间的计算压力，提高计算速度。

#### LTV结果计算

```sql
select t.plat_id,t.channel_id,t.event_date, t.reg_users,
ltv_1, ltv_2,ltv_3,ltv_4,ltv_5,ltv_6,ltv_7,ltv_10,ltv_14,ltv_30,ltv_2m,ltv_3m,ltv_6m,ltv_9m,ltv_1year,
ltv_1_users, ltv_2_users,ltv_3_users,ltv_4_users,ltv_5_users,ltv_6_users,ltv_7_users,ltv_10_users,ltv_14_users,ltv_30_users,ltv_2m_users,
ltv_3m_users,ltv_6m_users,ltv_9m_users,ltv_1year_users
from 
(
  select plat_id,channel_id,reg_date as event_date, count(distinct user_id) as reg_users
  from tmp_user_ltv 
  group by plat_id,channel_id,reg_date
) t 
left outer join 
(
  select plat_id,channel_id,reg_date as event_date, 
  sum(case when reg_date=pay_date then recharge_amount else 0 end) as ltv_1, 
  sum(case when pay_date<=date_add(reg_date,1) then recharge_amount else 0 end) as ltv_2,
  sum(case when pay_date<=date_add(reg_date,2) then recharge_amount else 0 end) as ltv_3,
  sum(case when pay_date<=date_add(reg_date,3) then recharge_amount else 0 end) as ltv_4,
  sum(case when pay_date<=date_add(reg_date,4) then recharge_amount else 0 end) as ltv_5,
  sum(case when pay_date<=date_add(reg_date,5) then recharge_amount else 0 end) as ltv_6,
  sum(case when pay_date<=date_add(reg_date,6) then recharge_amount else 0 end) as ltv_7,
  sum(case when pay_date<=date_add(reg_date,9) then recharge_amount else 0 end) as ltv_10,
  sum(case when pay_date<=date_add(reg_date,13) then recharge_amount else 0 end) as ltv_14,
  sum(case when pay_date<=date_add(reg_date,29) then recharge_amount else 0 end) as ltv_30,
  sum(case when pay_date<=date_add(reg_date,59) then recharge_amount else 0 end) as ltv_2m,
  sum(case when pay_date<=date_add(reg_date,89) then recharge_amount else 0 end) as ltv_3m,
  sum(case when pay_date<=date_add(reg_date,179) then recharge_amount else 0 end) as ltv_6m,
  sum(case when pay_date<=date_add(reg_date,269) then recharge_amount else 0 end) as ltv_9m,
  sum(case when pay_date<=date_add(reg_date,355) then recharge_amount else 0 end) as ltv_1year
  from tmp_user_ltv 
  where recharge_amount>0
  group by plat_id,channel_id,reg_date
) t1 on (t1.plat_id=t.plat_id and t1.channel_id=t.channel_id and t1.event_date=t.event_date)
left outer join 
(
  select plat_id,channel_id,reg_date as event_date,
  count(case when reg_date=pay_date then user_id end) as ltv_1_users, 
  count(case when pay_date=date_add(reg_date,1)   then user_id end) as ltv_2_users,
  count(case when pay_date=date_add(reg_date,2)   then user_id end) as ltv_3_users,
  count(case when pay_date=date_add(reg_date,3)   then user_id end) as ltv_4_users,
  count(case when pay_date=date_add(reg_date,4)   then user_id end) as ltv_5_users,
  count(case when pay_date=date_add(reg_date,5)   then user_id end) as ltv_6_users,
  count(case when pay_date=date_add(reg_date,6)   then user_id end) as ltv_7_users,
  count(case when pay_date=date_add(reg_date,9)   then user_id end) as ltv_10_users,
  count(case when pay_date=date_add(reg_date,13)  then user_id end) as ltv_14_users,
  count(case when pay_date=date_add(reg_date,29)  then user_id end) as ltv_30_users,
  count(case when pay_date=date_add(reg_date,59)  then user_id end) as ltv_2m_users,
  count(case when pay_date=date_add(reg_date,89)  then user_id end) as ltv_3m_users,
  count(case when pay_date=date_add(reg_date,179) then user_id end) as ltv_6m_users,
  count(case when pay_date=date_add(reg_date,269) then user_id end) as ltv_9m_users,
  count(case when pay_date=date_add(reg_date,355) then user_id end) as ltv_1year_users
  from 
  (
   select plat_id,channel_id,reg_date,pay_date,user_id
   from tmp_user_ltv 
   where recharge_amount>0
   group by plat_id,channel_id,reg_date,pay_date,user_id
  ) t 
  group by plat_id,channel_id,reg_date
) t2 on (t2.plat_id=t.plat_id and t2.channel_id=t.channel_id and t2.event_date=t.event_date);
```



### 2.4 虚拟币产出和消耗 

我们主要就是计算虚拟货币的消耗产出和消费，这个是玩家在游戏内部付费行为的重要指标，也是运营调节玩家在游戏内部的交易意向的关键信息。

虚拟币购入：**时期内每日玩家充值所购入的虚拟币数量；**

虚拟币赠予：**时期内每日玩家所获得赠予的虚拟币数量；**

虚拟币消耗：**时期内每日玩家在全部消费点中所消耗掉的虚拟币数量；**

这里我们从中间层的表出这个数据，根据**日期和平台作为维度**，**计算用户购入、赠予、消费的数量。**

在 dw_user_behavior_info 中，有 acer_count充值后获得的元宝数量，有acer_stock 元宝存量， 而**昨日的元宝存量，减去今日的元宝存量，如果得到是正数，就是消耗掉的元宝，如果是负数，那就没有消耗掉，而这一部分减去充值的元宝，就是被赠送的元宝。**

这里的算法涉及到一个日期交错的相减算法，因此要用到一个开窗函数，LAG（）。

LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值

第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）

#### 定义sql语句

```sql
select login_date as event_date, plat_id, 
acer_count as buy_num,
case when consumer>0 then consumer else 0 end as consumer_num,
case when consumer<0 then abs(consumer)-acer_count else 0 end as give_num
from 
(
  select login_date,plat_id,acer_count,acer_stock,
  last_acer_stock-acer_stock as consumer
  from 
  (
    select login_date,plat_id,acer_count,acer_stock,
    LAG(acer_stock,1,0) OVER(PARTITION BY plat_id ORDER BY login_date) AS last_acer_stock
    from 
    (
      select login_date,plat_id,sum(acer_count) as acer_count,sum(acer_stock) as acer_stock
      from dw_user_behavior_info 
      group by login_date,plat_id
    ) t 
  ) t 
) t ;

```

### 2.5 消费喜好 

消费喜好主要就是统计每个平台的每个物品的购买次数和购买金额。

这个涉及到每个商品类别了和次数，而中间层是属于粒度整合层，没有明细的计算，因此这个数据要从原始层来计算。

这里的recharge_purpose 是充值用途，充值用途就包含了各种物品，所以要以此为维度。

```sql
select part_date,plat_id,recharge_purpose,
count(order_id) as buy_times,
sum(recharge_amount)
from ods_role_recharge 
group by part_date,plat_id,recharge_purpose;
```

 

### 2.6 消费点 

这个消费点同样是计算每种物品的购买情况，但是同时也要计算消耗情况，虚拟币总值，就是指的是购买的金额。如下图所示。

![image-20200520165553364](数仓项目.assets/image-20200520165553364.png)

这里同样有个消耗，消耗同样是根据昨日和今日差值所计算的，因此还要用到开窗函数，LAG（）。

同时这个还加了关卡的等级，因此这个表还要加等级表的关联，作为维度条件，关联等级时，要定位到每个用户每个角色在当日的等级。

```sql
select part_date,plat_id,recharge_purpose,level_after,buy_times,recharge_amount,acer_count,
last_acer_stock-acer_count as consumer_num
from 
(
  select part_date,plat_id,recharge_purpose,level_after,buy_times,recharge_amount,acer_count,
  LAG(acer_count,1,0) OVER(PARTITION BY plat_id,recharge_purpose ORDER BY part_date) AS last_acer_stock
  from 
  (
    select t.part_date,t.plat_id,t.recharge_purpose,t1.level_after,
    sum(buy_times) as buy_times,
    sum(recharge_amount) as recharge_amount,
    sum(acer_count) as acer_count
    from  
    (
      select part_date,plat_id,user_id,role_id,recharge_purpose,
      count(order_id) as buy_times,
      sum(recharge_amount) as recharge_amount,
      sum(acer_count) as acer_count
      from ods_role_recharge 
      group by part_date,plat_id,user_id,role_id,recharge_purpose
    ) t 
    left outer join 
    (
      select part_date,plat_id,user_id,role_id,max(level_after) as level_after
      from ods_role_level_up 
      group by part_date,plat_id,user_id,role_id
    ) t1 on (t1.part_date=t.part_date and t1.plat_id=t.plat_id and t1.user_id=t.user_id and t1.role_id=t.role_id)
    group by t.part_date,t.plat_id,t.recharge_purpose,t1.level_after
  ) t 
) t ;

```

### 2.7、使用shell脚本统计汇总以上指标

```
cd /home/hadoop/bin/

vim res_user_pay.sh
```

```sql
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".tmp_user_ltv;

create table if not exists "$DBNAME".tmp_user_ltv as 
select t.channel_id,t.plat_id,t.server_id, 
t.user_id,t.device_brand,t.device_type,t.reg_date,t.reg_time,t.role_id,t.role_name,t.role_create_date,t.role_create_time,
t1.pay_date,case when t1.recharge_amount is null then 0 else t1.recharge_amount end as recharge_amount, 0 as operating_system
from dw_user_basic_info t 
left outer join 
(
  select channel_id,plat_id,user_id,server_id,from_unixtime(event_time,'yyyy-MM-dd') as pay_date,role_id,sum(recharge_amount) as recharge_amount
  from ods_role_recharge  
  group by channel_id,plat_id,user_id,server_id,from_unixtime(event_time,'yyyy-MM-dd'),role_id
) t1 on (t1.channel_id=t.channel_id and t1.plat_id=t.plat_id and t1.server_id=t.server_id and t1.user_id=t.user_id and t1.role_id=t.role_id);


drop table if exists "$DBNAME".res_ltv_count;

create table if not exists  "$DBNAME".res_ltv_count as 
select t.plat_id,t.channel_id,t.event_date, t.reg_users,
ltv_1, ltv_2,ltv_3,ltv_4,ltv_5,ltv_6,ltv_7,ltv_10,ltv_14,ltv_30,ltv_2m,ltv_3m,ltv_6m,ltv_9m,ltv_1year,
ltv_1_users, ltv_2_users,ltv_3_users,ltv_4_users,ltv_5_users,ltv_6_users,ltv_7_users,ltv_10_users,ltv_14_users,ltv_30_users,ltv_2m_users,
ltv_3m_users,ltv_6m_users,ltv_9m_users,ltv_1year_users
from 
(
  select plat_id,channel_id,reg_date as event_date, count(distinct user_id) as reg_users
  from tmp_user_ltv 
  group by plat_id,channel_id,reg_date
) t 
left outer join 
(
  select plat_id,channel_id,reg_date as event_date, 
  sum(case when reg_date=pay_date then recharge_amount else 0 end) as ltv_1, 
  sum(case when pay_date<=date_add(reg_date,1) then recharge_amount else 0 end) as ltv_2,
  sum(case when pay_date<=date_add(reg_date,2) then recharge_amount else 0 end) as ltv_3,
  sum(case when pay_date<=date_add(reg_date,3) then recharge_amount else 0 end) as ltv_4,
  sum(case when pay_date<=date_add(reg_date,4) then recharge_amount else 0 end) as ltv_5,
  sum(case when pay_date<=date_add(reg_date,5) then recharge_amount else 0 end) as ltv_6,
  sum(case when pay_date<=date_add(reg_date,6) then recharge_amount else 0 end) as ltv_7,
  sum(case when pay_date<=date_add(reg_date,9) then recharge_amount else 0 end) as ltv_10,
  sum(case when pay_date<=date_add(reg_date,13) then recharge_amount else 0 end) as ltv_14,
  sum(case when pay_date<=date_add(reg_date,29) then recharge_amount else 0 end) as ltv_30,
  sum(case when pay_date<=date_add(reg_date,59) then recharge_amount else 0 end) as ltv_2m,
  sum(case when pay_date<=date_add(reg_date,89) then recharge_amount else 0 end) as ltv_3m,
  sum(case when pay_date<=date_add(reg_date,179) then recharge_amount else 0 end) as ltv_6m,
  sum(case when pay_date<=date_add(reg_date,269) then recharge_amount else 0 end) as ltv_9m,
  sum(case when pay_date<=date_add(reg_date,355) then recharge_amount else 0 end) as ltv_1year
  from tmp_user_ltv 
  where recharge_amount>0
  group by plat_id,channel_id,reg_date
) t1 on (t1.plat_id=t.plat_id and t1.channel_id=t.channel_id and t1.event_date=t.event_date)
left outer join 
(
  select plat_id,channel_id,reg_date as event_date,
  count(case when reg_date=pay_date then user_id end) as ltv_1_users, 
  count(case when pay_date=date_add(reg_date,1)   then user_id end) as ltv_2_users,
  count(case when pay_date=date_add(reg_date,2)   then user_id end) as ltv_3_users,
  count(case when pay_date=date_add(reg_date,3)   then user_id end) as ltv_4_users,
  count(case when pay_date=date_add(reg_date,4)   then user_id end) as ltv_5_users,
  count(case when pay_date=date_add(reg_date,5)   then user_id end) as ltv_6_users,
  count(case when pay_date=date_add(reg_date,6)   then user_id end) as ltv_7_users,
  count(case when pay_date=date_add(reg_date,9)   then user_id end) as ltv_10_users,
  count(case when pay_date=date_add(reg_date,13)  then user_id end) as ltv_14_users,
  count(case when pay_date=date_add(reg_date,29)  then user_id end) as ltv_30_users,
  count(case when pay_date=date_add(reg_date,59)  then user_id end) as ltv_2m_users,
  count(case when pay_date=date_add(reg_date,89)  then user_id end) as ltv_3m_users,
  count(case when pay_date=date_add(reg_date,179) then user_id end) as ltv_6m_users,
  count(case when pay_date=date_add(reg_date,269) then user_id end) as ltv_9m_users,
  count(case when pay_date=date_add(reg_date,355) then user_id end) as ltv_1year_users
  from 
  (
   select plat_id,channel_id,reg_date,pay_date,user_id
   from tmp_user_ltv 
   where recharge_amount>0
   group by plat_id,channel_id,reg_date,pay_date,user_id
  ) t 
  group by plat_id,channel_id,reg_date
) t2 on (t2.plat_id=t.plat_id and t2.channel_id=t.channel_id and t2.event_date=t.event_date);


drop table if exists "$DBNAME".res_stock_consumer;

create table if not exists "$DBNAME".res_stock_consumer as 
select login_date as event_date, plat_id, 
acer_count as buy_num,
case when consumer>0 then consumer else 0 end as consumer_num,
case when consumer<0 then abs(consumer)-acer_count else 0 end as give_num
from 
(
  select login_date,plat_id,acer_count,acer_stock,
  last_acer_stock-acer_stock as consumer
  from 
  (
    select login_date,plat_id,acer_count,acer_stock,
    LAG(acer_stock,1,0) OVER(PARTITION BY plat_id ORDER BY login_date) AS last_acer_stock
    from 
    (
      select login_date,plat_id,sum(acer_count) as acer_count,sum(acer_stock) as acer_stock
      from dw_user_behavior_info 
      group by login_date,plat_id
    ) t 
  ) t 
) t ;


drop table if exists "$DBNAME".res_consumer_hobby;

create table if not exists "$DBNAME".res_consumer_hobby as
select part_date,plat_id,recharge_purpose,
count(order_id) as buy_times,
sum(recharge_amount) as sum_amount
from ods_role_recharge 
group by part_date,plat_id,recharge_purpose;

drop table if exists "$DBNAME".res_consumer_point;

create table if not exists "$DBNAME".res_consumer_point as 
select part_date,plat_id,recharge_purpose,level_after,buy_times,recharge_amount,acer_count,
last_acer_stock-acer_count as consumer_num
from 
(
  select part_date,plat_id,recharge_purpose,level_after,buy_times,recharge_amount,acer_count,
  LAG(acer_count,1,0) OVER(PARTITION BY plat_id,recharge_purpose ORDER BY part_date) AS last_acer_stock
  from 
  (
    select t.part_date,t.plat_id,t.recharge_purpose,t1.level_after,
    sum(buy_times) as buy_times,
    sum(recharge_amount) as recharge_amount,
    sum(acer_count) as acer_count
    from  
    (
      select part_date,plat_id,user_id,role_id,recharge_purpose,
      count(order_id) as buy_times,
      sum(recharge_amount) as recharge_amount,
      sum(acer_count) as acer_count
      from ods_role_recharge 
      group by part_date,plat_id,user_id,role_id,recharge_purpose
    ) t 
    left outer join 
    (
      select part_date,plat_id,user_id,role_id,max(level_after) as level_after
      from ods_role_level_up 
      group by part_date,plat_id,user_id,role_id
    ) t1 on (t1.part_date=t.part_date and t1.plat_id=t.plat_id and t1.user_id=t.user_id and t1.role_id=t.role_id)
    group by t.part_date,t.plat_id,t.recharge_purpose,t1.level_after
  ) t 
) t ;
"
$hive_home -e "$sql"
```

执行shell脚本

```sql
sh res_user_pay.sh
```

## 3、用户留存指标

用户留存，就是要看用户**从注册到后面的时间是否还会登录**，比如1日注册的用户在第二天、第三天如果还登录的话，就分别叫做次留（次日留存）、三留（第三日留存）。这个指标用户判断用户的粘度。

先创建一个临时表，以用户属性中间层表来关联登陆日志，用于获取所有注册用户的创建角色信息，加上最近30天的登陆时间，用于计算留存。在实际业务中，一般留存就是看到30天，超过30天的留存就不会有太大的意义了。

### 创建临时表用于计算用户留存

```sql
create table tmp_user_stay as 
select t.channel_id,t.plat_id,t.server_id,t.user_id,t.device_brand,t.device_type,
t.reg_date,t.reg_time,t.role_id,t.role_name,t.role_create_date,t.role_create_time,t1.login_date 
from dw_user_basic_info t 
left outer join 
(
  select channel_id,plat_id,user_id,server_id, from_unixtime(event_time,'yyyy-MM-dd') as login_date ,role_id
  from ods_user_login 
  group by channel_id,plat_id,user_id,server_id, from_unixtime(event_time,'yyyy-MM-dd'),role_id
) t1 on (t1.plat_id=t.plat_id and t1.server_id=t.server_id and t1.user_id=t.user_id and t1.role_id=t.role_id);

```

### 计算用户留存

根据用户登录属性的临时表来计算每天每个平台下每个渠道的注册用户和这些用户的留存数，指标为：

注册用户数、次留、3留、4留、5留、6留、7留、15留、月留。

```sql
select plat_id,channel_id, reg_date as event_date,
count(distinct user_id) as reg_users,
count(distinct case when datediff(login_date,reg_date)=1 then user_id end) as stay_s,
count(distinct case when datediff(login_date,reg_date)=2 then user_id end) as stay_3_s,
count(distinct case when datediff(login_date,reg_date)=3 then user_id end) as stay_4_s,
count(distinct case when datediff(login_date,reg_date)=4 then user_id end) as stay_5_s,
count(distinct case when datediff(login_date,reg_date)=5 then user_id end) as stay_6_s,
count(distinct case when datediff(login_date,reg_date)=6 then user_id end) as stay_7_s,
count(distinct case when datediff(login_date,reg_date)=14 then user_id end) as stay_15_s,
count(distinct case when datediff(login_date,reg_date)=29 then user_id end) as stay_30_s
from tmp_user_stay 
group by plat_id,channel_id,reg_date;
```

### 使用shell脚本来计算用户留存

```
cd /home/hadoop/bin/

vim res_user_stay_long.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".tmp_user_stay;

create table if not exists  "$DBNAME".tmp_user_stay as 
select t.channel_id,t.plat_id,t.server_id,t.user_id,t.device_brand,t.device_type,
t.reg_date,t.reg_time,t.role_id,t.role_name,t.role_create_date,t.role_create_time,t1.login_date 
from dw_user_basic_info t 
left outer join 
(
  select channel_id,plat_id,user_id,server_id, from_unixtime(event_time,'yyyy-MM-dd') as login_date ,role_id
  from ods_user_login 
  group by channel_id,plat_id,user_id,server_id, from_unixtime(event_time,'yyyy-MM-dd'),role_id
) t1 on (t1.plat_id=t.plat_id and t1.server_id=t.server_id and t1.user_id=t.user_id and t1.role_id=t.role_id);

drop table if exists "$DBNAME".res_user_stay;

create table "$DBNAME".res_user_stay as 
select plat_id,channel_id, reg_date as event_date,
count(distinct user_id) as reg_users,
count(distinct case when datediff(login_date,reg_date)=1 then user_id end) as stay_s,
count(distinct case when datediff(login_date,reg_date)=2 then user_id end) as stay_3_s,
count(distinct case when datediff(login_date,reg_date)=3 then user_id end) as stay_4_s,
count(distinct case when datediff(login_date,reg_date)=4 then user_id end) as stay_5_s,
count(distinct case when datediff(login_date,reg_date)=5 then user_id end) as stay_6_s,
count(distinct case when datediff(login_date,reg_date)=6 then user_id end) as stay_7_s,
count(distinct case when datediff(login_date,reg_date)=14 then user_id end) as stay_15_s,
count(distinct case when datediff(login_date,reg_date)=29 then user_id end) as stay_30_s
from tmp_user_stay 
group by plat_id,channel_id,reg_date;
"
$hive_home -e "$sql"
```

## 4、用户流失指标

建一个临时表，获取到每个用户的登录日期，并且根据每个平台渠道下的每个用户，按照登录日期做一个倒序排序，并以此做好标记。

### 创建临时表用于计算用户流失

```sql
create table tmp_user_lost as 
select t.plat_id,t.channel_id,t.user_id,login_date,
ROW_NUMBER() OVER(PARTITION BY t.channel_id,t.plat_id,t.user_id ORDER BY login_date desc) AS rn
from 
(
  select plat_id,channel_id,user_id,from_unixtime(event_time,'yyyy-MM-dd') as login_date
  from ods_user_login
  group by plat_id,channel_id,user_id,from_unixtime(event_time,'yyyy-MM-dd')
) t ;
```

 

### 计算用户流失

我们要目标用户群1天，3天，7天，15天，30天，60天流失率。x天流失率=目标用户在两次登录日期的间隔天数的用户除以目标日期的登录用户。我们要计算的指标为：

登录用户数，登录间隔1天的用户数，登录间隔3天的用户数，登录间隔7天的用户数，登录间隔15天的用户数，登录间隔30天的用户数，登录间隔60天的用户数。

 

```sql
select plat_id,channel_id,login_date as event_date, 
count(distinct user_id) as users,
count(distinct case when num>=1 then user_id end) as login1,
count(distinct case when num>=3 then user_id end) as login3,
count(distinct case when num>=7 then user_id end) as login7,
count(distinct case when num>=15 then user_id end) as login15,
count(distinct case when num>=30 then user_id end) as login30,
count(distinct case when num>=60 then user_id end) as login60
from 
(
  select plat_id,channel_id,user_id,login_date,datediff(login_date_2,login_date)-1 as num
  from 
  (
    select t.plat_id,t.channel_id,t.user_id,t.login_date,
    case when t1.login_date is null then from_unixtime(unix_timestamp(),'yyyy-MM-dd') else t1.login_date end as login_date_2
    from tmp_user_lost t 
    left outer join tmp_user_lost t1 on (t1.plat_id=t.plat_id and t1.channel_id=t.channel_id and t1.user_id=t.user_id and t1.rn+1=t.rn)
  ) t 
) t 
group by plat_id,channel_id,login_date;

```



### 使用shell脚本统计汇总用户流失

```
cd /home/hadoop/bin/

vim res_user_lost.sh
```

```sql
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".tmp_user_lost;

create table if not exists  "$DBNAME".tmp_user_lost as 
select t.plat_id,t.channel_id,t.user_id,login_date,
ROW_NUMBER() OVER(PARTITION BY t.channel_id,t.plat_id,t.user_id ORDER BY login_date desc) AS rn
from 
(
  select plat_id,channel_id,user_id,from_unixtime(event_time,'yyyy-MM-dd') as login_date
  from ods_user_login
  group by plat_id,channel_id,user_id,from_unixtime(event_time,'yyyy-MM-dd')
) t ;

drop table if exists "$DBNAME".res_user_lost;

create table if not exists   "$DBNAME".res_user_lost as 
select plat_id,channel_id,login_date as event_date, 
count(distinct user_id) as users,
count(distinct case when num>=1 then user_id end) as login1,
count(distinct case when num>=3 then user_id end) as login3,
count(distinct case when num>=7 then user_id end) as login7,
count(distinct case when num>=15 then user_id end) as login15,
count(distinct case when num>=30 then user_id end) as login30,
count(distinct case when num>=60 then user_id end) as login60
from 
(
  select plat_id,channel_id,user_id,login_date,datediff(login_date_2,login_date)-1 as num
  from 
  (
    select t.plat_id,t.channel_id,t.user_id,t.login_date,
    case when t1.login_date is null then from_unixtime(unix_timestamp(),'yyyy-MM-dd') else t1.login_date end as login_date_2
    from tmp_user_lost t 
    left outer join tmp_user_lost t1 on (t1.plat_id=t.plat_id and t1.channel_id=t.channel_id and t1.user_id=t.user_id and t1.rn+1=t.rn)
  ) t 
) t 
group by plat_id,channel_id,login_date;

"
$hive_home -e "$sql"
```



## 5、任务分析

这个主要是计算游戏任务的的情况，每个任务的类型，进入次数，平均完成时间，失败次数。

这个数据来源于任务日志，但是任务日志没有整合到中间层，因为任务日志量比较大，而且任务类型维度多，因此这个任务的统计适合直接从原始层出。

### 任务分析指标统计

这里的op_type的值分别为， 1：接受任务，2：完成任务，3：放弃任务

```sql
select part_date,plat_id,task_type,task_id,
count(1) as enter_times,
sum(cost_time) as cost_time,
count(case when op_type=3 then task_id end) as fail_times
from ods_task_log
group by part_date,plat_id,task_type,task_id;
```

### 使用shell脚本统计任务分析

```
cd /home/hadoop/bin/

vim res_mission.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".tmp_mission;

create table if not exists "$DBNAME".res_mission as 
select part_date,plat_id,task_type,task_id,
count(1) as enter_times,
sum(cost_time) as cost_time,
count(case when op_type=3 then task_id end) as fail_times
from ods_task_log
group by part_date,plat_id,task_type,task_id;
"
$hive_home -e "$sql"
```

## 6、渠道数据

### 1、渠道数量指标

渠道数量指标，主要是统计**分析不同渠道下的设备激活和新玩家注册**，这样可以统计到用户的转化，找到最有价值的渠道商。

这个渠道是用户基础信息，可以在中间层 dw_user_basic_info 表中直接统计计算。

```sql
select login_date,channel_id,
count(distinct client_ip,device_brand,device_type) as device_num,
count(distinct user_id) as reg_user
from dw_user_behavior_info 
group by login_date,channel_id  ;
```

 

### 2、渠道质量指标

渠道质量，就是统计不同渠道下的用户的活跃量，一日玩家量，就是每天只玩了一次的玩家，付费比例。

这个数据我们可以从中间层 dw_user_behavior_info 表出，因为这里我们整合了每个用户的行为记录，包括登录和付费。

```sql
select login_date,channel_id,
count(distinct user_id) as active_users,
count(distinct case when login_times=1 then user_id end) as play_once_users,
count(distinct case when recharge_amount>0 then user_id end) as pay_users
from dw_user_behavior_info
group by login_date,channel_id;
```

### 3、渠道收入指标

渠道收入主要就是统计每个渠道下的付费用户和付费金额

这个数据也可以直接从中间层 dw_user_behavior_info 表出。

```sql
select login_date,channel_id,
count(distinct user_id) as pay_user,
sum(recharge_amount) as pay_money
from dw_user_behavior_info
where recharge_amount>0
group by login_date,channel_id;
```

### 4、渠道玩家留存

渠道玩家留存就跟之前的留存一样，只是按照渠道来区分。

这个直接可以直接套用之前计算留存的语句和算法。因为留存的算法都是统一的。

```sql
select plat_id,channel_id, reg_date as event_date,
count(distinct user_id) as reg_users,
count(distinct case when datediff(login_date,reg_date)=1 then user_id end) as stay_s,
count(distinct case when datediff(login_date,reg_date)=2 then user_id end) as stay_3_s,
count(distinct case when datediff(login_date,reg_date)=3 then user_id end) as stay_4_s,
count(distinct case when datediff(login_date,reg_date)=4 then user_id end) as stay_5_s,
count(distinct case when datediff(login_date,reg_date)=5 then user_id end) as stay_6_s,
count(distinct case when datediff(login_date,reg_date)=6 then user_id end) as stay_7_s,
count(distinct case when datediff(login_date,reg_date)=14 then user_id end) as stay_15_s,
count(distinct case when datediff(login_date,reg_date)=29 then user_id end) as stay_30_s
from tmp_user_stay 
group by plat_id,channel_id,reg_date;
```



### 5 、使用shell脚本实现以上统计

```
cd /home/hadoop/bin

vim res_channel_count.sh
```

```sql
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".res_channel_num;

create table if not exists  "$DBNAME".res_channel_num as 
select login_date,channel_id,
count(distinct client_ip,device_brand,device_type) as device_num,
count(distinct user_id) as reg_user,
count(distinct user_id) as active_users,
count(distinct case when login_times=1 then user_id end) as play_once_users,
count(distinct case when recharge_amount>0 then user_id end) as pay_users,
count(distinct user_id) as pay_user,
sum(recharge_amount) as pay_money 
from dw_user_behavior_info 
where recharge_amount>0
group by login_date,channel_id ;

drop table if exists "$DBNAME".res_channel_stay;

create table if not exists "$DBNAME".res_channel_stay as 
select plat_id,channel_id, reg_date as event_date,
count(distinct user_id) as reg_users,
count(distinct case when datediff(login_date,reg_date)=1 then user_id end) as stay_s,
count(distinct case when datediff(login_date,reg_date)=2 then user_id end) as stay_3_s,
count(distinct case when datediff(login_date,reg_date)=3 then user_id end) as stay_4_s,
count(distinct case when datediff(login_date,reg_date)=4 then user_id end) as stay_5_s,
count(distinct case when datediff(login_date,reg_date)=5 then user_id end) as stay_6_s,
count(distinct case when datediff(login_date,reg_date)=6 then user_id end) as stay_7_s,
count(distinct case when datediff(login_date,reg_date)=14 then user_id end) as stay_15_s,
count(distinct case when datediff(login_date,reg_date)=29 then user_id end) as stay_30_s
from tmp_user_stay 
group by plat_id,channel_id,reg_date;
"
$hive_home -e "$sql"
```

 

## 7、玩家流失的最后3步操作

这个数据就是要看每个游戏中的用户，在退出游戏时候是哪几步操作，这样可以反映最后那几个按钮的设置是否合理，并能获取到最常见的操作步骤加以进一步分析。

### 任务分析指标统计

计算方法就是根据每日登录用户的登录操作，在获取到这部分用户在游戏中所操作的过按钮，按照时间倒叙排序取最后3个按钮，就是用户的最后3步操作的按钮。

```sql
select t.plat_id,t.user_id,role_name,opt_one,opt_two,opt_three
from 
(
  select plat_id,user_id from ods_user_login where part_date='2019-10-21' and op_type=1 group by plat_id,user_id
) t 
join 
(
  select plat_id,user_id, role_name,split(tt,',')[2] as opt_one, split(tt,',')[1] as opt_two, split(tt,',')[0] as opt_three
  from 
  (
    select plat_id,user_id,role_name,concat_ws(',',collect_list(button_id)) as tt
    from 
    (
      select t.plat_id,channel_id,user_id,role_id,role_name,case when t1.button_name is null then t.button_id else t1.button_name end as button_id,
      ROW_NUMBER() OVER(PARTITION BY user_id,role_name ORDER BY event_time desc) as rn 
      from ods_panel_op t 
      left outer join ods_dim_game_button t1 on (t1.button_id=t.button_id)
      where part_date='2019-10-03'
    ) t 
    where rn<=3 
    group by plat_id,user_id,role_name
  ) t 
) t1 on (t1.plat_id=t.plat_id and t1.user_id=t.user_id);
```

### 使用shell脚本统计最后3步操作

```
cd /home/hadoop/bin/

vim res_user_last_operate.sh
```

```sql
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".res_user_last_operate;

create table if not exists "$DBNAME".res_user_last_operate as 
select t.plat_id,t.user_id,role_name,opt_one,opt_two,opt_three
from 
(
  select plat_id,user_id from ods_user_login where part_date='2019-10-21' and op_type=1 group by plat_id,user_id
) t 
join 
(
  select plat_id,user_id, role_name,split(tt,',')[2] as opt_one, split(tt,',')[1] as opt_two, split(tt,',')[0] as opt_three
  from 
  (
    select plat_id,user_id,role_name,concat_ws(',',collect_list(button_id)) as tt
    from 
    (
      select t.plat_id,channel_id,user_id,role_id,role_name,case when t1.button_name is null then t.button_id else t1.button_name end as button_id,
      ROW_NUMBER() OVER(PARTITION BY user_id,role_name ORDER BY event_time desc) as rn 
      from ods_panel_op t 
      left outer join ods_dim_game_button t1 on (t1.button_id=t.button_id)
      where part_date='2019-10-03'
    ) t 
    where rn<=3 
    group by plat_id,user_id,role_name
  ) t 
) t1 on (t1.plat_id=t.plat_id and t1.user_id=t.user_id);

"
$hive_home -e "$sql"
```



## 8、游戏虚拟币总量计算

在游戏日志中， 如果要查看每个玩家的总共拥有多少虚拟币是比较麻烦的，因为用户的虚拟币来源很多，比如来自充值、系统赠送、道具交易、任务奖励、活动奖励。

而这些总量一直是在变化的，他在获取的同时也在产生消耗。而日志数据记录的每一条数据都是固定的，每一条日志都会记录玩家当前时间相应的存量。

而每一次玩家对虚拟币的获得或者消费，日志都会记录一次他的总量，因此如果要获取玩家当前最准确的虚拟币，就需要从虚拟币的产出和消费整合之后取最新的记录值，才是最正确当天这个玩家真实的虚拟币总量。

### 虚拟币总量计算统计

因此我们计算时就要将虚拟币产出和消费整合起来，然后根据相应的时间进行倒叙排序取最新的一条存量，取到每个用户每个角色下的虚拟币总量，而每个用户又有多个角色，再讲每个角色下的虚拟币加起来，就是每个用户总的虚拟币。

```sql
select plat_id,channel_id,user_id,sum(monetary_stock) as monetary_stock
from 
(
  select plat_id,channel_id,user_id,role_id,monetary_stock
  from 
  (
    select plat_id,channel_id,user_id,role_id,monetary_stock,event_time,
    ROW_NUMBER() OVER(PARTITION BY plat_id,user_id,role_id ORDER BY event_time desc) rn
    from 
    (
      select plat_id,channel_id,user_id,role_id,monetary_stock,event_time
      from ods_monetary_consume 
      where part_date>='2019-10-03' and part_date<='2019-10-04' and monetary_stock>0
      union all  
      select plat_id,channel_id,user_id,role_id,monetary_stock,event_time
      from ods_monetary_output
      where part_date>='2019-10-04' and part_date<='2019-10-05' and monetary_stock>0
    ) t 
  ) t 
  where rn=1
) t 
group by plat_id,channel_id,user_id;
```

### 使用shell脚本实现虚拟币总量统计

```
cd /home/hadoop/bin/

vim res_virtual_coin.sh
```

```sql
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".res_virtual_coin;

create table if not exists "$DBNAME".res_virtual_coin as 
select plat_id,channel_id,user_id,sum(monetary_stock) as monetary_stock
from 
(
  select plat_id,channel_id,user_id,role_id,monetary_stock
  from 
  (
    select plat_id,channel_id,user_id,role_id,monetary_stock,event_time,
    ROW_NUMBER() OVER(PARTITION BY plat_id,user_id,role_id ORDER BY event_time desc) rn
    from 
    (
      select plat_id,channel_id,user_id,role_id,monetary_stock,event_time
      from ods_monetary_consume 
      where part_date>='2019-10-03' and part_date<='2019-10-04' and monetary_stock>0
      union all  
      select plat_id,channel_id,user_id,role_id,monetary_stock,event_time
      from ods_monetary_output
      where part_date>='2019-10-04' and part_date<='2019-10-05' and monetary_stock>0
    ) t 
  ) t 
  where rn=1
) t 
group by plat_id,channel_id,user_id;
"
$hive_home -e "$sql"
```



## 9、玩家死亡关卡统计

死亡关卡统计就是要统计每个关卡下死亡多少人，其中多少人是当天注册进来的新玩家，运营会可以通过这个指标来参考每个关卡的难易程度。

### 玩家死亡关卡统计

计算每天每个平台、渠道、每个关卡下的死亡人数和死亡次数，以及新注册玩家的死亡人数。

```sql
select t.event_date, t.plat_id,t.channel_id,t.map_id,
count(distinct t.user_id) as death_users, sum(death_times) as death_times,
count(distinct t1.user_id) as death_new_users
from 
(
  select from_unixtime(event_time,'yyyy-MM-dd') as event_date,plat_id,channel_id,user_id,map_id,
  count(distinct log_id) as death_times
  from ods_death_log
  where part_date='2019-10-03'
  group by from_unixtime(event_time,'yyyy-MM-dd'),plat_id,channel_id,user_id,map_id
) t 
left outer join 
(
  select plat_id,user_id,from_unixtime(event_time,'yyyy-MM-dd') as reg_date
  from ods_game_create
  where part_date='2019-10-01'
) t1 on (t1.plat_id=t.plat_id and t1.user_id=t.user_id)
group by t.event_date, t.plat_id,t.channel_id,t.map_id;

```

### 使用shell脚本实现玩家死亡关卡统计

```
cd /home/hadoop/bin/

vim res_user_die_channel.sh
```

```sql
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".res_user_die_channel;

create table if not exists "$DBNAME".res_user_die_channel as 
select t.event_date, t.plat_id,t.channel_id,t.map_id,
count(distinct t.user_id) as death_users, sum(death_times) as death_times,
count(distinct t1.user_id) as death_new_users
from 
(
  select from_unixtime(event_time,'yyyy-MM-dd') as event_date,plat_id,channel_id,user_id,map_id,
  count(distinct log_id) as death_times
  from ods_death_log
  where part_date='2019-10-03'
  group by from_unixtime(event_time,'yyyy-MM-dd'),plat_id,channel_id,user_id,map_id
) t 
left outer join 
(
  select plat_id,user_id,from_unixtime(event_time,'yyyy-MM-dd') as reg_date
  from ods_game_create
  where part_date='2019-10-01'
) t1 on (t1.plat_id=t.plat_id and t1.user_id=t.user_id)
group by t.event_date, t.plat_id,t.channel_id,t.map_id;
"
$hive_home -e "$sql"
```



## 10、玩家分享行为效果分析

玩家分享，就是当玩家讲游戏分享到朋友圈或者群里，这个其他的用户通过被分享的这个地址下载或者进入的游戏，产生的效果。

用户愿意分享这个游戏产品，就说明用户的认可度高，因此我们要统计分享相关的信息，包括分享的用户数，分享的次数，分享带来的新用户，以及这些分享带来的用户的留存率。

### 玩家分享行为效果统计

因此我们一般都会计算每天，每个平台下每个渠道的分享人数、分享次数、分享带来的新玩家数，30天内的分享留存率。

```sql
select t.ddate,t.plat_id,t.channel_id,
share_users, share_times,
case when share_newusers  is null then 0 else share_newusers  end as share_newusers,
case when stay_2  is null then 0 else stay_2  end as stay_2,
case when stay_3  is null then 0 else stay_3  end as stay_3,
case when stay_4  is null then 0 else stay_4  end as stay_4,
case when stay_5  is null then 0 else stay_5  end as stay_5,
case when stay_6  is null then 0 else stay_6  end as stay_6,
case when stay_7  is null then 0 else stay_7  end as stay_7,
case when stay_10 is null then 0 else stay_10 end as stay_10,
case when stay_15 is null then 0 else stay_15 end as stay_15,
case when stay_30 is null then 0 else stay_30 end as stay_30
from 
(
  select from_unixtime(event_time,'yyyy-MM-dd') as ddate,plat_id,channel_id,
  count(distinct user_id) as share_users, count(distinct event_time,log_id) as share_times
  from ods_user_share where part_date>='2019-11-01' and part_date<='2019-11-05'  
  group by from_unixtime(event_time,'yyyy-MM-dd'),plat_id,channel_id
) t 
left outer join 
(
  select t.ddate,t.plat_id,t.channel_id,
  count(distinct t.user_id) as share_newusers,
  count(distinct case when datediff(t1.ddate,t.ddate)=1  then t.user_id end) as stay_2,
  count(distinct case when datediff(t1.ddate,t.ddate)=2  then t.user_id end) as stay_3,
  count(distinct case when datediff(t1.ddate,t.ddate)=3  then t.user_id end) as stay_4,
  count(distinct case when datediff(t1.ddate,t.ddate)=4  then t.user_id end) as stay_5,
  count(distinct case when datediff(t1.ddate,t.ddate)=5  then t.user_id end) as stay_6,
  count(distinct case when datediff(t1.ddate,t.ddate)=6  then t.user_id end) as stay_7,
  count(distinct case when datediff(t1.ddate,t.ddate)=9  then t.user_id end) as stay_10,
  count(distinct case when datediff(t1.ddate,t.ddate)=14 then t.user_id end) as stay_15,
  count(distinct case when datediff(t1.ddate,t.ddate)=29 then t.user_id end) as stay_30
  from 
  (
    select t.plat_id,channel_id,t.user_id,t.ddate
    from 
    (
      select plat_id,channel_id,user_id,from_unixtime(event_time,'yyyy-MM-dd') as ddate
      from ods_game_create
      where part_date>='2019-10-01' and part_date<='2019-10-05'
    ) t 
    join 
    (
      select plat_id, user_id,from_unixtime(event_time,'yyyy-MM-dd') as ddate
      from ods_user_share where part_date>='2019-10-01' and part_date<='2019-10-05' and share_id is not null 
      group by plat_id, user_id,from_unixtime(event_time,'yyyy-MM-dd')
    ) t1 on (t1.plat_id=t.plat_id and t1.ddate=t.ddate and t1.user_id=t.user_id)
  ) t 
  left outer join 
  (
    select user_id,from_unixtime(event_time,'yyyy-MM-dd') as ddate
    from ods_user_login where part_date>='2019-10-01' and part_date<='2019-10-05'
    group by user_id,from_unixtime(event_time,'yyyy-MM-dd')
  ) t1 on (t1.user_id=t.user_id)
  group by t.ddate,t.plat_id,t.channel_id
) t1 on (t.ddate=t1.ddate and t.plat_id=t1.plat_id and t.channel_id=t1.channel_id);

```



### 使用shell脚本来实现玩家分享效果统计

```
cd /home/hadoop/bin/

vim res_user_share.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".res_user_share;

create table if not exists "$DBNAME".res_user_share as 
select t.ddate,t.plat_id,t.channel_id,
share_users, share_times,
case when share_newusers  is null then 0 else share_newusers  end as share_newusers,
case when stay_2  is null then 0 else stay_2  end as stay_2,
case when stay_3  is null then 0 else stay_3  end as stay_3,
case when stay_4  is null then 0 else stay_4  end as stay_4,
case when stay_5  is null then 0 else stay_5  end as stay_5,
case when stay_6  is null then 0 else stay_6  end as stay_6,
case when stay_7  is null then 0 else stay_7  end as stay_7,
case when stay_10 is null then 0 else stay_10 end as stay_10,
case when stay_15 is null then 0 else stay_15 end as stay_15,
case when stay_30 is null then 0 else stay_30 end as stay_30
from 
(
  select from_unixtime(event_time,'yyyy-MM-dd') as ddate,plat_id,channel_id,
  count(distinct user_id) as share_users, count(distinct event_time,log_id) as share_times
  from ods_user_share where part_date>='2019-11-01' and part_date<='2019-11-05'  
  group by from_unixtime(event_time,'yyyy-MM-dd'),plat_id,channel_id
) t 
left outer join 
(
  select t.ddate,t.plat_id,t.channel_id,
  count(distinct t.user_id) as share_newusers,
  count(distinct case when datediff(t1.ddate,t.ddate)=1  then t.user_id end) as stay_2,
  count(distinct case when datediff(t1.ddate,t.ddate)=2  then t.user_id end) as stay_3,
  count(distinct case when datediff(t1.ddate,t.ddate)=3  then t.user_id end) as stay_4,
  count(distinct case when datediff(t1.ddate,t.ddate)=4  then t.user_id end) as stay_5,
  count(distinct case when datediff(t1.ddate,t.ddate)=5  then t.user_id end) as stay_6,
  count(distinct case when datediff(t1.ddate,t.ddate)=6  then t.user_id end) as stay_7,
  count(distinct case when datediff(t1.ddate,t.ddate)=9  then t.user_id end) as stay_10,
  count(distinct case when datediff(t1.ddate,t.ddate)=14 then t.user_id end) as stay_15,
  count(distinct case when datediff(t1.ddate,t.ddate)=29 then t.user_id end) as stay_30
  from 
  (
    select t.plat_id,channel_id,t.user_id,t.ddate
    from 
    (
      select plat_id,channel_id,user_id,from_unixtime(event_time,'yyyy-MM-dd') as ddate
      from ods_game_create
      where part_date>='2019-10-01' and part_date<='2019-10-05'
    ) t 
    join 
    (
      select plat_id, user_id,from_unixtime(event_time,'yyyy-MM-dd') as ddate
      from ods_user_share where part_date>='2019-10-01' and part_date<='2019-10-05' and share_id is not null 
      group by plat_id, user_id,from_unixtime(event_time,'yyyy-MM-dd')
    ) t1 on (t1.plat_id=t.plat_id and t1.ddate=t.ddate and t1.user_id=t.user_id)
  ) t 
  left outer join 
  (
    select user_id,from_unixtime(event_time,'yyyy-MM-dd') as ddate
    from ods_user_login where part_date>='2019-10-01' and part_date<='2019-10-05'
    group by user_id,from_unixtime(event_time,'yyyy-MM-dd')
  ) t1 on (t1.user_id=t.user_id)
  group by t.ddate,t.plat_id,t.channel_id
) t1 on (t.ddate=t1.ddate and t.plat_id=t1.plat_id and t.channel_id=t1.channel_id);

"
$hive_home -e "$sql"

```



## 11、关卡玩家统计

关卡玩家统计主要就是统计每个关卡的玩家人数，以及每个关卡的新用户，这个有利于运营掌握整个游戏的玩家进度。

### 关卡玩家统计

```sql
select t.plat_id,t.channel_id,event_date,level_after,
count(distinct t.user_id) as levelup_users,
count(distinct t1.user_id) as levelup_new_users
from 
(
  select from_unixtime(event_time,'yyyy-MM-dd') as event_date,plat_id,channel_id,user_id,max(level_after) as level_after
  from ods_game_levelup
  where part_date='2019-10-03'
  group by from_unixtime(event_time,'yyyy-MM-dd'),plat_id,channel_id,user_id
) t 
left outer join 
(
  select plat_id,user_id,from_unixtime(event_time,'yyyy-MM-dd') as reg_date
  from ods_game_create
  where part_date='2019-10-03'
) t1 on (t1.plat_id=t.plat_id and t1.user_id=t.user_id)
group by t.plat_id,t.channel_id,event_date,level_after;

```

 

### 使用shell脚本统计关卡玩家

```
cd /home/hadoop/bin/

vim res_user_pass.sh
```

 

```sql
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="
set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;
create database if not exists "$DBNAME";
use "$DBNAME";
drop table if exists "$DBNAME".res_user_pass;

create table if not exists "$DBNAME".res_user_pass as 
select t.plat_id,t.channel_id,event_date,level_after,
count(distinct t.user_id) as levelup_users,
count(distinct t1.user_id) as levelup_new_users
from 
(
  select from_unixtime(event_time,'yyyy-MM-dd') as event_date,plat_id,channel_id,user_id,max(level_after) as level_after
  from ods_game_levelup
  where part_date='2019-10-03'
  group by from_unixtime(event_time,'yyyy-MM-dd'),plat_id,channel_id,user_id
) t 
left outer join 
(
  select plat_id,user_id,from_unixtime(event_time,'yyyy-MM-dd') as reg_date
  from ods_game_create
  where part_date='2019-10-03'
) t1 on (t1.plat_id=t.plat_id and t1.user_id=t.user_id)
group by t.plat_id,t.channel_id,event_date,level_after;
"
$hive_home -e "$sql"

```



## 12、使用shell脚本一键执行所有统计指标

```
cd /home/hadoop/bin

vim execute_res.sh
```

```sql
#!/bin/bash
for m in  res_user_basic_count.sh res_user_pay.sh res_user_stay.sh res_user_lost.sh res_mission.sh res_channel_count.sh res_user_last_operate.sh res_virtual_coin.sh res_user_die_channel.sh res_user_share.sh res_user_pass.sh 
do
  /usr/bin/sh $m
done
```

## 13、数仓当中的缓慢变化维

### 1、什么是缓慢变化维

顾名思义，缓慢变化维度（slowly changing dimension, SCD）就是数据仓库维度表中，那些随时间变化比较不明显，但仍然会发生变化的维度。考虑以下两个情境：

Ø 在员工维度表中，某员工原来在北京分公司工作，后来调往上海分公司，那么“工作地点”就是一个缓慢变化维度；

Ø 在采购维度表中，办公电脑原来从戴尔供应商处进货，后来换成了联想，那么“供应商”就是一个缓慢变化维度。

如果在数据库当中，我们直接对数据进行更细即可，但是在数仓当中，不建议对数据做更新或者插入操作，那么数据仓库当中如何处理缓慢变化维度就是一个比较棘手的问题。

处理缓慢变化维度是Kimball数仓体系中永恒的话题，因为数据仓库的本质，以及维度表在维度建模中的基础作用，我们几乎总是要跟踪维度的变更（change tracking），以保留历史，并提供准确的查询和分析结果。在《The Data Warehouse Toolkit, 3rd Edition》一书的第5章，Kimball提出了多种缓慢变化维度的类型和处理方法，其中前五种是原生的，后面的方法都是混合方法（hybrid techniques），

 

### 2、缓慢变化维度带来的后果

由于维度变化缓慢，且不知道究竟何时可能会发生变化（可能几个月或者几年有可能更新一次），所以我们在设计数据仓库的数据同步的时候，对于一些缓慢变化维度，就比较棘手，因为我们不知道该何时去同步这些变化了的维度

### 3、如何解决维度的缓慢变化

对于缓慢变化维，我们可以有多种解决方法，针对不同场景，我们可以使用不同的解决方法来应对我们的缓慢变化维度，以下为各种对于缓慢变化维度的处理手段

缓慢变化维总共有7中：

变化类型1：改写属性值 

变化类型2：添加维度行  

变化类型3：增加维度列  

变化类型4：添加历史表

变化类型5：混合模式之可预见 的多重变化 

变化类型6：非常规混合模型

附加类型7：混合模式之不可预见的单重变化

 

#### 第一种方式：重写覆盖属性值

当一个维度值的源发生变化 ，并且不需要在星型模式中保留变化历史时，通常采用新数据来覆盖旧数据 ，这个方法有个前提，那就是用户不关心这个数据的变化或者这个数据是错误数据 。这样的处理使属性所反映的总是最新的赋值 。

![image-20200520180103088](数仓项目.assets/image-20200520180103088.png) 

#### 第二种方式：添加维度行

保留事实的历史环境 ，并插入新的维度行 。

例如现在有一张表结构如下：

 ![image-20200520180111291](数仓项目.assets/image-20200520180111291.png)

#### 第三种方式：增加维度列

用不同的字段来保存不同的值，实际上就是在后面添加一个字段，这个字段用 来保存变化后的当前值，而原来 的值则被称为变化前的值，总的来说这种方法通过添加宇段来保存变化后的痕迹 。

 ![image-20200520180120650](数仓项目.assets/image-20200520180120650.png)

#### 第四种方式：添加历史表

另外建一个表来保存历史记录 ，这种方式就是将历史数据与当前数据完 全分开 来，在维度 中只保存当前的数据 。

 ![image-20200520180128773](数仓项目.assets/image-20200520180128773.png)

#### 第五种方式：混合模式之可预见的多重变化

混合模式之可预见的多重变化最经常用于处理营销机构重组 。营销机构按年度重新审视其销售地域分布的情形进行研究 。假设5年的时间内，营销机构进行了5次重组 。查找任意一年的营销情况。

 ![image-20200520180205807](数仓项目.assets/image-20200520180205807.png)

#### 第六种方式：非常规混合模型

给出一个版本号来标识数据是否为 当前存储值，如果是，那么版本号为0；如果 不是，那么版本号为非0。当插入数据的时候 就会对之前的数据版本号进行修改 ，每插入一次，对应的历史记录的版本号就会增加一，这样用户就可以通过版本号来查询指定历史数据。

 ![image-20200520180213999](数仓项目.assets/image-20200520180213999.png)

 

#### 第七种方式：混合模式之不可预见的单重变化

通过类型2、类型3 以及类型1响应 方法组合而成 。它通过类型2来不断增加新的信息，捕获变化内容，然后根据之前的历史信息，再通过类型3响应方法将当前值与历史值加入表 中，跟踪当前分配情形，形成历史 与当前信息相关联 ，而后续的变化情况则按 照类型1响应方法做 出处理。

 ![image-20200520180219573](数仓项目.assets/image-20200520180219573.png)

## 14、数据仓库之元数据管理

元数据（Meta Data），主要记录数据仓库中**模型的定义**、各层级间的**映射关系**、**监控**数据仓库的**数据状态**及 **ETL 的任务运行状态**。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。元数据是数据仓库管理系统的重要组成部分，元数据管理是企业级数据仓库中的关键组件，贯穿了数据仓库的整个生命周期，使用元数据驱动数据仓库的开发，**使数据仓库自动化，可视化**

### 1、元数据类型

元数据可分为**技术元数据、业务元数据和管理过程元数据。**
 1、 技术元数据为开发和管理数据仓库的 IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。主要包含以下作用

Ø 数据仓库结构的描述，包括仓库模式、视图、维、层次结构和导出数据的定义，以及数据集市的位置和内容；

Ø 业务系统、数据仓库和数据集市的体系结构和模式

Ø 汇总用的算法，包括度量和维定义算法，数据粒度、主题领域、聚集、汇总、预定义的查询与报告；

Ø 由操作环境到数据仓库环境的映射，包括源数据和它们的内容、数据分割、数据提取、清理、转换规则和数据刷新规则、安全（用户授权和存取控制）。

 

2、 业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。

Ø 企业概念模型：这是业务元数据所应提供的重要的信息，它表示企业数据模型的高层信息、整个企业的业务概念和相互关系。以这个企业模型为基础，不懂数据库技术和SQL语句的业务人员对数据仓库中的数据也能做到心中有数。

Ø 多维数据模型：这是企业概念模型的重要组成部分，它告诉业务分析人员在数据集市当中有哪些维、维的类别、数据立方体以及数据集市中的聚合规则。这里的数据立方体表示某主题领域业务事实表和维表的多维组织形式。

Ø 业务概念模型和物理数据之间的依赖：以上提到的业务元数据只是表示出了数据的业务视图，这些业务视图与实际的数据仓库或数据库、多维数据库中的表、字段、维、层次等之间的对应关系也应该在元数据知识库中有所体现。

3、 管理过程元数据指描述管理领域相关的概念、关系和规则的数据，主要包括管理流程、人员组织、角色职责等信息。

 

### 2、元数据功能

1、血缘分析：向上追溯元数据对象的数据来源。血缘分析可以帮助您轻松回答：'我正在查看的报告数据来源是什么？'以及'对当前分析的数据应用了哪些转换处理？'等问题。这样的机制及对这些问题的回答确保了对所分析的数据更高的信任水平，并有助于实现许多行业(包括医疗、金融、银行和制造业等)对所呈现数据的特殊监管及合规性要求。

2、影响分析：向下追溯元数据对象对下游的影响。影响分析可以让您轻松应对变更可能产生的影响，自动识别与其相关的依赖项和潜在的影响还可以跟踪所有对象及其依赖关系，最后我们还提供数据全生命周期的可视化显示。例如，如果您的某一信息系统中准备将“销售额”从包含税费更改为不包括税费，则SE-DWA将自动显示所有使用了“销售金额”字段，以便您可以确定有哪些工作需要完成，并且建议您在更改前完成该工作。

3、同步检查：检查源表到目标表的数据结构是否发生变更。

4、指标一致性分析：定期分析指标定义是否和实际情况一致。

5、实体关联查询：事实表与维度表的代理键自动关联

### 3、元数据应用

1、ETL自动化管理：使用元数据信息自动生成物理模型，ETL程序脚本，任务依赖关系和调度程序。

2、数据质量管理：使用数据质量规则元数据进行数据质量测量。数据质量根据设定的规则帮助您过滤出有问题的数据，并智能分析数据质量缺陷。

3、数据安全管理：使用元数据信息进行报表权限控制。可以方便查看用户和访问权限，并启用对象级和行级安全管理。对象级安全性确保通过身份验证的用户只能访问他们被授权查看的数据、表或列，其它数据则不可见。基于行的安全性会更进一步，可以限制特定的组成员只可以访问表中特定的数据。

4、数据标准管理：使用元数据信息生成标准的维度模型。

5、数据接口管理：使用元数据信息进行接口统一管理。多种数据源接入，并提供多种插件对接最流行的源系统。应该可以简单方便获取数据。

6、项目文档管理：使用元数据可以自动、方便的生成的健壮全面的项目文档，其以帮助您应对各种对于数据合规性要求。读取元数据模型，并生成pdf格式的描述文件。生成文档您查看每个对象的名称、设置、描述和代码。

7、数据语义管理：业务用户在自助服务分析中面临的挑战他们不了解数据仓库从而无法正确解释数据，使用元数据可以语义层建模，使用易于业务用户理解的描述来转换数据。

### 4、如何实现元数据管理

由以上几节我们了解到元数据几乎可以被称为是数据仓库乃至商业智能（BI）系统的“灵魂”，正是由于元数据在整个数据仓库生命周期中有着重要的地位，各个厂商的数据仓库解决方案都提到了关于对元数据的管理。但遗憾的是对于元数据的管理，各个解决方案都没有明确提出一个完整的管理模式；它们提供的仅仅是对特定的局部元数据的管理。正是因为对于元数据管理，没有一个统一的标准，所以现在市面上针对元数据的管理也是比较混乱，有的公司自研一些javaWeb的系统，用于元数据管理，有的公司采用第三方的一些BI工具来做元数据管理，有的公司为了省钱也甚至使用一些开源的工具等，例如像Atlas这样的工具等。

**老师梳理笔记：**

```
一个事实，有很多个维度，有的维度是不会变动的，有的维度是经常变动的，还有的维度是缓慢变化的
不变维度：不会变化
变动维度：每个人每次充值都有可能不一样
缓慢变化维度：居住地  有可能在缓慢变化

业务系统当中：
不变维度：不会修改
变动维度：每个人每次充值都有可能不一样  **》每次记录不同的值
缓慢变化维度：居住地  有可能在缓慢变化  **》 变动之后，就进行更新操作

数据仓库当中的数据怎么办：数仓都是T+1的操作
不变维度：不会修改  **》直接从业务系统当中获取即可，获取一次即可
变动维度：每个人每次充值都有可能不一样  **》每次记录不同的值   **》 每天将新增的数据导入一份到数据仓库里面来
缓慢变化维度：居住地  有可能在缓慢变化  **》 变动之后，就进行更新操作   **》 最麻烦的  **》什么时候去导入变更的缓慢变化的维度的数据？？

缓慢变化维？？


存在维度变化了如何通知数据仓库那边来同步数据的问题？？？
涉及到元数据管理

元数据：描述数据的数据

hadoop当中元数据管理
hive当中：元数据管理
数仓当中：元数据管理

数仓当中的元数据管理：技术元数据 + 业务元数据
技术元数据：
	技术人员自己定义的一些关键信息，例如数仓什么时候开始抽取数据，什么时候开始执行数据分析，什么时候去进行数据清洗等等
	抽取数据的字段定义
	可以做到让我们数仓的自动化运行
	
业务元数据：
	业务人员需要定义的一些规则，例如数据如何进行清洗，哪些不满足条件的数据进行清洗
	数据清洗之后，数据该怎么处理
	可以做到，提需求 不写代码 减少sql语句的开发

	
元数据可以做什么：可以做数据的血缘追溯  **》每一条数据从哪里来，到哪里去，中间经过了哪些步骤等等
原来数据库当中有100W条数据，为什么到了你们数仓里面就只剩下了50W条？
什么原因导致哪些数据被干掉了？？？
	

元数据的管理工具：有一些专业的付费的工具，也有一些apache免费开源的Atlas框架
```



## :rainbow:游戏数据集市层体系

## 1、 数据集市层概念

**老师笔记——数据集市与dw层的却别：**

```
集市层主要就是为了各个业务线提供数据支持
集市层上方可以做数据报表展示，可以做用户画像，可以支持数据提取

dw层：主要就是各种宽表   **》 有各种维度，但是没有针对某一些具体的维度去分析
res层：集市层  主要的作用这里就是为了做数据报表展示  按照业务线进行划分，构建不同的集市层
大仓库  小集市

集市层：更多的关注更细粒度的指标的求取

以后说各种维度：其实就是对应sql当中的group by操作
```

### 1.1 数据集市概念

数据集市(Data Market) ，也叫数据市场，数据集市就是满足特定的部门或者用户的需求，按照多维的方式进行存储，包括定义维度、需要计算的指标、维度的层次等，生成面向决策分析需求的数据立方体。

**在我们的数据仓库里面，就是结果层的数据**，通过**sqoop**等数据同步工具推向业务数据库的那一层，作为我们的数据集市。**专门在mysql中建立一个数据库，这个数据库就是专门存放我们大数据平台计算之后的结果数据，**然后让各个部门的PHP、前端、web调用。

![数据集市](数仓项目.assets/clip_image002-1589972438366.png)

### 1.2 数据集市的建设

数据部门对这个数据库有增删改查的权限，因为数据部门对数据要负责，如果数据计算错误，那就要删掉数据重新推送过去，PHP和前端只做查询和聚合，不做业务处理。所以数据集市里面的数据一定要配合各个业务部门的需求，**直接针对需求的界面来设计mysql的表结构。**

比如用户留存：

这里要显示的次留、7日留存、30日留存，然后以日期作为维度和查询条件，那么我们在设计这个mysql表的时候，就至少要有日期、次留、7日留存、30日留存这4个字段。

但是作为数据集市的功能，我们要考虑到业务的扩展性，因为可能对商务人员来说，只需要这几个简单的指标看看就行了，不用那么复杂，因为商务的侧重点是在对渠道合作的接洽上，是跟外部公司交换和对接数据的，他们也不能有太过于真实和复杂的数据。

而对运营来说，在考虑指标体系的时候会看更细的指标，因为他们要考虑的就是整个游戏的运作情况，如何来提高游戏的活跃，他们就更为关心留存的**细度**。

而商务、市场、运营的数据都由数据仓库的数据集市统一提供，他们都在这里取数据。所以如果我们的集市表设计的过于简单，那我们就不能同时满足他们的需求，我们**要定义更多的表结构，和计算任务，因此我们要扩展我们的基础指标细度**，由PHP去控制不同岗位的人员对这个数据系统的数据查看权限。

### 1.3、使用sql语句实现统计指标

因此我们要将我们的集市表扩展为：注册用户数、次留、3留、4留、5留、6留、7留、15留、月留。如果还有其他业务情况也可以在次扩展，重点是在满足业务。

hive当中创建表

```sql
use game_center;
CREATE TABLE res_user_stay(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  reg_users    int  COMMENT '注册人数',
  stay_s       int  COMMENT '次留人数',
  stay_3_s     int  COMMENT '3留人数',
  stay_4_s     int  COMMENT '4留人数',
  stay_5_s     int  COMMENT '5留人数',
  stay_6_s     int  COMMENT '6留人数',
  stay_7_s     int  COMMENT '7留人数',
  stay_15_s    int  COMMENT '15留人数',
  stay_30_s    int  COMMENT '30留人数'
)
comment '用户留存集市表'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

向hive表当中插入数据

```sql
insert overwrite table res_user_stay
select plat_id,channel_id, reg_date as event_date,
count(distinct user_id) as reg_users,
count(distinct case when datediff(login_date,reg_date)=1 then user_id end) as stay_s,
count(distinct case when datediff(login_date,reg_date)=2 then user_id end) as stay_3_s,
count(distinct case when datediff(login_date,reg_date)=3 then user_id end) as stay_4_s,
count(distinct case when datediff(login_date,reg_date)=4 then user_id end) as stay_5_s,
count(distinct case when datediff(login_date,reg_date)=5 then user_id end) as stay_6_s,
count(distinct case when datediff(login_date,reg_date)=6 then user_id end) as stay_7_s,
count(distinct case when datediff(login_date,reg_date)=14 then user_id end) as stay_15_s,
count(distinct case when datediff(login_date,reg_date)=29 then user_id end) as stay_30_s
from tmp_user_stay 
group by plat_id,channel_id,reg_date;
```

 

### 1.4、通过shell脚本实现指标统计

node03执行以下命令创建shell脚本

```
cd /home/hadoop/bin/

vim res_user_stay.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".res_user_stay;

CREATE TABLE if not exists  "$DBNAME".res_user_stay(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  reg_users    int  COMMENT '注册人数',
  stay_s       int  COMMENT '次留人数',
  stay_3_s     int  COMMENT '3留人数',
  stay_4_s     int  COMMENT '4留人数',
  stay_5_s     int  COMMENT '5留人数',
  stay_6_s     int  COMMENT '6留人数',
  stay_7_s     int  COMMENT '7留人数',
  stay_15_s    int  COMMENT '15留人数',
  stay_30_s    int  COMMENT '30留人数'
)
comment '用户留存集市表'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table res_user_stay
select plat_id,channel_id, reg_date as event_date,
count(distinct user_id) as reg_users,
count(distinct case when datediff(login_date,reg_date)=1 then user_id end) as stay_s,
count(distinct case when datediff(login_date,reg_date)=2 then user_id end) as stay_3_s,
count(distinct case when datediff(login_date,reg_date)=3 then user_id end) as stay_4_s,
count(distinct case when datediff(login_date,reg_date)=4 then user_id end) as stay_5_s,
count(distinct case when datediff(login_date,reg_date)=5 then user_id end) as stay_6_s,
count(distinct case when datediff(login_date,reg_date)=6 then user_id end) as stay_7_s,
count(distinct case when datediff(login_date,reg_date)=14 then user_id end) as stay_15_s,
count(distinct case when datediff(login_date,reg_date)=29 then user_id end) as stay_30_s
from tmp_user_stay 
group by plat_id,channel_id,reg_date;
"
$hive_home -e "$sql"
```

## 2、游戏运营业务数据支撑体系

### 2.1 新增玩家

游戏运营主要关注的是游戏内的行为一切行为，**每日的新增玩家的走势**真正的体现一款游戏在市场上的热度和持续度。

![图3](数仓项目.assets/clip_image002-1589973691401.png)

玩家的账户类型，就是玩家的注册来源，根据注册来源可以反映游戏每个推广渠道的效果，因此运营要重点关注。为此，我们要每日计算每个渠道的注册人数，以及注册的设备数。

创建hive表

```sql
CREATE TABLE res_new_users(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  reg_users    int  COMMENT '注册人数',
  reg_ips      int  COMMENT '注册ip数'
)
comment '新增玩家'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
向hive表当中插入数据
insert overwrite table res_new_users
select plat_id,channel_id, reg_date as event_date,
count(distinct user_id) as reg_users,count(distinct client_ip) as reg_ips
from dw_user_basic_info 
group by plat_id,channel_id,reg_date;
```

### 2.2 活跃玩家

活跃玩家就是**每日的登录用户数，也称之为DAU**，即当日有开启过游戏的玩家数，其中当日新增玩家带来的活跃。分为老玩家和新玩家，新玩家就是当日注册并且登录的玩家，老玩家就是非登录当日注册的玩家。

创建hive表

```sql
CREATE TABLE res_active_users(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  new_users    int  COMMENT '新玩家',
  old_users    int  COMMENT '老玩家',
  all_users    int  COMMENT '总计'
)
comment '活跃玩家'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

向hive表当中插入数据

```sql
insert overwrite table res_active_users
select plat_id,channel_id,part_date as event_date,
count(distinct case when reg_date=login_date then user_id end) as new_users,
count(distinct case when reg_date<login_date then user_id end) as old_users,
count(distinct user_id) as all_users
from dw_user_behavior_info 
group by plat_id,channel_id,part_date;
```



### 2.3 游戏习惯

游戏习惯统计，当日所选玩家平均进行游戏的时长（所选玩家当日游戏总时长/当日所选玩家数）和平均每玩家的游戏次数（所选玩家当日游戏次数/每日所选玩家数）。

![img](数仓项目.assets/clip_image010-1589973691401.jpg)

图7

![img](数仓项目.assets/clip_image012-1589973691401.jpg)

图8

创建hive表

```sql
CREATE TABLE res_play_habit(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  online_times    int  COMMENT '在线时长',
  login_times    int  COMMENT '登录次数即游戏次数'
)
comment '游戏习惯'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

向hive表当中插入数据

```sql
insert overwrite table res_play_habit
select plat_id,channel_id,part_date as event_date,
sum(online_times) as online_times,
sum(login_times) as login_times
from dw_user_behavior_info 
group by plat_id,channel_id,part_date;
```

### 2.4 等级分析

#### 2.4.1 等级详解

分析每日所有等级的升级在线时长、付费次数、付费付费金额。判断等级对用户付费的影响，借此来设定游戏的等级设置和升级难度是否合理。

![图9](数仓项目.assets/clip_image014.png)

图9

创建hive表

```sql
CREATE TABLE res_level_info(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  role_level_after    int COMMENT '等级',
  online_times    int  COMMENT '等级在线时长',
  pay_times   int  COMMENT '充值次数',
  recharge_amount   double   COMMENT '充值金额'
)
comment '游戏习惯'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

向hive表当中插入数据

```sql
insert overwrite table res_level_info
select plat_id,channel_id,part_date as event_date,role_level_after,
sum(online_times) as online_times,
sum(pay_times) as pay_times,
sum(recharge_amount) as recharge_amount
from dw_user_behavior_info 
group by plat_id,channel_id,part_date,role_level_after;
```

#### 2.4.2 等级分布

等级分布，就是计算每个等级的活跃用户数，也就是登录人数。

![图10](数仓项目.assets/clip_image016.png)

图10

![图11](数仓项目.assets/clip_image018-1589973691402.png)

图11

创建hive表

```sql
CREATE TABLE res_level_users(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  role_level_after    int COMMENT '等级',
  users    int  COMMENT '人数'
)
comment '等级分布'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

插入数据

```sql
insert overwrite table res_level_users
select plat_id,channel_id,part_date as event_date,role_level_after,
count(distinct user_id) as users
from dw_user_behavior_info 
group by plat_id,channel_id,part_date,role_level_after;
```

 

#### 2.4.3 新玩家进度

新玩家进度是要查看新注册的玩家的等级情况，这个能反应游戏对玩家的难度是否合适，如果进度过快，那就难度过低，容易玩腻，难度过高，就容易产生挫败感，所以控制这个进度的节奏很重要。

![图12](数仓项目.assets/clip_image020.png)

图12

![图13](数仓项目.assets/clip_image022.png)

图13

 这个的表结构是跟等级分布是一样的，只不过不同的是这个新玩家进度只针对新注册的玩家统计，因此在计算等级的时候要加上注册时间。这里直接从上面等级分布的代码复用，**加个注册时间和登录时间相等的条件即可。**

 **所以说一个完善的中间层，对数据仓库太重要了。**

创建hive表

```sql
CREATE TABLE res_newuser_level(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  role_level_after    int COMMENT '等级',
  users    int  COMMENT '人数'
)
comment '等级分布'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

向hive表当中插入数据

```sql
insert overwrite table res_newuser_level
select plat_id,channel_id,part_date as event_date,role_level_after,
count(distinct user_id) as users
from dw_user_behavior_info 
where reg_date=login_date
group by plat_id,channel_id,part_date,role_level_after;
```

### 2.5 设备分析

设备分析就是分析每种手机型号和品牌的注册玩家。

![图24](数仓项目.assets/clip_image024-1589973691402.png)

图24

![图25](数仓项目.assets/clip_image026-1589973691402.png)

图25

这个数据可以直接从dw_user_basic_info表出，这里能统计到所有需要的信息。

创建hive表

```sql
CREATE TABLE res_device_info(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  device_brand   string COMMENT '设备品牌',
  device_type    string COMMENT '设备型号',
  users    int  COMMENT '人数'
)
comment '等级分布'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

向hive表当中插入数据

```sql
insert overwrite table res_device_info
select plat_id,channel_id,reg_date as event_date,device_brand,device_type,
count(distinct user_id) as users
from dw_user_basic_info 
group by plat_id,channel_id,reg_date,device_brand,device_type;
```

### 2.6、使用shell脚本统计以上所有指标

node03执行以下命令来创建shell脚本

```
cd /home/hadoop/bin/

vim res_user_business.sh
```

```sql
#!/bin/bash

DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".res_new_users;

create table if not exists   "$DBNAME".res_new_users(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  reg_users    int  COMMENT '注册人数',
  reg_ips      int  COMMENT '注册ip数'
)
comment '新增玩家'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_new_users
select plat_id,channel_id, reg_date as event_date,
count(distinct user_id) as reg_users,count(distinct client_ip) as reg_ips
from dw_user_basic_info 
group by plat_id,channel_id,reg_date;


drop table if exists "$DBNAME".res_active_users;

create table if not exists   "$DBNAME".res_active_users(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  new_users    int  COMMENT '新玩家',
  old_users    int  COMMENT '老玩家',
  all_users    int  COMMENT '总计'
)
comment '活跃玩家'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


insert overwrite table "$DBNAME".res_active_users
select plat_id,channel_id,part_date as event_date,
count(distinct case when reg_date=login_date then user_id end) as new_users,
count(distinct case when reg_date<login_date then user_id end) as old_users,
count(distinct user_id) as all_users
from dw_user_behavior_info 
group by plat_id,channel_id,part_date;

drop table if exists "$DBNAME".res_play_habit;

create table if not exists   "$DBNAME".res_play_habit(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  online_times    int  COMMENT '在线时长',
  login_times    int  COMMENT '登录次数即游戏次数'
)
comment '游戏习惯'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_play_habit
select plat_id,channel_id,part_date as event_date,
sum(online_times) as online_times,
sum(login_times) as login_times
from dw_user_behavior_info 
group by plat_id,channel_id,part_date;

drop table if exists "$DBNAME".res_level_info;

create table if not exists   "$DBNAME".res_level_info(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  role_level_after    int COMMENT '等级',
  online_times    int  COMMENT '等级在线时长',
  pay_times   int  COMMENT '充值次数',
  recharge_amount   double   COMMENT '充值金额'
)
comment '游戏习惯'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_level_info
select plat_id,channel_id,part_date as event_date,role_level_after,
sum(online_times) as online_times,
sum(pay_times) as pay_times,
sum(recharge_amount) as recharge_amount
from dw_user_behavior_info 
group by plat_id,channel_id,part_date,role_level_after;


drop table if exists "$DBNAME".res_level_users;

create table if not exists   "$DBNAME".res_level_users(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  role_level_after    int COMMENT '等级',
  users    int  COMMENT '人数'
)
comment '等级分布'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_level_users
select plat_id,channel_id,part_date as event_date,role_level_after,
count(distinct user_id) as users
from dw_user_behavior_info 
group by plat_id,channel_id,part_date,role_level_after;

drop table if exists "$DBNAME".res_newuser_level;

create table if not exists   "$DBNAME".res_newuser_level(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  role_level_after    int COMMENT '等级',
  users    int  COMMENT '人数'
)
comment '等级分布'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_newuser_level
select plat_id,channel_id,part_date as event_date,role_level_after,
count(distinct user_id) as users
from dw_user_behavior_info 
where reg_date=login_date
group by plat_id,channel_id,part_date,role_level_after;

drop table if exists "$DBNAME".res_device_info;

create table if not exists   "$DBNAME".res_device_info(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  device_brand   string COMMENT '设备品牌',
  device_type    string COMMENT '设备型号',
  users    int  COMMENT '人数'
)
comment '等级分布'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_device_info
select plat_id,channel_id,reg_date as event_date,device_brand,device_type,
count(distinct user_id) as users
from dw_user_basic_info 
group by plat_id,channel_id,reg_date,device_brand,device_type;



"
$hive_home -e "$sql"
```

## 3、游戏市场业务数据支撑体系

### 3.1 付费转化

付费转化是统计每日的新增付费玩家和累计的付费玩家，累计的付费玩家就是每天都在计算历史所有的付费玩家，这个需要用到开窗函数，累计求和。

![img](数仓项目.assets/clip_image002.jpg)

图14

![图15](数仓项目.assets/clip_image004-1589975572673.png)

图15

创建hive表

```sql
CREATE TABLE res_pay_trans(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  new_pay_users    int COMMENT '新增付费玩家',
  all_pay_users    int  COMMENT '总付费玩家'
)
comment '付费转化'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

插入数据

```sql
insert overwrite table res_pay_trans
select plat_id,channel_id,event_date,new_pay_users,
SUM(new_pay_users) OVER(PARTITION BY plat_id,channel_id ORDER BY event_date) AS all_pay_users
from 
(
  select plat_id,channel_id,part_date as event_date,count(distinct user_id) as new_pay_users
  from dw_user_behavior_info 
  where recharge_amount>0
  group by plat_id,channel_id,part_date
) t ;
```

sum over的使用：

```
sum  over 

SUM(new_pay_users) OVER(PARTITION BY plat_id,channel_id ORDER BY event_date)
按照plat_id,channel_id 这几个条件进行分组，分组之后，求和
然后按照event_date进行排序ORDER BY event_date 



类似于
rank  over
dense_rank  over
row_number  over
```

 这里有个开窗函数，SUM(new_pay_users) OVER(PARTITION BY plat_id,channel_id ORDER BY event_date)

这个的功能就是根据 plat_id,channel_id 分组，按照event_date日期排序，将new_pay_users每个event_dat后面的值都加起来。

达到下面的效果：

| plat_id | channel_id | event_date | new_pay_users | all_pay_users |
| :-----: | :--------: | :--------: | :-----------: | :-----------: |
|    1    |     1      | 2019-10-10 |       1       |       1       |
|    1    |     1      | 2019-10-11 |       5       |       6       |
|    1    |     1      | 2019-10-12 |       7       |      13       |
|    1    |     1      | 2019-10-13 |       3       |      16       |
|    1    |     1      | 2019-10-14 |       2       |      18       |
|    1    |     1      | 2019-10-15 |       4       |      22       |
|    1    |     1      | 2019-10-16 |       4       |      26       |

### 3.2 收入分析

收入分析主要就是计算玩家的充值次数、充值人数、充值金额

![图16](数仓项目.assets/clip_image006-1589975572674.png)

图16

![图17](数仓项目.assets/clip_image008-1589975572674.png)

图17

创建hive表

```sql
CREATE TABLE res_pay_info(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  pay_money    double COMMENT '充值金额',
  pay_users    int  COMMENT '充值人数',
  pay_times    int  COMMENT '充值次数'
)
comment '收入分析'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

插入数据

```sql
insert overwrite table res_pay_info
select plat_id,channel_id,part_date as event_date,
sum(recharge_amount) as pay_money,
count(distinct user_id) as pay_users,
sum(pay_times) as pay_times
from dw_user_behavior_info 
where recharge_amount>0
group by plat_id,channel_id,part_date;
```

### 3.3 付费习惯

付费习惯是判断**充值次数**的用户人数，这个计算我们要先**将次数区间分好**，然后在按照每个次数区间去分组统计相应的人数。

![图18](数仓项目.assets/clip_image010.png)

图18

![图19](数仓项目.assets/clip_image012.png)

图19

创建hive表

```sql
CREATE TABLE res_pay_habit(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  pay_times_id int COMMENT '充值次数id,用于区间排序',
  pay_times_area    string  COMMENT '充值次数',
  pay_users    int  COMMENT '充值人数'
)
comment '付费习惯'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

插入数据

```sql
insert overwrite table res_pay_habit
select plat_id,channel_id, event_date,pay_times_id,pay_times_area,
count(1) as pay_users
from 
(
  select plat_id,channel_id, event_date,user_id,
  case 
  when pay_times=0 or pay_times is null then 0
  when pay_times=1 then  1
  when pay_times=2 then  2
  when pay_times=3 then  3
  when pay_times=4 then  4
  when pay_times=5 then  5
  when pay_times>=6 and pay_times<=10 then 6
  when pay_times>=11 and pay_times<=20 then 7
  when pay_times>=21 and pay_times<=30 then 8
  when pay_times>=31 and pay_times<=40 then 9
  else 10 end as pay_times_id,
  case 
  when pay_times=0 or pay_times is null then '未付费'
  when pay_times=1 then  '1次'
  when pay_times=2 then  '2次'
  when pay_times=3 then  '3次'
  when pay_times=4 then  '4次'
  when pay_times=5 then  '5次'
  when pay_times>=6 and pay_times<=10 then '6~10次'
  when pay_times>=11 and pay_times<=20 then '11~20次'
  when pay_times>=21 and pay_times<=30 then '21~30次'
  when pay_times>=31 and pay_times<=40 then '31~40次'
  else '40次以上' end as pay_times_area
  from 
  (
    select plat_id,channel_id,part_date as event_date,user_id,
    sum(pay_times) as pay_times
    from dw_user_behavior_info 
    where recharge_amount>0
    group by plat_id,channel_id,part_date,user_id
  ) t 
) t 
group by plat_id,channel_id, event_date,pay_times_id,pay_times_area;
```

 

### 3.4 付费渗透

付费渗透就是要看当日的活跃玩家中，有多少玩家在充钱的情况，并计算一个他们之间的比例，所以这里就是要统计每日的活跃玩家和付费玩家。

![图20](数仓项目.assets/clip_image014-1589975572674.png)

图20

![图21](数仓项目.assets/clip_image016-1589975572674.png)

图21

创建hive表

```sql
CREATE TABLE res_pay_seep(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  active_user  int  COMMENT '活跃人数',
  pay_users    int  COMMENT '付费人数'
)
comment '等级分布'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

插入数据

```sql
insert overwrite table res_pay_seep
select plat_id,channel_id,part_date as event_date,
count(distinct user_id) as active_user,
count(distinct case when recharge_amount>0 then user_id end) as active_user
from dw_user_behavior_info 
group by plat_id,channel_id,part_date;
```

### 3.5 新玩家价值 

新玩家价值，就是要看当日注册的玩家在接下来的7天的充值，以及14天、30天、60天、90天内的充值。

![图22](数仓项目.assets/clip_image018-1589975572674.png)

图22

![图23](数仓项目.assets/clip_image020-1589975572674.png)

图23

这个计算就要稍微复杂一点，需要用用户基础属性表去关联充值表，要仅以用户注册时间为维度，用户在充值中有多条记录，因此在关联时会产生笛卡尔积的情况，但是我们有中间层行为整合层，这一层就只需要计算一个表就行，以注册时间为总维度就可以。

创建hive表

```sql
CREATE TABLE res_newuser_value(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  pay_money7   double   COMMENT '7日贡献',
  pay_money14  double   COMMENT '14日贡献',
  pay_money30  double   COMMENT '30日贡献',
  pay_money60  double   COMMENT '60日贡献',
  pay_money90  double   COMMENT '90日贡献'
)
comment '新玩家价值'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

插入数据

```sql
insert overwrite table res_newuser_value
select plat_id,channel_id,reg_date as event_date,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=7 then recharge_amount else 0 end) as pay_money7,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=14 then recharge_amount else 0 end) as pay_money14,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=30 then recharge_amount else 0 end) as pay_money30,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=60 then recharge_amount else 0 end) as pay_money60,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=90 then recharge_amount else 0 end) as pay_money90
from dw_user_behavior_info 
group by plat_id,channel_id,reg_date;
```



### 3.6、使用shell脚本实现以上统计分析

```
cd /home/hadoop/bin/

vim res_user_pay_hobby.sh
```

```sql
#!/bin/bash
DBNAME=game_center
hive_home=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive
## 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   part_date=$1
else 
   part_date=`date -d "-1 day" +%F`
fi 
echo "**=日志日期为 $part_date**="

sql="

set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=262144;
set hive.exec.mode.local.auto.input.files.max=5;

create database if not exists "$DBNAME";
use "$DBNAME";

drop table if exists "$DBNAME".res_pay_trans;

create table if not exists "$DBNAME".res_pay_trans(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  new_pay_users    int COMMENT '新增付费玩家',
  all_pay_users    int  COMMENT '总付费玩家'
)
comment '付费转化'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_pay_trans
select plat_id,channel_id,event_date,new_pay_users,
SUM(new_pay_users) OVER(PARTITION BY plat_id,channel_id ORDER BY event_date) AS all_pay_users
from 
(
  select plat_id,channel_id,part_date as event_date,count(distinct user_id) as new_pay_users
  from dw_user_behavior_info 
  where recharge_amount>0
  group by plat_id,channel_id,part_date
) t ;


drop table if exists "$DBNAME".res_pay_info;

create table if not exists "$DBNAME".res_pay_info(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  pay_money    double COMMENT '充值金额',
  pay_users    int  COMMENT '充值人数',
  pay_times    int  COMMENT '充值次数'
)
comment '收入分析'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


insert overwrite table "$DBNAME".res_pay_info
select plat_id,channel_id,part_date as event_date,
sum(recharge_amount) as pay_money,
count(distinct user_id) as pay_users,
sum(pay_times) as pay_times
from dw_user_behavior_info 
where recharge_amount>0
group by plat_id,channel_id,part_date;

drop table if exists "$DBNAME".res_pay_habit;

create table if not exists "$DBNAME".res_pay_habit(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  pay_times_id int COMMENT '充值次数id,用于区间排序',
  pay_times_area    string  COMMENT '充值次数',
  pay_users    int  COMMENT '充值人数'
)
comment '付费习惯'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;


insert overwrite table "$DBNAME".res_pay_habit
select plat_id,channel_id, event_date,pay_times_id,pay_times_area,
count(1) as pay_users
from 
(
  select plat_id,channel_id, event_date,user_id,
  case 
  when pay_times=0 or pay_times is null then 0
  when pay_times=1 then  1
  when pay_times=2 then  2
  when pay_times=3 then  3
  when pay_times=4 then  4
  when pay_times=5 then  5
  when pay_times>=6 and pay_times<=10 then 6
  when pay_times>=11 and pay_times<=20 then 7
  when pay_times>=21 and pay_times<=30 then 8
  when pay_times>=31 and pay_times<=40 then 9
  else 10 end as pay_times_id,
  case 
  when pay_times=0 or pay_times is null then '未付费'
  when pay_times=1 then  '1次'
  when pay_times=2 then  '2次'
  when pay_times=3 then  '3次'
  when pay_times=4 then  '4次'
  when pay_times=5 then  '5次'
  when pay_times>=6 and pay_times<=10 then '6~10次'
  when pay_times>=11 and pay_times<=20 then '11~20次'
  when pay_times>=21 and pay_times<=30 then '21~30次'
  when pay_times>=31 and pay_times<=40 then '31~40次'
  else '40次以上' end as pay_times_area
  from 
  (
    select plat_id,channel_id,part_date as event_date,user_id,
    sum(pay_times) as pay_times
    from dw_user_behavior_info 
    where recharge_amount>0
    group by plat_id,channel_id,part_date,user_id
  ) t 
) t 
group by plat_id,channel_id, event_date,pay_times_id,pay_times_area;

drop table if exists "$DBNAME".res_pay_seep;

create table if not exists "$DBNAME".res_pay_seep(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  active_user  int  COMMENT '活跃人数',
  pay_users    int  COMMENT '付费人数'
)
comment '等级分布'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_pay_seep
select plat_id,channel_id,part_date as event_date,
count(distinct user_id) as active_user,
count(distinct case when recharge_amount>0 then user_id end) as active_user
from dw_user_behavior_info 
group by plat_id,channel_id,part_date;

drop table if exists "$DBNAME".res_newuser_value;

create table if not exists "$DBNAME".res_newuser_value(
  plat_id      int  COMMENT '平台ID',
  channel_id   int  COMMENT '渠道id',
  event_date   date COMMENT '时间',
  pay_money7   double   COMMENT '7日贡献',
  pay_money14  double   COMMENT '14日贡献',
  pay_money30  double   COMMENT '30日贡献',
  pay_money60  double   COMMENT '60日贡献',
  pay_money90  double   COMMENT '90日贡献'
)
comment '新玩家价值'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

insert overwrite table "$DBNAME".res_newuser_value
select plat_id,channel_id,reg_date as event_date,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=7 then recharge_amount else 0 end) as pay_money7,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=14 then recharge_amount else 0 end) as pay_money14,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=30 then recharge_amount else 0 end) as pay_money30,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=60 then recharge_amount else 0 end) as pay_money60,
sum(case when recharge_amount>0 and datediff(part_date,reg_date)<=90 then recharge_amount else 0 end) as pay_money90
from dw_user_behavior_info 
group by plat_id,channel_id,reg_date;

"
$hive_home -e "$sql"
```



## 4、使用shell脚本执行以上所有任务

创建shell脚本一键执行以上所有任务

```
cd /home/hadoop/bin/

vim execute_res2.sh
```

```sql
#!/bin/bash
for m in  res_user_business.sh res_user_pay_hobby.sh                                                                                           
do
  /usr/bin/sh $m
done
```

## 5、数据导出sqoop任务

前面我们已经计算出来ods层各种指标，接下来我们可以将我们计算的结果导出到mysql里面去

### 1、 mysql当中创建数据库以及数据库表

```sql
/*
SQLyog Ultimate v12.09 (64 bit)
MySQL - 5.7.27 : Database - game_center
*********************************************************************
*/

/*!40101 SET NAMES utf8 */;

/*!40101 SET SQL_MODE=''*/;

/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;
CREATE DATABASE /*!32312 IF NOT EXISTS*/`game_center` /*!40100 DEFAULT CHARACTER SET utf8 */;

USE `game_center`;

/*Table structure for table `res_active_users` */

DROP TABLE IF EXISTS `res_active_users`;

CREATE TABLE `res_active_users` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `plat_id` varchar(16) DEFAULT NULL,
  `channel_id` varchar(16) DEFAULT NULL,
  `event_date` varchar(16) DEFAULT NULL,
  `new_users` varchar(16) DEFAULT NULL,
  `old_users` varchar(16) DEFAULT NULL,
  `all_users` varchar(16) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=64 DEFAULT CHARSET=utf8;

/*Table structure for table `res_channel_num` */

DROP TABLE IF EXISTS `res_channel_num`;

CREATE TABLE `res_channel_num` (
  `login_date` varchar(64) DEFAULT NULL,
  `channel_id` varchar(64) DEFAULT NULL,
  `device_num` varchar(64) DEFAULT NULL,
  `reg_user` varchar(64) DEFAULT NULL,
  `active_users` varchar(64) DEFAULT NULL,
  `play_once_users` varchar(64) DEFAULT NULL,
  `pay_users` varchar(64) DEFAULT NULL,
  `pay_user` varchar(64) DEFAULT NULL,
  `pay_money` varchar(64) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*Table structure for table `res_channel_num_temp` */

DROP TABLE IF EXISTS `res_channel_num_temp`;

CREATE TABLE `res_channel_num_temp` (
  `login_date` varchar(64) DEFAULT NULL,
  `channel_id` varchar(64) DEFAULT NULL,
  `device_num` varchar(64) DEFAULT NULL,
  `reg_user` varchar(64) DEFAULT NULL,
  `active_users` varchar(64) DEFAULT NULL,
  `play_once_users` varchar(64) DEFAULT NULL,
  `pay_users` varchar(64) DEFAULT NULL,
  `pay_user` varchar(64) DEFAULT NULL,
  `pay_money` varchar(64) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;
```

 

### 2、sqoop任务导出

将我们计算的结果，使用sqoop将数据导出到mysql当中去,我们这里以`res_active_users` `res_basic_count` `res_channel_num` `res_channel_stay` `res_consumer_hobby` `res_consumer_point` 这几张表作为导出示例，进行数据导出即可

#### 1、 导出res_active_users

**面试题：如果mysql表有id为自增主键，如何实现导出?**

解决办法：使用--update-key指定mysql的自增主键名称，使用--columns指定从hive表导入的几个字段分别对应mysql的哪几个字段。

将我们的活跃用户数据导出到mysql表，其中mysql的表的id为自增主键id，使用sqoop语句导出如下：

```sh
cd /kkb/install/sqoop-1.4.6-cdh5.14.2

bin/sqoop export --connect jdbc:mysql://node03:3306/game_center \
 --username root --password 123456 \
 --table res_active_users --export-dir /user/hive/warehouse/game_center.db/res_active_users  \
 --update-key "id" --columns "plat_id,channel_id,event_date,new_users,old_users,all_users" \
 --update-mode allowinsert -m 1 --input-null-string '\\N' \
 --input-null-non-string '\\N' --fields-terminated-by '\t'  
```

#### 2、导出res_channel_num基本统计数据

**面试题：如何实现导出数据的事务性**

场景1：如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。

**由于Sqoop将导出过程分解为多个事务，因此失败的导出作业可能会导致将部分数据提交到数据库。** 这可能进一步导致后续作业由于某些情况下的插入冲突而失败，或导致其他作业中的重复数据。 您可以通过****–staging-table****选项指定登台表来解决此问题，该选项充当用于**暂存导出数据的辅助表**。 分阶段数据最终在单个事务中移动到目标表。

示意图如下：B表和A表的结构完全一样，数据先导入B表中，如果，导入出错，则B表的数据会被清空，如果导入不出错，B表的数据会进一步导入A表中，然后再清空B表数据。

<img src="数仓项目.assets/image-20200520230313666.png" alt="image-20200520230313666" style="zoom: 67%;" />

```sh
cd /kkb/install/sqoop-1.4.6-cdh5.14.2

bin/sqoop export --connect jdbc:mysql://node03:3306/game_center \
--username root --password 123456 \
--table res_channel_num --export-dir /user/hive/warehouse/game_center.db/res_channel_num  \
--columns "login_date,channel_id,device_num,reg_user,active_users,play_once_users,pay_users,pay_user,pay_money" \
--update-mode allowinsert -m 1 \
--staging-table res_channel_num_temp --clear-staging-table \
--input-null-string '\\N' \
--input-null-non-string '\\N' --fields-terminated-by '\001'  
```

#### 3、使用shell脚本实现数据导出

node03执行以下命令创建shell脚本

```
cd /home/hadoop/bin/

vim export_res.sh
```

```sh
#!/bin/bash

/kkb/install/sqoop-1.4.6-cdh5.14.2/bin/sqoop export --connect jdbc:mysql://node03:3306/game_center \
 --username root --password 123456 \
 --table res_active_users --export-dir /user/hive/warehouse/game_center.db/res_active_users  \
 --update-key "id" --columns "plat_id,channel_id,event_date,new_users,old_users,all_users" \
 --update-mode allowinsert -m 1 --input-null-string '\\N' \
 --input-null-non-string '\\N' --fields-terminated-by '\t' 
 

/kkb/install/sqoop-1.4.6-cdh5.14.2/bin/sqoop export --connect jdbc:mysql://node03:3306/game_center \
--username root --password 123456 \
--table res_channel_num --export-dir /user/hive/warehouse/game_center.db/res_channel_num  \
--columns "login_date,channel_id,device_num,reg_user,active_users,play_once_users,pay_users,pay_user,pay_money" \
--update-mode allowinsert -m 1 \
--staging-table res_channel_num_temp --clear-staging-table \
--input-null-string '\\N' \
--input-null-non-string '\\N' --fields-terminated-by '\001'  
```

##  6、azkaban计算任务调度

前面我们已经通过各种层级的计算，将数据加载到ODS层，然后再dw层对数据进行宽表的处理，在res层对数据进行各种指标的统计，最终将统计的结果数据导出到了mysql数据库里面去了，我们就得到了我们的数据存储在mysql里面，然后通过通过javaWeb或者PHP程序做成报表展示，实现数据的报表统计结果，我们这里的数据处理形成了一整套的完整依赖先后过程，我们可以**通过azkaban实现工作任务流的调度，通过任务流调度来实现程序的自动化运行**

### 1、定义ods.job

```
type=command

command=/usr/bin/sh execute_ods.sh
```

### 2、定义dw.job

```
type=command

command=/usr/bin/sh execute_dw.sh

dependencies=ods
```

### 3、定义res1.job

```
type=command

command=/usr/bin/sh execute_res.sh

dependencies=dw 
```

### 4、定义res2.job

```
type=command

command=/usr/bin/sh execute_res2.sh

dependencies=dw
```

### 5、定义export.job

```
type=command

command=/usr/bin/sh export_res.sh

dependencies=res1,res2
```

### 6、将任务以及脚本打包

**将所有的任务以及脚本打包成为zip的包**，然后上传azkaban，进行执行以及定时任务调度

![img](数仓项目.assets/clip_image002-1589987318431.jpg)

 **老师笔记：数据仓库的常用两套框架：**

```
任务的调度以及hive的数仓的工具
impala +oozie +hue  **=》 可以实现通过页面的托拉拽的方式，实现任务的调度
数仓最少两套技术  **》 azkaban + hive + sqoop之类的
还有一套架构  impala +oozie  +hue  +datax/sqoop之类的

两套架构都是做数据分析，数仓之类的
impala吃的内存比较多  **》 运行速度比较快
hive  =》内存吃的少，可以节约服务器的内存资源，任务运行比较稳定   **》 运行速度比较慢
```

## 7、hive特殊语法及任务调优

### 1、hive特殊语法

#### 1、多维分析 grouping sets

从前面的学习，可观察到，hive当中多维度的分析**》其实就是在用不同的条件进行group  by操作，大部分sql语句都是在group  by操作

grouping sets语法说明：

```sql
Grouping sets  e.g:
select a,b,count(1) … Group by a,b grouping sets((a,b),(a),(b))
=group by a,b  union all  group by a  union all  group by b
```

 按照频道进行统计每个频道的人数，按照角色性别进行统计人数，以及频道和角色统计人数

```sql
select channel_id,role_sex,count(1) as total_person  from  ods_role_create group by channel_id,role_sex grouping sets((channel_id,role_sex),(channel_id),(role_sex));
```

```sql
+-------------+-----------+---------------+--+
| channel_id  | role_sex  | total_person  |
+-------------+-----------+---------------+--+
| NULL        | 0         | 102692        |
| NULL        | 1         | 82487         |
| 1           | NULL      | 172018        |
| 1           | 0         | 94226         |
| 1           | 1         | 77792         |
| 2           | NULL      | 13161         |
| 2           | 0         | 8466          |
| 2           | 1         | 4695          |
+-------------+-----------+---------------+--+
```

 为助于理解，我们来查看一下

```sql
select channel_id,role_sex,count(1) as total_person  from  ods_role_create group by channel_id,role_sex;
```

的结果：

![image-20200520234054649](数仓项目.assets/image-20200520234054649.png)



#### 2、cube数据分析

语法说明：

```sql
e.g:
select a,b,count(1) … Group by a,b with cube
= group by a,b grouping sets ((a,b),(a),(b),())
=group by a,b  union all  group by a  union all  group by b  union all
select count(1) ….
```

 需求：按照渠道id，角色性别分别进行多维度分析

```sql
select channel_id,role_sex,count(1) as total_person  from  ods_role_create 
group by channel_id,role_sex  with cube ;
```

```sql
+-------------+-----------+---------------+--+
| channel_id  | role_sex  | total_person  |
+-------------+-----------+---------------+--+
| NULL        | NULL      | 185179        |
| NULL        | 0         | 102692        |
| NULL        | 1         | 82487         |
| 1           | NULL      | 172018        |
| 1           | 0         | 94226         |
| 1           | 1         | 77792         |
| 2           | NULL      | 13161         |
| 2           | 0         | 8466          |
| 2           | 1         | 4695          |
+-------------+-----------+---------------+--+
```

#### 1、 rollup切片函数

rollup可以实现从右到左递减多级的统计，显示统计某一层次结构的聚合

语法说明：

```sql
e.g:
select a,b,c,count(1) …group by a,b,c with rollup;
=grouping sets((a,b,c),(a,b),(a),())
=group by a,b,c  union all  group by a,b  union all  group by a  union all  select count(1)
```

需求：按照渠道id，角色性别进行多维分析

```sql
select channel_id,role_sex,count(1) as total_person  from  ods_role_create group by channel_id,role_sex with rollup;
```

```sql
+-------------+-----------+---------------+--+
| channel_id  | role_sex  | total_person  |
+-------------+-----------+---------------+--+
| NULL        | NULL      | 185179        |
| 1           | NULL      | 172018        |
| 1           | 0         | 94226         |
| 1           | 1         | 77792         |
| 2           | NULL      | 13161         |
| 2           | 0         | 8466          |
| 2           | 1         | 4695          |
+-------------+-----------+---------------+--+
```

 

### 2、任务调优

#### 1、开启并行执行

某些任务在hive当中可以并行的执行，那么我们就可以通过开启hive的并行任务，实现多任务的并行操作，提高任务并行度。是用于union all，或者一些join操作等

```sql
#允许任务并行执行
set hive.exec.parallel=true
#每个任务并行线程数
set hive.exec.parallel.thread.number=8;
```

#### 2、控制任务的MapTask的个数

合理的控制每个mapTask处理的数据量，推算mapTask的个数

每个Map最大输入大小 

```sql
set mapred.max.split.size=268435456;  
一个节点上split的至少的大小  
set mapred.min.split.size.per.node=100000000; 
 一个交换机下split的至少的大小 
set mapred.min.split.size.per.rack=100000000;  
执行Map前进行小文件合并
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
```

#### 3、合理的设置reduceTask的数量

我们可以通过设置每个reduceTask处理的数据量，以及reduceTaks的个数来实现reduceTask的个数的控制

```
每个reduce处理的数据量
set hive.exec.reducers.bytes.per.reducer=500000000;

制定reduce数量
set mapred.reduce.tasks = 20;
```

#### 4、控制任务数

合理的控制任务的数量，

```sh
任务数控制
Reduce输出文件数

map输入的小文件从哪里来，怎么避免map合并小文件
Reduce太多会导致作业生成小文件过多
小文件过多会降低namenode的性能  在Map-only的任务结束时合并小文件
set hive.merge.mapfiles = true
在Map-Reduce的任务结束时合并小文件  
set hive.merge.mapredfiles = true  
合并文件的大小  
set hive.merge.size.per.task = 256*1000*1000
输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件合并
set hive.merge.smallfiles.avgsize=16000000
```

#### 5、排序问题

对于排序问题，能够不用全局排序就一定不要使用全局排序order by，如果一定要使用order by一定要加上limit

➢  order by、sort by

➢  limit

#### 6、mapTask尽量多处理数据

能够在MapTask处理完成的任务，尽量在MapTask多处理任务，避免数据通过shuffle到reduceTask，通过网络拷贝导致性能低下

map多承担问题，减少reduce的计算成本和数据传输成本

➢ Map join

➢ Map aggr

#### 7、使用分区裁剪，列裁剪

对于分区表，在查询数据的时候，尽量带上分区值，对于无效的列，不要进行select操作

```
Ø  记录数裁剪

p 分区、分桶

p 无效记录map阶段剔除

Ø  列裁剪

p 剔除无效、非计算范围内的列数据

p 列式存储 
```

#### 8、尽量减少IO操作

在sql语句当中尽量减少IO操作

```sql
减少IO

Ø   多表插入
from a  insert overwrite table b
select col1,col2 where type='b’  insert overwrite table c  select col1,col3 where type='c’  insert ...

Ø   一次计算，多次使用
with tab1 as
    (select ... where... group by ...)  select ...

```

##  8、数仓当中的拉链表

### 1、什么是拉链表

拉链表是针对数据仓库设计中表存储数据的方式而定义的，顾名思义，所谓**拉链，就是记录历史**。**记录一个事物从开始，一直到当前状态的所有变化的信息。**

在数据分析中，有时会需要维护一些历史状态，比如订单状态变化、评分变化等，为了保存下来这些状态变化的路径，可以通过拉链表实现

### 2、拉链表的使用场景

拉链表主要用于**解决数仓当中一些缓慢变化维的数据，需要保存历史各个版本**

数据量比较大，但业务要求每次需要查询全量历史，每天存储一份全量数据太占用存储空间

记录变更不大，比如只有状态和更新时间有变动，其它字段都不变

### 3、实现过程

**通过在记录末尾增加start_date和end_date字段来实现**

同一ID按时间排序后，如果有较新的记录，则当前记录的end_date等于较新记录的start_date-1，如果没有较新的记录，则当前记录的end_date等于一个默认值，比如99991231

我们先看一下在Mysql关系型数据库里的user表中信息变化。在2017-01-01这一天表中的数据是：

| **注册日期** | **用户编号** | **手机号码** |
| :----------: | :----------: | :----------: |
|  2017-01-01  |     001      |    111111    |
|  2017-01-01  |     002      |    222222    |
|  2017-01-01  |     003      |    333333    |
|  2017-01-01  |     004      |    444444    |

在2017-01-02这一天表中的数据是， 用户002和004的手机号码进行了修改，005是新增用户：

| **注册日期** | **用户编号** | **手机号码** |       **备注**       |
| :----------: | :----------: | :----------: | :------------------: |
|  2017-01-01  |     001      |    111111    |                      |
|  2017-01-01  |     002      |    233333    | 由222222变成2333333  |
|  2017-01-01  |     003      |    333333    |                      |
|  2017-01-01  |     004      |   43434343   | 由444444变成43434343 |
|  2017-01-02  |     005      |    555555    |    2017-01-02新增    |

在2017-01-03这一天表中的数据是， 用户004和005的手机号码进行了修改，006是新增用户：

| **注册日期** | **用户编号** | **手机号码** |       **备注**       |
| :----------: | :----------: | :----------: | :------------------: |
|  2017-01-01  |     001      |    111111    |                      |
|  2017-01-01  |     002      |    233333    | 由222222变成2333333  |
|  2017-01-01  |     003      |    333333    |                      |
|  2017-01-01  |     004      |    654321    | 由43434343变成654321 |
|  2017-01-02  |     005      |    511111    |  由555555变成511111  |
|  2017-01-03  |     006      |    666666    |    2017-01-03新增    |

如果在数据仓库中设计成历史拉链表保存该表，则会有下面这样一张表，这是最新一天（即2017-01-03）的数据：

 ![img](数仓项目.assets/clip_image002-1589989837287.png)

**说明：**

- t_start_date表示该条记录的生命周期开始时间，t_end_date表示该条记录的生命周期结束时间。
- **t_end_date = '9999-12-31'表示该条记录目前处于有效状态。**
- 如果查询当前所有有效的记录，则select * from user where t_end_date = '9999-12-31'。
- 如果查询2017-01-02的历史快照，则select * from user where t_start_date <= '2017-01-02' and     t_end_date >= '2017-01-02'。（此处要好好理解，是拉链表比较重要的一块。）

### 4、Hive实现拉链表

只考虑实现，不考虑性能

 -- **时间粒度：天 day**，建模之前需要按照Kimball思想"四步走"战略，以订单表为例，数据如下，**每天的订单明细：**

```sh
orderid createtime modifiedtime status
1   2016-08-20  2016-08-20  创建
2   2016-08-20  2016-08-20  创建
3   2016-08-20  2016-08-20  创建
1   2016-08-20  2016-08-21  支付
2   2016-08-20  2016-08-21  完成
4   2016-08-21  2016-08-21  创建
1   2016-08-20  2016-08-22  完成
3   2016-08-20  2016-08-22  支付
4   2016-08-21  2016-08-22  支付
5   2016-08-22  2016-08-22  创建
```

 根据拉链表，我们先想得到

```sql
1  2016-08-20  2016-08-20  创建 2016-08-20  2016-08-20
1  2016-08-20  2016-08-21  支付 2016-08-21  2016-08-21
1  2016-08-20  2016-08-22  完成 2016-08-22  9999-12-31
2  2016-08-20  2016-08-20  创建 2016-08-20  2016-08-20
2  2016-08-20  2016-08-21  完成 2016-08-21  9999-12-31
3  2016-08-20  2016-08-20  创建 2016-08-20  2016-08-21
3  2016-08-20  2016-08-22  支付 2016-08-22  9999-12-31
4  2016-08-21  2016-08-21  创建 2016-08-21  2016-08-21
4  2016-08-21  2016-08-22  支付 2016-08-22  9999-12-31
5  2016-08-22  2016-08-22  创建 2016-08-22  9999-12-31
```


可以看出 1，2，3，4每个订单的状态都有，并且也能统计到当前的有效状态。

 初始化hive表

```sql
create database if not exists chain_action;
use chain_action;

-- 创建订单表，使用sqoop进行从mysql中导入hive
CREATE TABLE orders (
orderid INT,
createtime STRING,
modifiedtime STRING,
status STRING
) row format delimited fields terminated by '\t';
 
 -- 订单全量，按天进行分区
CREATE TABLE ods_orders_inc (
orderid INT,
createtime STRING,
modifiedtime STRING,
status STRING
) PARTITIONED BY (day STRING)
row format delimited fields terminated by '\t';
 
 -- 订单拉链表设计
CREATE TABLE dw_orders_his (
orderid INT,
createtime STRING,
modifiedtime STRING,
status STRING,
dw_start_date STRING,
dw_end_date STRING
) row format delimited fields terminated by '\t' ;
```

 加载订单表数据：

将附件当中的order_chain.txt数据上传到node03服务器的/kkb/install/hivedatas路径下，然后进行加载

```sql
load data local inpath '/kkb/install/hivedatas/order_chain.txt' into table orders;
```

 首先**全量更新**，我们先到2016-08-20为止的数据。

初始化，先把2016-08-20的数据初始化进去。

```sql
INSERT overwrite TABLE ods_orders_inc PARTITION (day = '2016-08-20')
SELECT orderid,createtime,modifiedtime,status
FROM orders
WHERE createtime < '2016-08-21' and modifiedtime <'2016-08-21';
```

刷新到dw中；

```sql
INSERT overwrite TABLE dw_orders_his
SELECT orderid,createtime,modifiedtime,status,
createtime AS dw_start_date,
'9999-12-31' AS dw_end_date
FROM ods_orders_inc
WHERE day = '2016-08-20';
```

结果测试如下：

```sql
select * from dw_orders_his;
OK
1  2016-08-20  2016-08-20  创建 2016-08-20  9999-12-31
2  2016-08-20  2016-08-20  创建 2016-08-20  9999-12-31
3  2016-08-20  2016-08-20  创建 2016-08-20  9999-12-31
```

增量更新2016-08-21的数据：

```sql
INSERT overwrite TABLE ods_orders_inc PARTITION (day = '2016-08-21')
SELECT orderid,createtime,modifiedtime,status
FROM orders
WHERE (createtime = '2016-08-21'  and modifiedtime = '2016-08-21') OR modifiedtime = '2016-08-21';
 
select * from ods_orders_inc where day='2016-08-21';
OK
1  2016-08-20  2016-08-21  支付 2016-08-21
2  2016-08-20  2016-08-21  完成 2016-08-21
4  2016-08-21  2016-08-21  创建 2016-08-21
```

先放到增量表中，然后进行关联到一张临时表中，在插入到新表中，（同样，可以采用其他语句进行实现）：

```sql
DROP TABLE IF EXISTS dw_orders_his_tmp;
-- 创建表
CREATE TABLE dw_orders_his_tmp AS
SELECT orderid,
createtime,
modifiedtime,
status,
dw_start_date,
dw_end_date
FROM (
    SELECT a.orderid,
    a.createtime,
    a.modifiedtime,
    a.status,
    a.dw_start_date,
    CASE WHEN b.orderid IS NOT NULL AND a.dw_end_date > '2016-08-21' THEN '2016-08-21' ELSE a.dw_end_date END AS dw_end_date
    FROM dw_orders_his a
    left outer join (SELECT * FROM ods_orders_inc WHERE day = '2016-08-21') b
    ON (a.orderid = b.orderid)
    UNION ALL
    SELECT orderid,
    createtime,
    modifiedtime,
    status,
    modifiedtime AS dw_start_date,
    '9999-12-31' AS dw_end_date
    FROM ods_orders_inc
    WHERE day = '2016-08-21'
) x
ORDER BY orderid,dw_start_date;
 -- 将数据插入到拉链表中
INSERT overwrite TABLE dw_orders_his SELECT * FROM dw_orders_his_tmp;

```

 在根据上面步骤把2016-08-22号的数据更新进去，最后结果如下：

```sql
select * from dw_orders_his;
OK
1  2016-08-20  2016-08-20  创建 2016-08-20  2016-08-20
1  2016-08-20  2016-08-21  支付 2016-08-21  2016-08-21
1  2016-08-20  2016-08-22  完成 2016-08-22  9999-12-31
2  2016-08-20  2016-08-20  创建 2016-08-20  2016-08-20
2  2016-08-20  2016-08-21  完成 2016-08-21  9999-12-31
3  2016-08-20  2016-08-20  创建 2016-08-20  2016-08-21
3  2016-08-20  2016-08-22  支付 2016-08-22  9999-12-31
4  2016-08-21  2016-08-21  创建 2016-08-21  2016-08-21
4  2016-08-21  2016-08-22  支付 2016-08-22  9999-12-31
5  2016-08-22  2016-08-22  创建 2016-08-22  9999-12-31
```

至此，得到了我们想要的数据。

 

 

 

  

  

  

